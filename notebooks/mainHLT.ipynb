{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudia/HLT/project/code/hlt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/claudia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import mapply\n",
    "# set mapply\n",
    "mapply.init(n_workers=-1, progressbar=True)\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Lollipop Remix</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>[Intro: Lil Wayne]\\nHaha\\nUh-huh\\nNo homo (You...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Losing Weight Pt. 2</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>[Chorus: Cam'ron]\\nAyo, fuck losing weight\\nI'...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>A Milli</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>It's time to get money\\nI got a million reason...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>Warning</td>\n",
       "      <td>The Notorious B.I.G.</td>\n",
       "      <td>[Produced by Easy Mo Bee]\\n\\n[Verse 1: The Not...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>On My Own</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>[Intro: Lil Wayne]\\nYeah, hit me with the snar...</td>\n",
       "      <td>rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84211</th>\n",
       "      <td>7864996</td>\n",
       "      <td>SUHO - Grey Suit English Translation</td>\n",
       "      <td>Genius English Translations</td>\n",
       "      <td>[Verse 1]\\nI feel faded, I'm not used to this ...</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84212</th>\n",
       "      <td>7865871</td>\n",
       "      <td>HEARTFIRST</td>\n",
       "      <td>Kelsea Ballerini</td>\n",
       "      <td>[Verse 1]\\nMet him at a party\\nAccidentally br...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84213</th>\n",
       "      <td>7868429</td>\n",
       "      <td>Never Had a Chance</td>\n",
       "      <td>Katherine Li</td>\n",
       "      <td>[Chorus]\\nWhy am I still here and waiting for ...</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84214</th>\n",
       "      <td>7871730</td>\n",
       "      <td>Red Alert Boss Metal Zone</td>\n",
       "      <td>Clutch</td>\n",
       "      <td>[Verse 1]\\nI used to work for Doc Tyrell\\nIt d...</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84215</th>\n",
       "      <td>7872000</td>\n",
       "      <td>Unpredictable</td>\n",
       "      <td>DESTIN CONRAD &amp; Kiana Led</td>\n",
       "      <td>[Verse 1: DESTIN CONRAD]\\nSee with you, bae\\nY...</td>\n",
       "      <td>rb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84216 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                 title  \\\n",
       "0            7                        Lollipop Remix   \n",
       "1           19                   Losing Weight Pt. 2   \n",
       "2           26                               A Milli   \n",
       "3           29                               Warning   \n",
       "4           40                             On My Own   \n",
       "...        ...                                   ...   \n",
       "84211  7864996  SUHO - Grey Suit English Translation   \n",
       "84212  7865871                            HEARTFIRST   \n",
       "84213  7868429                    Never Had a Chance   \n",
       "84214  7871730             Red Alert Boss Metal Zone   \n",
       "84215  7872000                         Unpredictable   \n",
       "\n",
       "                            artist  \\\n",
       "0                        Lil Wayne   \n",
       "1                          Cam'ron   \n",
       "2                         Fabolous   \n",
       "3             The Notorious B.I.G.   \n",
       "4                        Lil Wayne   \n",
       "...                            ...   \n",
       "84211  Genius English Translations   \n",
       "84212             Kelsea Ballerini   \n",
       "84213                 Katherine Li   \n",
       "84214                       Clutch   \n",
       "84215    DESTIN CONRAD & Kiana Led   \n",
       "\n",
       "                                                  lyrics      tag  \n",
       "0      [Intro: Lil Wayne]\\nHaha\\nUh-huh\\nNo homo (You...      rap  \n",
       "1      [Chorus: Cam'ron]\\nAyo, fuck losing weight\\nI'...      rap  \n",
       "2      It's time to get money\\nI got a million reason...      rap  \n",
       "3      [Produced by Easy Mo Bee]\\n\\n[Verse 1: The Not...      rap  \n",
       "4      [Intro: Lil Wayne]\\nYeah, hit me with the snar...      rap  \n",
       "...                                                  ...      ...  \n",
       "84211  [Verse 1]\\nI feel faded, I'm not used to this ...     rock  \n",
       "84212  [Verse 1]\\nMet him at a party\\nAccidentally br...  country  \n",
       "84213  [Chorus]\\nWhy am I still here and waiting for ...      pop  \n",
       "84214  [Verse 1]\\nI used to work for Doc Tyrell\\nIt d...     rock  \n",
       "84215  [Verse 1: DESTIN CONRAD]\\nSee with you, bae\\nY...       rb  \n",
       "\n",
       "[84216 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset\n",
    "lyrics = pd.read_csv('lyrics.csv')\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84216 entries, 0 to 84215\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      84216 non-null  int64 \n",
      " 1   title   84213 non-null  object\n",
      " 2   artist  84216 non-null  object\n",
      " 3   lyrics  84216 non-null  object\n",
      " 4   tag     84216 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "lyrics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     3\n",
       "artist    0\n",
       "lyrics    0\n",
       "tag       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "lyrics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "artist    0\n",
       "lyrics    0\n",
       "tag       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "lyrics = lyrics.dropna(subset='title')\n",
    "lyrics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pop:  0.2\n",
      "rock:  0.2\n",
      "rap:  0.2\n",
      "rb:  0.2\n",
      "country:  0.2\n"
     ]
    }
   ],
   "source": [
    "for tag in lyrics['tag'].unique():\n",
    "    tag_perc = len(lyrics[lyrics['tag'] == tag]) / len(lyrics)\n",
    "    print(f'{tag}: {tag_perc: .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Intro: Lil Wayne]\n",
      "Haha\n",
      "Uh-huh\n",
      "No homo (Young Mula, baby!)\n",
      "I say, he's so sweet, make her wanna lick the wrapper\n",
      "Remix, baby!\n",
      "\n",
      "[Verse 1: Kanye West]\n",
      "Lollipop, lollipop, breastses just like Dolly Parton\n",
      "She ride my spaceship 'til she hit the top\n",
      "That hit the spot\n",
      "'Til she ask, \"How many li-i-li-i-licks do it take\" 'til she get to shop?\n",
      "Don't worry why my wrists got so freeze\n",
      "Tell a girl, \"Like Doritos, that's not 'cho cheese\"\n",
      "Tell her friends, \"Like Fritos, I'm tryin' to lay\"\n",
      "I can't only have one, and I ain't trying to wait\n",
      "This a song with Wayne, so you know it's gon' melt\n",
      "But you ain't finna murder me like everybody else\n",
      "I'ma rap like I got some type respect for myself\n",
      "I don't do it for my health, man, I do it for the belt\n",
      "Man, I do it to the death, 'til the roof get melt\n",
      "Hundred degrees, drop the roof, so the coupe don't melt\n",
      "Man, the flow so cold, chicken soup won't help\n",
      "We need four more hoes, we need oh, oh, oh, oh!\n",
      "You know what it is when we out of town\n",
      "We balling too serious, and you out of bounds\n",
      "So come here, baby girl\n",
      "You're now fucking with the best in the world\n",
      "[Break: Kanye West & Static Major]\n",
      "Lollipop\n",
      "The best in the world\n",
      "Sh-Sh-She lick me like a lollipop\n",
      "World, world\n",
      "She— She lick me like a lollipop\n",
      "\n",
      "[Chorus: Static Major, Lil Wayne, & Both]\n",
      "Shawty want a thug, thug, thug\n",
      "Bottles in the club, club, club (Bottles in the club)\n",
      "Shawty wanna hump (Shawty wanna)\n",
      "You know I like to touch (Shawty wanna)\n",
      "Your lovely lady lumps, lumps, lumps (Shawty wanna)\n",
      "Shawty want a thug (I like that)\n",
      "Bottles in the club (Hey, I like that)\n",
      "Shawty wanna hump (I like that, haha!)\n",
      "You know I like to touch your lovely lady lumps, lumps, lumps\n",
      "\n",
      "[Verse 2: Lil Wayne]\n",
      "Shawty say she wanna lick the wrapper\n",
      "And she gonna lick the rapper\n",
      "And I just wanna act like a porno-flicking actor\n",
      "I Anita-bake her; now, she caught up in that \"Rapture\"\n",
      "I got so much chips, I swear, they call me \"Hewlett Packard\"\n",
      "I got so much chips, you can have a bag if you're a snacker\n",
      "Greedy mother-fudge cake; now, tell me how that fudge tastes\n",
      "I do it for Bloods' sake—soo-woo!\n",
      "Think it's voodoo how that roof do di-di-dissipate\n",
      "Your girl want to participate\n",
      "She so, so sophisticate, 'cause her brain is off the chain\n",
      "And then, my diamonds are in the choir because they sing from off my chain\n",
      "And my Nina just joined the gang because all she do is bang\n",
      "Like Ricky Martin; Wayne and Kanye: pick your poison\n",
      "If that woman wanna cut, then tell her I am Mr. Ointment\n",
      "Tell her to make an appointment with Mr. I-Can't-Make-An-Appointment\n",
      "Take my lollipop and enjoy it—remix!\n",
      "[Chorus: Static Major, Lil Wayne, & Both]\n",
      "Shawty want a thug, thug, thug (Shawty want a thug, yeah!)\n",
      "Bottles in the club, club, club (Bottles in the club, yeah!)\n",
      "Shawty wanna hump (Yeah! Shawty wanna)\n",
      "You know I like to touch (Yeah! Shawty wanna)\n",
      "Your lovely lady lumps, lumps, lumps (Shawty wanna)\n",
      "Shawty want a thug (I like that)\n",
      "Bottles in the club (Hey, I like that)\n",
      "Shawty wanna hump (I like that, haha!)\n",
      "You know I like to touch your lovely lady lumps, lumps, lumps\n",
      "\n",
      "[Verse 3: Lil Wayne]\n",
      "Why would she? She probably be the odd cookie\n",
      "In the plastic bag 'bout to get crushed by a building\n",
      "I've flushed out the feeling of me being the shit\n",
      "'Cause I was leaving skid marks on everywhere I sit\n",
      "I am everywhere, I'm it, like Hide-and-Go\n",
      "And I can go anywhere: eeny, mini, miney, mo\n",
      "I'm in your neighborhood, area, CD-thing, tapedeck\n",
      "​iPod, your girlfriend, and she say I got great sex\n",
      "Safe sex is great sex, better wear a latex\n",
      "'Cause you don't want that late text, that \"I think I'm late\" text\n",
      "Haha! So wrap it up!\n",
      "Bu-Bu-But he's so sweet, sh-she wanna lick the wrapper\n",
      "\n",
      "[Chorus: Static Major & Lil Wayne]\n",
      "Shawty want a thug\n",
      "Bottles in the club, club, club\n",
      "Shawty wanna hump\n",
      "You know I like to touch\n",
      "Your lovely lady lumps, lumps, lumps (Re-Re-Re-Remix, baby!)\n",
      "[Outro: Static Major]\n",
      "Lick me like a lollipop (Lollipop)\n",
      "She— She lick me like a lollipop (L-Lollipop)\n",
      "Sh-Sh-Sh-She lick me like a lollipop (Lollipop)\n",
      "She— She lick me like a lollipop (Lollipop)\n"
     ]
    }
   ],
   "source": [
    "# Lyrics contain meta information between square brackets\n",
    "print(lyrics['lyrics'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:02<00:00, 13.93it/s]\n",
      "/tmp/ipykernel_135481/3727505202.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  lyrics['cleaned_lyrics'] = lyrics['lyrics'].mapply(lambda x: del_sqbrackets.sub('', x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Haha\n",
      "Uh-huh\n",
      "No homo (Young Mula, baby!)\n",
      "I say, he's so sweet, make her wanna lick the wrapper\n",
      "Remix, baby!\n",
      "\n",
      "\n",
      "Lollipop, lollipop, breastses just like Dolly Parton\n",
      "She ride my spaceship 'til she hit the top\n",
      "That hit the spot\n",
      "'Til she ask, \"How many li-i-li-i-licks do it take\" 'til she get to shop?\n",
      "Don't worry why my wrists got so freeze\n",
      "Tell a girl, \"Like Doritos, that's not 'cho cheese\"\n",
      "Tell her friends, \"Like Fritos, I'm tryin' to lay\"\n",
      "I can't only have one, and I ain't trying to wait\n",
      "This a song with Wayne, so you know it's gon' melt\n",
      "But you ain't finna murder me like everybody else\n",
      "I'ma rap like I got some type respect for myself\n",
      "I don't do it for my health, man, I do it for the belt\n",
      "Man, I do it to the death, 'til the roof get melt\n",
      "Hundred degrees, drop the roof, so the coupe don't melt\n",
      "Man, the flow so cold, chicken soup won't help\n",
      "We need four more hoes, we need oh, oh, oh, oh!\n",
      "You know what it is when we out of town\n",
      "We balling too serious, and you out of bounds\n",
      "So come here, baby girl\n",
      "You're now fucking with the best in the world\n",
      "\n",
      "Lollipop\n",
      "The best in the world\n",
      "Sh-Sh-She lick me like a lollipop\n",
      "World, world\n",
      "She— She lick me like a lollipop\n",
      "\n",
      "\n",
      "Shawty want a thug, thug, thug\n",
      "Bottles in the club, club, club (Bottles in the club)\n",
      "Shawty wanna hump (Shawty wanna)\n",
      "You know I like to touch (Shawty wanna)\n",
      "Your lovely lady lumps, lumps, lumps (Shawty wanna)\n",
      "Shawty want a thug (I like that)\n",
      "Bottles in the club (Hey, I like that)\n",
      "Shawty wanna hump (I like that, haha!)\n",
      "You know I like to touch your lovely lady lumps, lumps, lumps\n",
      "\n",
      "\n",
      "Shawty say she wanna lick the wrapper\n",
      "And she gonna lick the rapper\n",
      "And I just wanna act like a porno-flicking actor\n",
      "I Anita-bake her; now, she caught up in that \"Rapture\"\n",
      "I got so much chips, I swear, they call me \"Hewlett Packard\"\n",
      "I got so much chips, you can have a bag if you're a snacker\n",
      "Greedy mother-fudge cake; now, tell me how that fudge tastes\n",
      "I do it for Bloods' sake—soo-woo!\n",
      "Think it's voodoo how that roof do di-di-dissipate\n",
      "Your girl want to participate\n",
      "She so, so sophisticate, 'cause her brain is off the chain\n",
      "And then, my diamonds are in the choir because they sing from off my chain\n",
      "And my Nina just joined the gang because all she do is bang\n",
      "Like Ricky Martin; Wayne and Kanye: pick your poison\n",
      "If that woman wanna cut, then tell her I am Mr. Ointment\n",
      "Tell her to make an appointment with Mr. I-Can't-Make-An-Appointment\n",
      "Take my lollipop and enjoy it—remix!\n",
      "\n",
      "Shawty want a thug, thug, thug (Shawty want a thug, yeah!)\n",
      "Bottles in the club, club, club (Bottles in the club, yeah!)\n",
      "Shawty wanna hump (Yeah! Shawty wanna)\n",
      "You know I like to touch (Yeah! Shawty wanna)\n",
      "Your lovely lady lumps, lumps, lumps (Shawty wanna)\n",
      "Shawty want a thug (I like that)\n",
      "Bottles in the club (Hey, I like that)\n",
      "Shawty wanna hump (I like that, haha!)\n",
      "You know I like to touch your lovely lady lumps, lumps, lumps\n",
      "\n",
      "\n",
      "Why would she? She probably be the odd cookie\n",
      "In the plastic bag 'bout to get crushed by a building\n",
      "I've flushed out the feeling of me being the shit\n",
      "'Cause I was leaving skid marks on everywhere I sit\n",
      "I am everywhere, I'm it, like Hide-and-Go\n",
      "And I can go anywhere: eeny, mini, miney, mo\n",
      "I'm in your neighborhood, area, CD-thing, tapedeck\n",
      "​iPod, your girlfriend, and she say I got great sex\n",
      "Safe sex is great sex, better wear a latex\n",
      "'Cause you don't want that late text, that \"I think I'm late\" text\n",
      "Haha! So wrap it up!\n",
      "Bu-Bu-But he's so sweet, sh-she wanna lick the wrapper\n",
      "\n",
      "\n",
      "Shawty want a thug\n",
      "Bottles in the club, club, club\n",
      "Shawty wanna hump\n",
      "You know I like to touch\n",
      "Your lovely lady lumps, lumps, lumps (Re-Re-Re-Remix, baby!)\n",
      "\n",
      "Lick me like a lollipop (Lollipop)\n",
      "She— She lick me like a lollipop (L-Lollipop)\n",
      "Sh-Sh-Sh-She lick me like a lollipop (Lollipop)\n",
      "She— She lick me like a lollipop (Lollipop)\n"
     ]
    }
   ],
   "source": [
    "# Clean lyrics (remove content btw square brackets)\n",
    "del_sqbrackets = re.compile(r'\\[.*?\\]')\n",
    "lyrics['cleaned_lyrics'] = lyrics['lyrics'].mapply(lambda x: del_sqbrackets.sub('', x))\n",
    "lyrics = lyrics.drop(columns=['lyrics'])\n",
    "lyrics = lyrics.rename(columns={'cleaned_lyrics': 'lyrics'})\n",
    "print(lyrics['lyrics'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Penny for a Thought',\n",
       " 'John 1',\n",
       " 'John 3',\n",
       " 'Im Nobody Who Are You?',\n",
       " 'Buffalo Bills',\n",
       " 'Do Not Go Gentle Into That Good Night',\n",
       " 'The Second Coming',\n",
       " 'Ego-Tripping there may be a reason',\n",
       " 'The Animal Soul',\n",
       " 'The Love Song of J. Alfred Prufrock',\n",
       " 'Kubla Khan',\n",
       " 'Mending Wall',\n",
       " 'The Mower Against The Gardens',\n",
       " 'The Road Not Taken',\n",
       " 'The Tyger',\n",
       " 'The Lamb',\n",
       " 'Singing In The Rain',\n",
       " 'Andak Andak',\n",
       " 'Genesis 1',\n",
       " 'Ecclesiastes 1',\n",
       " 'Sonnet 18',\n",
       " 'Baba Says Cool for Thought',\n",
       " 'Stopping by Woods on a Snowy Evening',\n",
       " 'Who Will Survive America',\n",
       " 'Comment 1',\n",
       " 'Nicki Minaj',\n",
       " 'The Emperor of Ice Cream',\n",
       " 'He Wishes for the Cloths of Heaven',\n",
       " 'Introduction / The Revolution Will Not Be Televised Small Talk at 125th and Lenox Version',\n",
       " 'The Bill of Rights',\n",
       " 'Jabberwocky',\n",
       " 'And Did Those Feet In Ancient Time',\n",
       " 'The Rime Of The Ancient Mariner',\n",
       " 'Psalm 23',\n",
       " 'Matthew 25',\n",
       " 'Is This Really What You Want?',\n",
       " 'To His Coy Mistress',\n",
       " 'Revelation 1',\n",
       " 'I Have a Dream',\n",
       " 'Mentos Theme Song',\n",
       " 'Symphony No. 9 in D minor Op. 125 4th Movement English translation',\n",
       " 'The U.S. Constitution Article 1',\n",
       " 'The U.S. Constitution Preamble',\n",
       " 'The U.S. Constitution Article 2',\n",
       " 'The U.S. Constitution Article 3',\n",
       " 'Meditation At Lagunitas',\n",
       " 'Can U C the Pride in the Panther Female Version',\n",
       " 'Can U C the Pride in the Panther Male Version',\n",
       " 'If There Be Pain',\n",
       " 'In the Event of My Demise',\n",
       " 'Team America: Fuck Yeah Anthem',\n",
       " 'Song for Junior',\n",
       " 'Your Revolution',\n",
       " 'E Pluribus Unum',\n",
       " 'Normal Guy',\n",
       " 'Kennys Dead',\n",
       " '911',\n",
       " 'Nowhere To Run',\n",
       " 'I Wrote This Song a Long Time Ago',\n",
       " 'Cant Touch Me',\n",
       " 'Blood',\n",
       " 'Untimely Meditations',\n",
       " 'Grippo',\n",
       " 'Telegram',\n",
       " 'Shut Yo Face Uncle Fucka',\n",
       " 'Song Of Solomon King James Version',\n",
       " 'Matthew 27',\n",
       " 'Meta-Historical',\n",
       " 'Nutting',\n",
       " 'Money Money Money',\n",
       " 'A Letter to the Law',\n",
       " 'Sonnet XVI On His Blindness',\n",
       " 'They Were Hunters',\n",
       " 'Paradise Lost Book 9',\n",
       " 'The Ten Commandments',\n",
       " 'New Rules',\n",
       " 'Go The Fuck To Sleep',\n",
       " 'Modern Man',\n",
       " 'Ambition Over Adversity',\n",
       " 'Spare Ass Annie',\n",
       " 'The Last Words of Dutch Schultz',\n",
       " 'The Red Capes Are Coming',\n",
       " 'Beautiful Lie',\n",
       " 'Naked Lunch Excerpt: The Man That Taught His Asshole To Talk',\n",
       " 'Luke 23',\n",
       " 'Revelation 4',\n",
       " 'Me  You',\n",
       " 'Coded Language',\n",
       " 'Thunder',\n",
       " 'Gay Fish',\n",
       " 'Eyes Closed',\n",
       " 'Hip Hop Fan Rant',\n",
       " 'Take Over Control',\n",
       " 'Marbury v. Madison',\n",
       " 'Paradise Regaind Book 2',\n",
       " 'Letter to My 16-Year-Old Self',\n",
       " 'The Magna Carta',\n",
       " 'She Walks in Beauty',\n",
       " 'Sonnet 4',\n",
       " 'Sonnet 15',\n",
       " 'DONDA 2012',\n",
       " '40 Things I Learned From ID4 That I Never Knew Before',\n",
       " 'Why I Hate Religion But Love Jesus',\n",
       " 'The Gettysburg Address',\n",
       " 'A Martian Sends a Postcard Home',\n",
       " 'Queen Mab Speech',\n",
       " 'State of the Union 2012',\n",
       " 'Sexual Healing',\n",
       " 'Why I Hate Religion But Love Jesus Muslim Version',\n",
       " 'Sex Marriage  Fairytales',\n",
       " 'SCP-001 Proposals',\n",
       " 'Innocent Criminal',\n",
       " 'Anniversary Awards',\n",
       " 'Jumping Gigawatts',\n",
       " 'Message to the Blackman in America Chapter 1',\n",
       " 'Power Anywhere Theres People',\n",
       " 'Design',\n",
       " 'Hello',\n",
       " 'The Art Of Poetry',\n",
       " 'The Bean Eaters',\n",
       " 'We Real Cool',\n",
       " 'Lycidas',\n",
       " 'Alastor or the Spirit of Solitude',\n",
       " 'My Heart Leaps Up',\n",
       " 'Signifying Rappers',\n",
       " 'The Negro Speaks of Rivers',\n",
       " 'Ozymandias',\n",
       " 'Love Poem Medley',\n",
       " 'Smoke',\n",
       " 'Conscience',\n",
       " 'The Sun Never Says',\n",
       " 'Daddy',\n",
       " 'Reluctance',\n",
       " 'Paradise Lost Book 7',\n",
       " 'Chip N Dales Rescue Rangers Theme Song',\n",
       " 'In Just-',\n",
       " 'Danse Russe',\n",
       " 'To Elsie-',\n",
       " 'This Is Just To Say',\n",
       " 'For the Babies',\n",
       " 'How to Fight',\n",
       " 'Should You Sell Your Company?',\n",
       " 'The Great Dictator Speech',\n",
       " 'I taste a liquor never brewed',\n",
       " 'To make a prairie it takes a clover and one bee',\n",
       " 'The NYU Lecture April 11 2012',\n",
       " 'Shooting An Elephant',\n",
       " 'Matthew 5',\n",
       " 'Instagram',\n",
       " 'The Fish',\n",
       " 'Canto I',\n",
       " 'Yale Alumni Magazine: Rap Unwrapped',\n",
       " 'Letter to Ex-Girlfriend 1981',\n",
       " 'Resolution and Independence',\n",
       " 'Expostulation and Reply',\n",
       " 'A slumber did my spirit seal',\n",
       " 'Song She dwelt among th untrodden ways',\n",
       " 'Michael',\n",
       " 'Kudzu',\n",
       " 'Prefatory Sonnet Nuns fret not',\n",
       " 'MEN  WOMEN',\n",
       " 'Alien vs. Predator',\n",
       " 'Prom Night',\n",
       " 'Damon the Mower',\n",
       " 'Sonnet 147',\n",
       " 'Bee Im expecting you',\n",
       " 'Dear Women...',\n",
       " 'To a Butterfly',\n",
       " 'Othello Act 1 Scene 1',\n",
       " 'The Death of Saint Narcissus',\n",
       " 'The Flea',\n",
       " 'Lineage',\n",
       " 'Lovesong',\n",
       " 'Examination at the Womb-Door',\n",
       " 'Vigilante',\n",
       " 'Howl',\n",
       " 'Kaddish',\n",
       " 'Be Drunk',\n",
       " 'The Raven',\n",
       " 'A Dream Within A Dream',\n",
       " 'Alone',\n",
       " 'Annabel Lee',\n",
       " 'Dream-Land',\n",
       " 'Lenore',\n",
       " 'The Bells',\n",
       " 'The Valley of Unrest',\n",
       " 'To Helen',\n",
       " 'To My Mother',\n",
       " 'Ulalume',\n",
       " 'A Supermarket in California',\n",
       " 'Out Out—',\n",
       " 'Acquainted with the Night',\n",
       " 'After Apple-Picking',\n",
       " 'An Old Mans Winter Night',\n",
       " 'Birches',\n",
       " 'Directive',\n",
       " 'Dust of Snow',\n",
       " 'Mowing',\n",
       " 'Home Burial',\n",
       " 'Nothing Gold Can Stay',\n",
       " 'October',\n",
       " 'The Death of the Hired Man',\n",
       " 'The Pasture',\n",
       " 'A Love Song',\n",
       " 'Landscape With the Fall of Icarus',\n",
       " 'Spring and All By the road to the contagious hospital',\n",
       " 'Spring Storm',\n",
       " 'The Descent',\n",
       " 'Lady Lazarus',\n",
       " 'Morning Song',\n",
       " 'Let America Be America Again',\n",
       " 'I Too sing America',\n",
       " 'Dreams',\n",
       " 'Dream Variations',\n",
       " 'Madam and the Phone Bill',\n",
       " 'Madam and Her Madam',\n",
       " 'Life is Fine',\n",
       " 'The Weary Blues',\n",
       " 'Theme for English B',\n",
       " 'Po Boy Blues',\n",
       " 'Song of Myself',\n",
       " 'A child said What is the grass?',\n",
       " 'America',\n",
       " 'I Sing the Body Electric',\n",
       " 'I Hear America Singing',\n",
       " 'Crossing Brooklyn Ferry',\n",
       " 'O Me O Life',\n",
       " 'O Captain My Captain',\n",
       " 'Miracles',\n",
       " 'Passage to India',\n",
       " 'Out of the Cradle Endlessly Rocking',\n",
       " 'On the Beach at Night Alone',\n",
       " 'World Below the Brine',\n",
       " 'When I Heard the Learnd Astronomer',\n",
       " 'A Bird came down the Walk 328',\n",
       " 'A Drop fell on the Apple Tree 794',\n",
       " 'A Man may make a Remark 952',\n",
       " 'Because I Could Not Stop for Death',\n",
       " 'Besides the Autumn poets sing 131',\n",
       " 'I felt a Funeral in my Brain 280',\n",
       " 'Come Slowly—Eden 211',\n",
       " 'Dear March - Come in - 1320',\n",
       " 'Fame is a fickle food 1659',\n",
       " 'Hope is the thing with feathers',\n",
       " 'I cannot live with You 640',\n",
       " 'I heard a Fly buzz 465',\n",
       " 'I like to see it lap the Miles 43',\n",
       " 'I measure every Grief I meet 561',\n",
       " 'It sifts from Leaden Sieves - 311',\n",
       " 'It was not Death for I stood up 510',\n",
       " 'Its all I have to bring today 26',\n",
       " 'My life closed twice before its close 96',\n",
       " 'Safe in their Alabaster Chambers 216',\n",
       " 'The Outlet 162',\n",
       " 'The Soul selects her own Society 303',\n",
       " 'The Soul unto itself 683',\n",
       " 'There is no frigate like a book 1263',\n",
       " 'Theres a certain Slant of light',\n",
       " 'We never know how high we are 1176',\n",
       " 'Wild Nights – Wild Nights 249',\n",
       " 'Maggie and milly and molly and may',\n",
       " 'I sing of Olaf glad and big',\n",
       " 'Anyone lived in a pretty how town',\n",
       " '9.',\n",
       " 'Why must itself up every of a park',\n",
       " 'The Cambridge ladies who live in furnished souls',\n",
       " 'Spring is like a perhaps hand',\n",
       " 'Somewhere I Have Never Traveled Gladly Beyond',\n",
       " 'R-p-o-p-h-e-s-s-a-g-r',\n",
       " 'My father moved through dooms of love',\n",
       " 'Rhapsody on a Windy Night',\n",
       " 'Portrait of a Lady',\n",
       " 'Hysteria',\n",
       " 'Cousin Nancy',\n",
       " 'La Figlia Che Piange',\n",
       " 'How Do I Love Thee? Sonnet 43',\n",
       " 'A Musical Instrument',\n",
       " 'To George Sand: A Recognition',\n",
       " 'To George Sand: A Desire',\n",
       " 'Since feeling is first',\n",
       " 'And Death Shall Have No Dominion',\n",
       " 'As You Like It Act 1 Scene 1',\n",
       " 'As You Like It Act 1 Scene 2',\n",
       " 'As You Like It Act 1 Scene 3',\n",
       " 'As You Like It Act 2 Scene 1',\n",
       " 'As You Like It Act 2 Scene 4',\n",
       " 'As You Like It Act 2 Scene 5',\n",
       " 'As You Like It Act 2 Scene 7',\n",
       " 'As You Like It Act 3 Scene 2',\n",
       " 'As You Like It Act 3 Scene 3',\n",
       " 'As You Like It Act 3 Scene 5',\n",
       " 'As You Like It Act 4 Scene 1',\n",
       " 'As You Like It Act 4 Scene 3',\n",
       " 'As You Like It Act 5 Scene 2',\n",
       " 'As You Like It Act 5 Scene 3',\n",
       " 'As You Like It Act 5 Scene 4',\n",
       " 'Essay on Criticism',\n",
       " 'The Golden Years',\n",
       " 'Some Days',\n",
       " 'Litany',\n",
       " 'Introduction to Poetry',\n",
       " 'Forgetfulness',\n",
       " 'So You Want to be a Writer?',\n",
       " 'Facing It',\n",
       " 'Where the Sidewalk Ends',\n",
       " 'Mr. Grumpledumps Song',\n",
       " 'September 1 1939',\n",
       " 'Queen Mab',\n",
       " 'Digging',\n",
       " 'Mid-Term Break',\n",
       " 'Death of A Naturalist',\n",
       " 'Half-Caste',\n",
       " 'Judges 16',\n",
       " 'When the Pawn...',\n",
       " 'Whitey on the Moon',\n",
       " 'The Destruction of Sennacherib',\n",
       " 'So well go no more a-roving',\n",
       " 'When We Two Parted',\n",
       " 'Lines Inscribed Upon a Cup Formed from a Skull',\n",
       " 'Don Juan  Dedication',\n",
       " 'Phenomenal Woman',\n",
       " 'I carry your heart with mei carry it in my heart',\n",
       " 'Still I Rise',\n",
       " 'Not Just Money',\n",
       " 'Pretty Lady',\n",
       " 'Magic City',\n",
       " 'A Song in the Front Yard',\n",
       " 'Kitchenette building',\n",
       " 'The Yachts',\n",
       " 'Us Ones In Between',\n",
       " 'Voices of Old People',\n",
       " 'The Charge of the Light Brigade',\n",
       " 'Brown v. Board of Education',\n",
       " 'Murderer',\n",
       " 'Frank Oceans Open Letter on Tumblr',\n",
       " 'Notes on The Waste Land',\n",
       " 'Too Experienced',\n",
       " 'Lady Mondegreen and the Miracle of Misheard Song Lyrics',\n",
       " 'Driver A',\n",
       " 'How We Will Read',\n",
       " 'Canary',\n",
       " 'Ophelia',\n",
       " 'My Bohemian Life',\n",
       " 'The Drunken Boat',\n",
       " 'Nettys Girl',\n",
       " 'Just One of Those Days',\n",
       " 'Moby-Dick Chap. 42: The Whiteness of the Whale',\n",
       " 'A Portrait of the Artist as a Young Man Chapter 1',\n",
       " 'A Portrait of the Artist as a Young Man Chap. 3',\n",
       " 'A Portrait of the Artist as a Young Man Chap. 4',\n",
       " 'Before the Law',\n",
       " 'Where I Lived and What I Lived For',\n",
       " 'Sounds',\n",
       " 'The Pond in Winter',\n",
       " 'Conclusion',\n",
       " 'The Dead',\n",
       " 'Moby-Dick Chap. 1: Loomings',\n",
       " 'Moby-Dick Chap. 2: The Carpet-Bag',\n",
       " 'Moby-Dick Chap. 3: The Spouter-Inn',\n",
       " 'Moby-Dick Chap. 4: The Counterpane',\n",
       " 'Moby-Dick Chap. 6: The Street',\n",
       " 'Moby-Dick Chap. 7: The Chapel',\n",
       " 'Moby-Dick Chap. 8: The Pulpit',\n",
       " 'Moby-Dick Chap. 9: The Sermon',\n",
       " 'Moby-Dick Chap. 10: A Bosom Friend',\n",
       " 'Moby-Dick Chap. 12: Biographical',\n",
       " 'Moby-Dick Chap. 14: Nantucket',\n",
       " 'Moby-Dick Chap. 16: The Ship',\n",
       " 'Moby-Dick Chap. 17: The Ramadan',\n",
       " 'Moby-Dick Chap. 19: The Prophet',\n",
       " 'Moby-Dick Chap. 22: Merry Christmas',\n",
       " 'Moby-Dick Chap. 23: The Lee Shore',\n",
       " 'Moby-Dick Chap. 24: The Advocate',\n",
       " 'Moby-Dick Chap. 28: Ahab',\n",
       " 'Moby-Dick Chap. 32: Cetology',\n",
       " 'Moby-Dick Chap. 35: The Mast-Head',\n",
       " 'Moby-Dick Chap. 36: The Quarter-Deck',\n",
       " 'Moby-Dick Chap. 37: Sunset',\n",
       " 'Moby-Dick Chap. 39: First Night Watch Fore-Top',\n",
       " 'Moby-Dick Chap. 41: Moby Dick',\n",
       " 'Moby-Dick Chap. 48: The First Lowering',\n",
       " 'Moby-Dick Chap. 55: Of the Monstrous Pictures of Whales',\n",
       " 'Moby-Dick Chap. 58: Brit',\n",
       " 'Moby-Dick Chap. 87: The Grand Armada',\n",
       " 'Moby-Dick Chap. 96: The Try-Works',\n",
       " 'Moby-Dick Chap. 99: The Doubloon',\n",
       " 'Moby-Dick Chap. 135: The Chase -- Third Day',\n",
       " 'Moby-Dick Chap. 136: Epilogue',\n",
       " 'The Catcher in the Rye Chap. 1',\n",
       " 'Metaphysics of Love',\n",
       " 'In drear-nighted December',\n",
       " 'Paradise Lost Book 1',\n",
       " 'Mud Puddle Blossom V',\n",
       " 'Glory Glory Man United',\n",
       " 'Kurt Cobains Suicide Note',\n",
       " 'First Speech as Romneys VP Nominee',\n",
       " 'Letter of Apology',\n",
       " 'OK',\n",
       " 'Plessy v. Ferguson',\n",
       " 'Black and Blue',\n",
       " 'Quiet As Its Kept',\n",
       " 'Here Is The House',\n",
       " 'The Bluest Eye Excerpt: Hid Like Thieves From Life',\n",
       " 'Furniture Without Memories',\n",
       " 'The Gift of the Dolls',\n",
       " 'They',\n",
       " 'The Great Gatsby Chapter IV',\n",
       " 'Job all chapters',\n",
       " 'By Any Means Necessary',\n",
       " 'The Ballot or the Bullet',\n",
       " 'The House Negro and the Field Negro',\n",
       " 'Sonnet 116',\n",
       " 'Sonnet 130',\n",
       " 'Day of the Dead',\n",
       " 'The Bat',\n",
       " 'The Partial Explanation',\n",
       " 'Domestic Work 1937',\n",
       " 'To a Mouse',\n",
       " 'Fun Facts About Sex',\n",
       " 'Santa Monica',\n",
       " 'Matthew 14',\n",
       " 'Ayesha Says Intro',\n",
       " 'Dream Boogie',\n",
       " 'Harlem What happens to a dream deferred?',\n",
       " '2012 DNC Speech',\n",
       " 'Heart Sutra',\n",
       " 'Glengarry Glen Ross Brass Balls Speech',\n",
       " 'Slaughterhouse-Five Chapter 2',\n",
       " 'Slaughterhouse-Five Chapter 4',\n",
       " 'The Picture of Dorian Gray Preface',\n",
       " 'If I Should Have a Daughter TED Talk',\n",
       " 'Sonnet 1',\n",
       " 'Sonnet 2',\n",
       " 'Sonnet 3',\n",
       " 'Sonnet 5',\n",
       " 'Sonnet 10',\n",
       " 'Sonnet 12',\n",
       " 'Sonnet 16',\n",
       " 'Sonnet 17',\n",
       " 'Sonnet 19',\n",
       " 'Sonnet 20',\n",
       " 'Sonnet 23',\n",
       " 'Sonnet 27',\n",
       " 'Sonnet 29',\n",
       " 'Sonnet 30',\n",
       " 'Sonnet 35',\n",
       " 'Sonnet 55',\n",
       " 'Sonnet 60',\n",
       " 'Sonnet 62',\n",
       " 'Sonnet 64',\n",
       " 'Sonnet 65',\n",
       " 'Sonnet 66',\n",
       " 'Sonnet 71',\n",
       " 'Sonnet 73',\n",
       " 'Sonnet 75',\n",
       " 'Sonnet 80',\n",
       " 'The Sun Rising',\n",
       " 'To The Virgins To Make Much Of Time',\n",
       " 'Sonnet 94',\n",
       " 'Sonnet 98',\n",
       " 'Sonnet 104',\n",
       " 'Sonnet 106',\n",
       " 'Sonnet 129',\n",
       " 'Sonnet 138',\n",
       " 'Sonnet 141',\n",
       " 'Sonnet 143',\n",
       " 'Sonnet 144',\n",
       " 'Sonnet 154',\n",
       " 'AP',\n",
       " 'Pancakes',\n",
       " 'Zarathustras Prologue',\n",
       " 'The Three Metamorphoses I',\n",
       " 'The Pale Criminal VI',\n",
       " 'Reading and Writing VII',\n",
       " 'Old and Young Women XVIII',\n",
       " 'Daddy',\n",
       " 'Funeral Oration',\n",
       " 'The Spirit of Gravity LV',\n",
       " 'Little Things',\n",
       " 'Harrison Bergeron',\n",
       " 'The Picture of Dorian Gray Chap. 1',\n",
       " 'The Picture of Dorian Gray Chap. 2',\n",
       " 'The Sign LXXX',\n",
       " 'How Doth the Little Crocodile',\n",
       " 'Against Idleness and Mischief',\n",
       " 'Marijuana',\n",
       " 'Jessie Redmon Fauset',\n",
       " 'Blue-Storybook-Eyes',\n",
       " 'Nothing Like The First Time',\n",
       " 'Lovely Mary Jane',\n",
       " 'Outdoors',\n",
       " 'The Clod and the Pebble',\n",
       " 'Wont you celebrate with me',\n",
       " 'Caged Bird',\n",
       " 'My Grandmothers Love Letters',\n",
       " 'The Launch Pad: Inside Y Combinator Silicon Valleys Most Exclusive School for Startups Excerpts',\n",
       " 'Why Andreessen Horowitz Is Investing in Rap Genius',\n",
       " 'Letter to Potential Facebook IPO Investors',\n",
       " 'Startup  Growth',\n",
       " 'We Wear the Mask',\n",
       " 'The Bluest Bluest Eyes',\n",
       " 'First Presidential Debate of 2012',\n",
       " 'U.S. v Syufy Enterprises',\n",
       " 'Book 1: Axioms or Laws of Motion',\n",
       " 'Twenty Questions',\n",
       " 'The Apology',\n",
       " 'The Republic Book 7: The Allegory of the Cave',\n",
       " 'The Prince Introduction English version',\n",
       " 'The Prince Chapter 1 English version',\n",
       " 'The Prince Chapter 2 English version',\n",
       " 'The Prince Chapter 3 English version',\n",
       " 'The Prince Chapter 6 English version',\n",
       " 'The Prince Chapter 7 English version',\n",
       " 'The Prince Chapter 9 English version',\n",
       " 'The Prince Chapter 15 English version',\n",
       " 'The Prince Chapter 17 English version',\n",
       " 'The Prince Chapter 18 English version',\n",
       " 'Letter to the Grand Duchess Christina',\n",
       " 'Fragments',\n",
       " 'The Critique of Pure Reason Introduction',\n",
       " 'The Critique of Pure Reason Part 1',\n",
       " 'The Critique of Pure Reason Part 29',\n",
       " 'The Garden',\n",
       " 'Stephen Sondheim On Rap',\n",
       " 'Simplex Munditiis Still to be neat',\n",
       " 'To His Mistress Going to Bed',\n",
       " 'The Canonization',\n",
       " 'Men',\n",
       " 'Musée des Beaux Arts',\n",
       " 'Candide Introduction',\n",
       " 'Candide I',\n",
       " 'Candide II',\n",
       " 'Candide III',\n",
       " 'Candide IV',\n",
       " 'Candide V',\n",
       " 'Candide VI',\n",
       " 'Candide IX',\n",
       " 'Candide X',\n",
       " 'Candide XI',\n",
       " 'Candide XII',\n",
       " 'Candide XVI',\n",
       " 'Candide XVIII',\n",
       " 'Candide XIX',\n",
       " 'The Ant and the Grasshopper',\n",
       " 'Candide XXV',\n",
       " 'Candide XXX',\n",
       " 'Fathers Day',\n",
       " 'The Cat-Maiden',\n",
       " 'A Lecture on Ethics 1929',\n",
       " 'The Dog and the Shadow',\n",
       " 'The Dog and the Wolf',\n",
       " 'The Fox the Cock and the Dog',\n",
       " 'Sick and Tired',\n",
       " 'The Fox and the Monkey',\n",
       " 'The Tortoise and the Hare',\n",
       " 'Ode on a Grecian Urn',\n",
       " 'A Valediction: Forbidding Mourning',\n",
       " 'Second Inaugural Address',\n",
       " 'Give Me Liberty or Give Me Death',\n",
       " 'Communist Manifesto Prologue',\n",
       " 'Communist Manifesto Chap. 1: Bourgeois and Proletarians',\n",
       " 'Communist Manifesto Chap. 2: Proletarians and Communists',\n",
       " 'Communist Manifesto Chap. 3: Socialist and Communist Literature Part 1: Reactionary Socialism',\n",
       " 'Communist Manifesto Chap. 3: Socialist and Communist Literature Part 3: Critical-Utopian Socialism and Communism',\n",
       " 'The Kingdom of the Lion',\n",
       " 'The Lion and the Hare',\n",
       " 'The Lion and the Mouse',\n",
       " 'The Man the Boy and the Donkey',\n",
       " 'The Mouse the Frog and the Hawk',\n",
       " 'Mayflower Compact',\n",
       " 'Thank U Mamma',\n",
       " 'The Thief and the Innkeeper',\n",
       " 'The Two Crabs',\n",
       " 'Virginia Declaration of Rights 1776',\n",
       " 'The Frog King or Iron Henry',\n",
       " 'English Bill of Rights 1689',\n",
       " 'Song',\n",
       " 'We Grow Accustomed to the Dark',\n",
       " 'Loveliest of trees the cherry now',\n",
       " 'Lift Every Voice and Sing',\n",
       " 'I Am Offering This Poem to You',\n",
       " '\\u200biTunes Terms of Service',\n",
       " 'Chapter 1: The Life and Death of Scyld',\n",
       " 'Chapter 2: Scylds Successors - Hrothgars Great Mead-Hall',\n",
       " 'Supreme Alphabet',\n",
       " 'Chapter 3: Grendel The Murderer',\n",
       " 'Chapter 4: Beowulf Goes To Hrothgars Assistance',\n",
       " 'Chapter 5: The Geats Reach Heorot',\n",
       " 'Chapter 6: Beowulf Introduces Himself At The Palace',\n",
       " 'Chapter 7: Hrothgar and Beowulf',\n",
       " 'Chapter 8: Hrothgar and Beowulf Continued',\n",
       " 'Chapter 9: Unferth Taunts Beowulf',\n",
       " 'Chapter 10: Beowulf Silences Unferth - Glee Is High',\n",
       " 'Chapter 11: All Sleep Save One',\n",
       " 'Chapter 12: Grendel and Beowulf',\n",
       " 'Chapter 13: Grendel Is Vanquished',\n",
       " 'Chapter 14: Rejoicing of the Danes',\n",
       " 'Chapter 15: Hrothgar’s Gratitude',\n",
       " 'Chapter 16: Hrothgar Lavishes Gifts Upon His Deliverer',\n",
       " 'Chapter 17: Banquet continued - The Scops Song of Finn and Hnæf',\n",
       " 'Chapter 18: The Finn Episode continued - The Banquet Continues',\n",
       " 'Sport',\n",
       " 'Chapter 19: Beowulf Receives Further Honor',\n",
       " 'Chapter 20: The Mother of Grendel',\n",
       " 'Chapter 21: Hrothgars Account of the Monsters',\n",
       " 'Chapter 22: Beowulf Seeks Grendels Mother',\n",
       " 'Chapter 23: Beowulfs Fight With Grendels Mother',\n",
       " 'Chapter 24: Beowulf Is Double-Conqueror',\n",
       " 'Chapter 25: Beowulf Brings His Trophies - Hrothgars Gratitute',\n",
       " 'Chapter 29: Beowulf and Higelac',\n",
       " 'Chapter 32: The Hoard and the Dragon',\n",
       " 'Chapter 34: Beowulf Seeks the Dragon - Beowulfs Reminiscences',\n",
       " 'Chapter 35: Reminiscences Continued - Beowulfs Last Battle',\n",
       " 'Chapter 37: The Fatal Struggle - Beowulfs Last Moments',\n",
       " 'Chapter 38: Wiglaf Plunders the Dragons Den - Beowulfs Death',\n",
       " 'Chapter 42: Wiglafs Sad Story - The Hoard Carried Off',\n",
       " 'Chapter 43: The Burning of Beowulf',\n",
       " 'Vice Presidential Debate: 2012',\n",
       " 'Desiderata',\n",
       " 'Message to the Grassroots',\n",
       " 'California',\n",
       " 'Puppy Love',\n",
       " 'Hey Babe',\n",
       " 'New York Interview - 1986',\n",
       " 'Ernesto Che Guevara - Speech to the UN English Translation',\n",
       " 'I Got Bitches',\n",
       " 'Lonely Can Be Sweet',\n",
       " 'Federalist No. 10',\n",
       " 'Chasing Kanye',\n",
       " 'Farewell Address 1796',\n",
       " 'Ballot or Bullet',\n",
       " 'Sloprano The Great Mighty Poos Song',\n",
       " 'Making Yourself a CEO',\n",
       " 'Second Presidential Debate of 2012',\n",
       " 'New Zealand Interview 1979',\n",
       " 'Dont Nobody Bring Me No Bad News',\n",
       " 'Your Good Thing Is About to End',\n",
       " 'Someone Like You',\n",
       " 'Your Guide to RapGenius.com',\n",
       " 'Invisible Man Epigraph  Prologue',\n",
       " 'Invisible Man Chap. 1: Battle Royal',\n",
       " 'Pound Cake Speech',\n",
       " 'Shades of Grey',\n",
       " 'Third Presidential Debate of 2012',\n",
       " 'Cuban Missile Crisis Speech',\n",
       " 'Epitaph on a Tyrant',\n",
       " 'The Snow Man',\n",
       " 'Full House Slam Poetry',\n",
       " 'Phatty',\n",
       " 'Welcome To DC',\n",
       " 'One Art',\n",
       " 'An Open Letter to Dr. Laura Schlesinger',\n",
       " 'Racist Rant',\n",
       " 'My Princess Gone',\n",
       " 'A Modest Proposal',\n",
       " 'Tao Te Ching Stephen Mitchell Translation',\n",
       " 'Chapter 1: Columbus The Indians and Human Progress',\n",
       " 'Chapter 2: Drawing the Color Line',\n",
       " 'Chapter 3: Persons of Mean and Vile Condition',\n",
       " 'Chapter 4: Tyranny is Tyranny',\n",
       " 'Chapter 5: A Kind of Revolution',\n",
       " 'Chapter 6: The Intimately Oppressed',\n",
       " 'Chapter 7: As Long As Grass Grows or Water Runs',\n",
       " 'Chapter 8: We Take Nothing by Conquest Thank God',\n",
       " 'Chapter 12: The Empire and the People',\n",
       " 'Chapter 13: The Socialist Challenge',\n",
       " 'Chapter 14: War Is the Health of the State',\n",
       " 'Chapter 16: A Peoples War?',\n",
       " 'Chapter 17: Or Does It Explode?',\n",
       " 'Chapter 18: The Impossible Victory: Vietnam',\n",
       " 'Chapter 19: Surprises',\n",
       " 'Chapter 20: The Seventies: Under Control?',\n",
       " 'Chapter 21: Carter-Reagan-Bush: The Bipartisan Consensus',\n",
       " 'Chapter 22: The Unreported Resistance',\n",
       " 'Chapter 23: The Clinton Presidency and the Crisis of Democracy',\n",
       " 'An Open Letter to Hobbyists',\n",
       " 'The Fall of Rome',\n",
       " 'Martin Luther King Jr. - I Have A Dream Traduction Française',\n",
       " 'Wild Dreams of a New Beginning',\n",
       " 'Void',\n",
       " 'Motives and Thoughts',\n",
       " '12 Fingers',\n",
       " 'Fire and Ice',\n",
       " 'Passing Time',\n",
       " 'Black Power',\n",
       " 'A Glorious Dawn',\n",
       " 'Thomas The Rhymer 37 B',\n",
       " 'The Legend of Sleepy Hollow',\n",
       " 'The Souls of Black Folk The Forethought',\n",
       " 'Alone With You Maddies Song',\n",
       " 'Strange Meeting',\n",
       " 'Know your enemy know yourself thats the politic. Dead Prez',\n",
       " 'Bluebird',\n",
       " 'The History of One Tough Motherfucker',\n",
       " '8 Count',\n",
       " 'The Owl and the Pussycat',\n",
       " 'Throwing Away the Alarm Clock',\n",
       " 'The Mockingbird',\n",
       " 'Angel',\n",
       " 'Exodus 1',\n",
       " 'Exodus 3',\n",
       " 'Exodus 4',\n",
       " 'Exodus 14',\n",
       " 'Exodus 15',\n",
       " 'Exodus 20',\n",
       " 'The Cask of Amontillado',\n",
       " 'Happy Birthday',\n",
       " 'The Telephone',\n",
       " 'Dinosauria We',\n",
       " 'Part of the List',\n",
       " 'James 1',\n",
       " 'Open Letter To Mitt Romney',\n",
       " 'Ill Do Anything For You',\n",
       " 'The Sweetheart of Sigma Chi',\n",
       " 'An Open Letter About Rape To  Richard Mourdock',\n",
       " 'Muah',\n",
       " '2012 Presidential Concession Speech',\n",
       " '2008 Presidential Acceptance Speech',\n",
       " '2012 Presidential Acceptance Speech',\n",
       " 'Elegy Written In A Country Churchyard',\n",
       " 'Herbalist',\n",
       " 'Body Beat',\n",
       " 'London poem',\n",
       " 'Letter To The U.K. Music Press Regarding Eric Clapton',\n",
       " 'The Last Question: Introduction',\n",
       " 'The Last Question: Part I',\n",
       " 'The Last Question: Part II',\n",
       " 'The Last Question: Part VII',\n",
       " 'The Nightingale a conversational poem.',\n",
       " 'Flashy Words',\n",
       " 'Hansel and Gretel',\n",
       " 'The Devil with the Three Golden Hairs',\n",
       " 'The Wishing-Table The Gold-Ass and The Cudgel in the Sack',\n",
       " 'The Juniper-Tree',\n",
       " 'Snow-White and the Seven Dwarves',\n",
       " 'Treaty of Versailles Articles 231 - 247',\n",
       " 'Revelation 6',\n",
       " 'Revelation 12',\n",
       " 'Revelation 13',\n",
       " 'Revelation 19',\n",
       " 'Revelation 20',\n",
       " 'Revelation 22',\n",
       " 'Revelation 21',\n",
       " 'A Rap Thing',\n",
       " 'Law 1: Never Outshine the Master - The Judgment',\n",
       " 'Law 1: Never Outshine the Master - Transgression Of The Law',\n",
       " 'Law 1: Never Outshine the Master - Interpretation of Transgression',\n",
       " 'Law 1: Never Outshine the Master - Observance Of The Law',\n",
       " 'Funky Dollar Bill',\n",
       " 'The Picard Song',\n",
       " 'Valkyrie Horsewhip Reel',\n",
       " 'The Declaration of Independence',\n",
       " 'Rapgenius House of L1N6O™',\n",
       " 'ODB 98 Grammy Interruption',\n",
       " 'CellnLs',\n",
       " 'The Ladys Dressing Room',\n",
       " 'The Mental Traveller',\n",
       " 'A Poison Tree',\n",
       " 'An Imperial Message',\n",
       " 'The Advocates',\n",
       " 'Is My Team Ploughing',\n",
       " 'Not Ideas About the Thing but the Thing Itself',\n",
       " 'Song for a Dark Girl',\n",
       " 'Gubbinal',\n",
       " 'Law 1: Never Outshine the Master - Interpretation of Observance',\n",
       " 'Troy Anthony Davis Last Words',\n",
       " 'Law 1: Never Outshine the Master - Keys to Power',\n",
       " 'Law 1: Never Outshine the Master - Reversal of Law',\n",
       " 'Warrior',\n",
       " 'The Brain – is wider than the Sky –',\n",
       " 'I Dont Need',\n",
       " 'Lit Genius Guidelines',\n",
       " 'Everybody Hates Ned Flanders',\n",
       " 'Unlike Us',\n",
       " 'I Dwell in Possibility',\n",
       " 'Letter About Her Dog Janet',\n",
       " 'Tell All the Truth But Tell it Slant',\n",
       " 'The Brain within its Groove',\n",
       " 'The United Fruit Company',\n",
       " 'The Handsomest Drowned Man In The World',\n",
       " 'A Boys Thanksgiving Day Over the river and through the wood',\n",
       " 'To Autumn',\n",
       " 'An Open Letter To President Obama',\n",
       " 'Heart of Darkness Section I',\n",
       " 'Heart of Darkness Section II',\n",
       " 'Heart of Darkness Section III',\n",
       " 'The Yellow Wallpaper',\n",
       " 'Bartleby the Scrivener: A Story of Wall Street',\n",
       " 'Rule Britannia',\n",
       " 'God Save the Queen or King',\n",
       " 'If',\n",
       " 'The British Grenadiers',\n",
       " 'I Vow to Thee My Country',\n",
       " 'The White Mans Burden',\n",
       " 'The Republic Book 1',\n",
       " 'The Republic Book 2',\n",
       " 'I Got a Thing You Got a Thing Everybodys Got a Thing',\n",
       " 'The Republic Book 8',\n",
       " 'Paradise Lost Book 2',\n",
       " 'Paradise Lost Book 3',\n",
       " 'Paradise Lost Book 4',\n",
       " 'Paradise Lost Book 5',\n",
       " 'The Masque of Anarchy',\n",
       " 'Paradise Lost Book 10',\n",
       " 'The Right to Be Unhappy Chapter XVII',\n",
       " 'Open Letter To Kanye West',\n",
       " 'On the Morning of Christs Nativity',\n",
       " 'Misogyny Speech',\n",
       " 'The Lyceum Address',\n",
       " 'Ulysses',\n",
       " 'The Triumph of Bullshit',\n",
       " 'Letter From Birmingham Jail',\n",
       " 'Xotic',\n",
       " 'The Political Zoo - Satirical essay on Newt Gingrich',\n",
       " 'The Fox and the Sick Lion',\n",
       " 'The Fly',\n",
       " 'The Human Abstract',\n",
       " 'Diving into the Wreck',\n",
       " 'Ode To a Large Tuna in the Market',\n",
       " 'A Conceit',\n",
       " 'A Prayer For Old Age',\n",
       " 'Instants',\n",
       " 'Limits',\n",
       " 'To A Cat',\n",
       " 'Shinto',\n",
       " 'The Other Tiger',\n",
       " 'And The Moon And The Stars And The World',\n",
       " 'Poem As the cat',\n",
       " 'Of De Witt Williams on his way to Lincoln Cemetery',\n",
       " 'Argument',\n",
       " 'Loaded Lux vs. Calicoe',\n",
       " 'Alone And Drinking Under The Moon',\n",
       " 'Ode: Intimations of Immortality from Recollections of Early Childhood Immortality Ode',\n",
       " 'The Case for the Fat Startup',\n",
       " 'A Thanksgiving Prayer',\n",
       " 'I Am Vertical',\n",
       " 'The Map',\n",
       " 'The Gentleman of Shalott',\n",
       " 'The Sandpiper',\n",
       " 'Plowmen',\n",
       " 'The Tell-Tale Heart',\n",
       " 'Information Man',\n",
       " 'Ars Poetica',\n",
       " 'An Open Letter To Juan Williams',\n",
       " 'My Last Duchess',\n",
       " 'Two Sisters of Persephone',\n",
       " 'An Open Letter From Mos Def',\n",
       " 'Who Killed the Jay-Z Jeep?',\n",
       " 'America',\n",
       " '1000 Words',\n",
       " 'Ulysses Chap. 1 - Telemachus',\n",
       " 'Ulysses Chap. 2 - Nestor',\n",
       " 'Reply',\n",
       " 'The Forge',\n",
       " 'Those Winter Sundays',\n",
       " 'Ulysses Chap. 3 - Proteus',\n",
       " 'Ulysses Chap. 4 - Calypso',\n",
       " 'Ulysses Chap. 5 - Lotus Eaters',\n",
       " 'Ulysses Chap. 6 - Hades',\n",
       " 'Ulysses Chap. 7 - Aeolus',\n",
       " 'Ulysses Chap. 8 - Lestrygonians',\n",
       " 'Ulysses Chap. 9 - Scylla and Charybdis',\n",
       " 'Lunar Baedeker',\n",
       " 'On Imagination',\n",
       " 'No Church in the Wild',\n",
       " 'Ulysses Chap. 10 - Wandering Rocks',\n",
       " 'Ulysses Chap. 11 - Sirens',\n",
       " 'Paris 7 A.M.',\n",
       " 'Ulysses Chap. 13 - Nausicaa',\n",
       " 'As I Lay Dying Excerpt',\n",
       " 'A Sane Revolution',\n",
       " 'Shake The Dust',\n",
       " 'Messages',\n",
       " 'Iron John',\n",
       " 'The Shepherd Boy',\n",
       " 'Resumé',\n",
       " 'Pull It Up',\n",
       " 'To My Despoiler',\n",
       " 'Macbeth Act 1 Scene 1',\n",
       " 'Macbeth Act 1 Scene 2',\n",
       " 'Madame la Fleurie',\n",
       " 'The Poor Man and the Rich Man',\n",
       " 'Deaths Messengers',\n",
       " 'Loving v. Virginia',\n",
       " 'Want A Natty',\n",
       " 'Smalltalk',\n",
       " 'Analysis of The Waste Land',\n",
       " 'Politics and the English Language',\n",
       " 'The Hunting of the Snark',\n",
       " 'Tradition and the Individual Talent',\n",
       " 'Checking Out Me History',\n",
       " 'The Fall of the House of Usher',\n",
       " 'Meditation XVII No man is an island',\n",
       " 'FIMILE',\n",
       " 'Matthew 1',\n",
       " 'Hamlet Act 1 Scene 1',\n",
       " 'Hamlet Act 1 Scene 2',\n",
       " 'Hamlet Act 1 Scene 3',\n",
       " 'Hamlet Act 1 Scene 4',\n",
       " 'Hamlet Act 1 Scene 5',\n",
       " 'Hamlet Act 2 Scene 1',\n",
       " 'Hamlet Act 2 Scene 2',\n",
       " 'Hamlet Act 3 Scene 1',\n",
       " 'Macbeth Act 1 Scene 3',\n",
       " 'Hamlet Act 3 Scene 2',\n",
       " 'Macbeth Act 1 Scene 4',\n",
       " 'Hamlet Act 3 Scene 3',\n",
       " 'Macbeth Act 1 Scene 5',\n",
       " 'Macbeth Act 1 Scene 6',\n",
       " 'Macbeth Act 1 Scene 7',\n",
       " 'Macbeth Act 2 Scene 1',\n",
       " 'Macbeth Act 2 Scene 2',\n",
       " 'Hamlet Act 3 Scene 4',\n",
       " 'Hamlet Act 4 Scene 1',\n",
       " 'Hamlet Act 4 Scene 2',\n",
       " 'Macbeth Act 2 Scene 3',\n",
       " 'Hamlet Act 4 Scene 3',\n",
       " 'Hamlet Act 4 Scene 4',\n",
       " 'Macbeth Act 2 Scene 4',\n",
       " 'Macbeth Act 3 Scene 1',\n",
       " 'Hamlet Act 4 Scene 5',\n",
       " 'Hamlet Act 4 Scene 6',\n",
       " 'Hamlet Act 4 Scene 7',\n",
       " 'Macbeth Act 3 Scene 2',\n",
       " 'Hamlet Act 5 Scene 1',\n",
       " 'Macbeth Act 3 Scene 3',\n",
       " 'Hamlet Act 5 Scene 2',\n",
       " 'Macbeth Act 3 Scene 4',\n",
       " 'Macbeth Act 3 Scene 5',\n",
       " 'Macbeth Act 3 Scene 6',\n",
       " 'Macbeth Act 4 Scene 1',\n",
       " 'Macbeth Act 4 Scene 2',\n",
       " 'Macbeth Act 4 Scene 3',\n",
       " 'Macbeth Act 5 Scene 1',\n",
       " 'Macbeth Act 5 Scene 2',\n",
       " 'Macbeth Act 5 Scene 3',\n",
       " 'Here Am I',\n",
       " 'Macbeth Act 5 Scene 4',\n",
       " 'Macbeth Act 5 Scene 5',\n",
       " 'Macbeth Act 5 Scene 6',\n",
       " 'Macbeth Act 5 Scene 7',\n",
       " 'Glossary of Literary Terms',\n",
       " 'Macbeth Act 5 Scene 8 Final Scene',\n",
       " 'Inferno V:129',\n",
       " 'Open Letter to Westboro Baptist Church',\n",
       " 'Othello Act 1 Scene 2',\n",
       " 'Othello Act 1 Scene 3',\n",
       " 'Othello Act 2 Scene 1',\n",
       " 'Othello Act 2 Scene 2',\n",
       " 'Othello Act 2 Scene 3',\n",
       " 'Othello Act 3 Scene 1',\n",
       " 'Othello Act 3 Scene 2',\n",
       " 'Othello Act 3 Scene 3',\n",
       " 'Othello Act 3 Scene 4',\n",
       " 'Othello Act 4 Scene 1',\n",
       " 'Othello Act 4 Scene 2',\n",
       " 'Othello Act 4 Scene 3',\n",
       " 'After great pain a formal feeling comes J341 F372',\n",
       " 'Othello Act 5 Scene 1',\n",
       " 'Othello Act 5 Scene 2',\n",
       " 'Romeo and Juliet Act 1 Prologue',\n",
       " 'Romeo and Juliet Act 1 Scene 1',\n",
       " 'Mathew 7',\n",
       " 'Romeo and Juliet Act 1 Scene 2',\n",
       " 'Romeo and Juliet Act 1 Scene 3',\n",
       " 'Luke 2',\n",
       " 'Romeo and Juliet Act 1 Scene 4',\n",
       " 'Romeo and Juliet Act 1 Scene 5',\n",
       " 'Romeo and Juliet Act 2 Prologue',\n",
       " 'Romeo and Juliet Act 2 Scene 1',\n",
       " 'King Lear Act 1 Scene 1',\n",
       " 'Romeo and Juliet Act 2 Scene 2 The Balcony Scene',\n",
       " 'Romeo and Juliet Act 2 Scene 3',\n",
       " 'King Lear Act 1 Scene 2',\n",
       " 'Romeo and Juliet Act 2 Scene 4',\n",
       " 'Romeo and Juliet Act 2 Scene 5',\n",
       " 'Romeo and Juliet Act 2 Scene 6',\n",
       " 'King Lear Act 1 Scene 3',\n",
       " 'King Lear Act 1 Scene 4',\n",
       " 'Romeo and Juliet Act 3 Scene 1',\n",
       " 'Romeo and Juliet Act 3 Scene 2',\n",
       " 'Romeo and Juliet Act 3 Scene 3',\n",
       " 'Romeo and Juliet Act 3 Scene 4',\n",
       " 'King Lear Act 1 Scene 5',\n",
       " 'Romeo and Juliet Act 3 Scene 5',\n",
       " 'King Lear Act 2 Scene 1',\n",
       " 'King Lear Act 2 Scene 2',\n",
       " 'Romeo and Juliet Act 4 Scene 1',\n",
       " 'King Lear Act 2 Scene 3',\n",
       " 'Romeo and Juliet Act 4 Scene 2',\n",
       " 'Romeo and Juliet Act 4 Scene 3',\n",
       " 'Romeo and Juliet Act 4 Scene 4',\n",
       " 'King Lear Act 2 Scene 4',\n",
       " 'King Lear Act 3 Scene 1',\n",
       " 'Romeo and Juliet Act 4 Scene 5',\n",
       " 'King Lear Act 3 Scene 2',\n",
       " 'King Lear Act 3 Scene 3',\n",
       " 'Romeo and Juliet Act 5 Scene 1',\n",
       " 'Romeo and Juliet Act 5 Scene 2',\n",
       " 'King Lear Act 3 Scene 4',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# misc lyrics category contain unexpected items\n",
    "misc_lyrics = lyrics[lyrics['tag'] == 'misc']\n",
    "misc_lyrics['title'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "\n",
      "\n",
      "2012 Presidential Acceptance Speech----------------------------\n",
      "Thank you so much. Tonight, more than 200 years after a former colony won the right to determine its own destiny, the task of perfecting our union moves forward. It moves forward because of you. It moves forward because you reaffirmed the spirit that has triumphed over war and depression, the spirit that has lifted this country from the depths of despair to the great heights of hope, the belief that while each of us will pursue our own individual dreams, we are an American family and we rise or fall together as one nation and as one people.\n",
      "\n",
      "Tonight, in this election, you, the American people, reminded us that while our road has been hard, while our journey has been long, we have picked ourselves up, we have fought our way back, and we know in our hearts that for the United States of Ameri\n",
      "\n",
      "Treaty of Versailles Articles 231 - 247----------------------------\n",
      "Reparations\n",
      "\n",
      "PART VIII\n",
      "\n",
      "SECTION I\n",
      "\n",
      "GENERAL PROVISIONS\n",
      "\n",
      "Article 231\n",
      "\n",
      "The Allied and Associated Governments affirm and Germany accepts the responsibility of Germany and her allies for causing all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies\n",
      "\n",
      "Article 232\n",
      "\n",
      "The Allied and Associated Governments recognise that the resources of Germany are not adequate, after taking into account permanent diminutions of such resources which will result from other provisions of the present Treaty, to make complete reparation for all such loss and damage\n",
      "\n",
      "The Allied and Associated Governments, however, require, and Germany undertakes, that she will make compensation\n",
      "\n",
      "Hansel and Gretel----------------------------\n",
      "Hard by a great forest dwelt a poor wood-cutter with his wife\n",
      "and his two children.  The boy was called Hansel and the\n",
      "girl Gretel.  He had little to bite and to break, and once when\n",
      "great dearth fell on the land, he could no longer procure even daily\n",
      "bread.  Now when he thought over this by night in his bed, and\n",
      "tossed about in his anxiety, he groaned and said to his wife, what\n",
      "is to become of us.  How are we to feed our poor children, when\n",
      "we no longer have anything even for ourselves.  I'll tell you what,\n",
      "husband, answered the woman, early to-morrow morning we\n",
      "will take the children out into the forest to where it is the\n",
      "thickest.  There we will light a fire for them, and give each of\n",
      "them one more piece of bread, and then we will go to our work and leave them alone.  They will not find\n",
      "\n",
      "The Picture of Dorian Gray Preface----------------------------\n",
      "        THE artist is the creator of beautiful things. To reveal art and conceal the artist is art's aim.\n",
      "\n",
      "        The critic is he who can translate into another manner or a new material his impression of beautiful things.\n",
      "\n",
      "        The highest as the lowest form of criticism is a mode of autobiography.\n",
      "\n",
      "        Those who find ugly meanings in beautiful things are corrupt without being charming. This is a fault.\n",
      "\n",
      "        Those who find beautiful meanings in beautiful things are the cultivated. For these there is hope.\n",
      "\n",
      "        They are the elect to whom beautiful things mean only beauty.\n",
      "\n",
      "        There is no such thing as a moral or an immoral book. Books are well written, or badly written. That is all.\n",
      "\n",
      "        The nineteenth century dislike of realism is the rage of Caliban seeing his ow\n"
     ]
    }
   ],
   "source": [
    "print('Examples:\\n')\n",
    "print('\\n2012 Presidential Acceptance Speech----------------------------')\n",
    "print(misc_lyrics[misc_lyrics['title'] == '2012 Presidential Acceptance Speech']['lyrics'].item()[0:800])\n",
    "print('\\nTreaty of Versailles Articles 231 - 247----------------------------')\n",
    "print(misc_lyrics[misc_lyrics['title'] == 'Treaty of Versailles Articles 231 - 247']['lyrics'].item()[0:800])\n",
    "print('\\nHansel and Gretel----------------------------')\n",
    "print(misc_lyrics[misc_lyrics['title'] == 'Hansel and Gretel']['lyrics'].item()[0:800])\n",
    "print('\\nThe Picture of Dorian Gray Preface----------------------------')\n",
    "print(misc_lyrics[misc_lyrics['title'] == 'The Picture of Dorian Gray Preface']['lyrics'].item()[0:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>tag</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Lollipop Remix</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nHaha\\nUh-huh\\nNo homo (Young Mula, baby!)\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Losing Weight Pt. 2</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nAyo, fuck losing weight\\nI'm back on these h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>A Milli</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>rap</td>\n",
       "      <td>It's time to get money\\nI got a million reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>Warning</td>\n",
       "      <td>The Notorious B.I.G.</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\n\\n\\nWho the fuck is this, pagin' me at 5:46 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>On My Own</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nYeah, hit me with the snares, Fresh\\nYeah-ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84211</th>\n",
       "      <td>7864996</td>\n",
       "      <td>SUHO - Grey Suit English Translation</td>\n",
       "      <td>Genius English Translations</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nI feel faded, I'm not used to this new color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84212</th>\n",
       "      <td>7865871</td>\n",
       "      <td>HEARTFIRST</td>\n",
       "      <td>Kelsea Ballerini</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nMet him at a party\\nAccidentally brushed his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84213</th>\n",
       "      <td>7868429</td>\n",
       "      <td>Never Had a Chance</td>\n",
       "      <td>Katherine Li</td>\n",
       "      <td>pop</td>\n",
       "      <td>\\nWhy am I still here and waiting for us to ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84214</th>\n",
       "      <td>7871730</td>\n",
       "      <td>Red Alert Boss Metal Zone</td>\n",
       "      <td>Clutch</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nI used to work for Doc Tyrell\\nIt didn't end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84215</th>\n",
       "      <td>7872000</td>\n",
       "      <td>Unpredictable</td>\n",
       "      <td>DESTIN CONRAD &amp; Kiana Led</td>\n",
       "      <td>rb</td>\n",
       "      <td>\\nSee with you, bae\\nYou make the room spin ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70178 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                 title  \\\n",
       "0            7                        Lollipop Remix   \n",
       "1           19                   Losing Weight Pt. 2   \n",
       "2           26                               A Milli   \n",
       "3           29                               Warning   \n",
       "4           40                             On My Own   \n",
       "...        ...                                   ...   \n",
       "84211  7864996  SUHO - Grey Suit English Translation   \n",
       "84212  7865871                            HEARTFIRST   \n",
       "84213  7868429                    Never Had a Chance   \n",
       "84214  7871730             Red Alert Boss Metal Zone   \n",
       "84215  7872000                         Unpredictable   \n",
       "\n",
       "                            artist      tag  \\\n",
       "0                        Lil Wayne      rap   \n",
       "1                          Cam'ron      rap   \n",
       "2                         Fabolous      rap   \n",
       "3             The Notorious B.I.G.      rap   \n",
       "4                        Lil Wayne      rap   \n",
       "...                            ...      ...   \n",
       "84211  Genius English Translations     rock   \n",
       "84212             Kelsea Ballerini  country   \n",
       "84213                 Katherine Li      pop   \n",
       "84214                       Clutch     rock   \n",
       "84215    DESTIN CONRAD & Kiana Led       rb   \n",
       "\n",
       "                                                  lyrics  \n",
       "0      \\nHaha\\nUh-huh\\nNo homo (Young Mula, baby!)\\nI...  \n",
       "1      \\nAyo, fuck losing weight\\nI'm back on these h...  \n",
       "2      It's time to get money\\nI got a million reason...  \n",
       "3      \\n\\n\\nWho the fuck is this, pagin' me at 5:46 ...  \n",
       "4      \\nYeah, hit me with the snares, Fresh\\nYeah-ye...  \n",
       "...                                                  ...  \n",
       "84211  \\nI feel faded, I'm not used to this new color...  \n",
       "84212  \\nMet him at a party\\nAccidentally brushed his...  \n",
       "84213  \\nWhy am I still here and waiting for us to ha...  \n",
       "84214  \\nI used to work for Doc Tyrell\\nIt didn't end...  \n",
       "84215  \\nSee with you, bae\\nYou make the room spin ar...  \n",
       "\n",
       "[70178 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all rows with the 'misc' tag\n",
    "lyrics = lyrics.loc[lyrics['tag'] != 'misc']\n",
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>tag</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Lollipop Remix</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nHaha\\nUh-huh\\nNo homo (Young Mula, baby!)\\nI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Losing Weight Pt. 2</td>\n",
       "      <td>Cam'ron</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nAyo, fuck losing weight\\nI'm back on these h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>A Milli</td>\n",
       "      <td>Fabolous</td>\n",
       "      <td>rap</td>\n",
       "      <td>It's time to get money\\nI got a million reason...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>Warning</td>\n",
       "      <td>The Notorious B.I.G.</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\n\\n\\nWho the fuck is this, pagin' me at 5:46 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>On My Own</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nYeah, hit me with the snares, Fresh\\nYeah-ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84211</th>\n",
       "      <td>7864996</td>\n",
       "      <td>SUHO - Grey Suit English Translation</td>\n",
       "      <td>Genius English Translations</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nI feel faded, I'm not used to this new color...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84212</th>\n",
       "      <td>7865871</td>\n",
       "      <td>HEARTFIRST</td>\n",
       "      <td>Kelsea Ballerini</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nMet him at a party\\nAccidentally brushed his...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84213</th>\n",
       "      <td>7868429</td>\n",
       "      <td>Never Had a Chance</td>\n",
       "      <td>Katherine Li</td>\n",
       "      <td>pop</td>\n",
       "      <td>\\nWhy am I still here and waiting for us to ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84214</th>\n",
       "      <td>7871730</td>\n",
       "      <td>Red Alert Boss Metal Zone</td>\n",
       "      <td>Clutch</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nI used to work for Doc Tyrell\\nIt didn't end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84215</th>\n",
       "      <td>7872000</td>\n",
       "      <td>Unpredictable</td>\n",
       "      <td>DESTIN CONRAD &amp; Kiana Led</td>\n",
       "      <td>rb</td>\n",
       "      <td>\\nSee with you, bae\\nYou make the room spin ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70178 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                 title  \\\n",
       "0            7                        Lollipop Remix   \n",
       "1           19                   Losing Weight Pt. 2   \n",
       "2           26                               A Milli   \n",
       "3           29                               Warning   \n",
       "4           40                             On My Own   \n",
       "...        ...                                   ...   \n",
       "84211  7864996  SUHO - Grey Suit English Translation   \n",
       "84212  7865871                            HEARTFIRST   \n",
       "84213  7868429                    Never Had a Chance   \n",
       "84214  7871730             Red Alert Boss Metal Zone   \n",
       "84215  7872000                         Unpredictable   \n",
       "\n",
       "                            artist      tag  \\\n",
       "0                        Lil Wayne      rap   \n",
       "1                          Cam'ron      rap   \n",
       "2                         Fabolous      rap   \n",
       "3             The Notorious B.I.G.      rap   \n",
       "4                        Lil Wayne      rap   \n",
       "...                            ...      ...   \n",
       "84211  Genius English Translations     rock   \n",
       "84212             Kelsea Ballerini  country   \n",
       "84213                 Katherine Li      pop   \n",
       "84214                       Clutch     rock   \n",
       "84215    DESTIN CONRAD & Kiana Led       rb   \n",
       "\n",
       "                                                  lyrics  \n",
       "0      \\nHaha\\nUh-huh\\nNo homo (Young Mula, baby!)\\nI...  \n",
       "1      \\nAyo, fuck losing weight\\nI'm back on these h...  \n",
       "2      It's time to get money\\nI got a million reason...  \n",
       "3      \\n\\n\\nWho the fuck is this, pagin' me at 5:46 ...  \n",
       "4      \\nYeah, hit me with the snares, Fresh\\nYeah-ye...  \n",
       "...                                                  ...  \n",
       "84211  \\nI feel faded, I'm not used to this new color...  \n",
       "84212  \\nMet him at a party\\nAccidentally brushed his...  \n",
       "84213  \\nWhy am I still here and waiting for us to ha...  \n",
       "84214  \\nI used to work for Doc Tyrell\\nIt didn't end...  \n",
       "84215  \\nSee with you, bae\\nYou make the room spin ar...  \n",
       "\n",
       "[70178 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 70178 entries, 0 to 84215\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      70178 non-null  int64 \n",
      " 1   title   70178 non-null  object\n",
      " 2   artist  70178 non-null  object\n",
      " 3   tag     70178 non-null  object\n",
      " 4   lyrics  70178 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.2+ MB\n"
     ]
    }
   ],
   "source": [
    "lyrics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "lyrics.to_csv('cleaned_lyrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 42.94it/s]\n"
     ]
    }
   ],
   "source": [
    "lyrics = pd.read_csv('cleaned_lyrics.csv')\n",
    "lyrics['length'] = lyrics['lyrics'].mapply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock -----------------------------------\n",
      "mean length: 1190.5120769504808\n",
      "max length: 12661\n",
      "min length: 101\n",
      "pop -----------------------------------\n",
      "mean length: 1361.075591336563\n",
      "max length: 46201\n",
      "min length: 100\n",
      "rb -----------------------------------\n",
      "mean length: 1654.6956893480585\n",
      "max length: 73878\n",
      "min length: 101\n",
      "country -----------------------------------\n",
      "mean length: 1295.5785836420632\n",
      "max length: 16283\n",
      "min length: 100\n",
      "rap -----------------------------------\n",
      "mean length: 2673.737674551154\n",
      "max length: 79626\n",
      "min length: 112\n"
     ]
    }
   ],
   "source": [
    "mean_lengths = []\n",
    "max_lengths = []\n",
    "min_lengths = []\n",
    "\n",
    "for tag in lyrics['tag'].unique():\n",
    "    tag_lengths = lyrics[lyrics['tag'] == tag]['length']\n",
    "\n",
    "    mean_len = tag_lengths.mean()\n",
    "    max_len = tag_lengths.max()\n",
    "    min_len = tag_lengths.min()\n",
    "    \n",
    "    mean_lengths.append(mean_len)\n",
    "    max_lengths.append(max_len)\n",
    "    min_lengths.append(min_len)\n",
    "\n",
    "    print(f'{tag} -----------------------------------')\n",
    "    print(f'mean length: {mean_len}\\nmax length: {max_len}\\nmin length: {min_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:58<00:00,  1.46s/it]\n",
      "100%|██████████| 40/40 [01:15<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock -----------------------------------\n",
      "mean length: 267.00263626647666\n",
      "max length: 2881\n",
      "min length: 20\n",
      "pop -----------------------------------\n",
      "mean length: 315.6692077514962\n",
      "max length: 7787\n",
      "min length: 18\n",
      "rb -----------------------------------\n",
      "mean length: 402.37691485571787\n",
      "max length: 18087\n",
      "min length: 21\n",
      "country -----------------------------------\n",
      "mean length: 294.30443146195495\n",
      "max length: 3674\n",
      "min length: 24\n",
      "rap -----------------------------------\n",
      "mean length: 627.5619834710744\n",
      "max length: 16129\n",
      "min length: 24\n"
     ]
    }
   ],
   "source": [
    "lyrics['tokens'] = lyrics['lyrics'].mapply(lambda x: nltk.word_tokenize(x))\n",
    "lyrics['n_tokens'] = lyrics['tokens'].mapply(lambda x: len(x))\n",
    "\n",
    "mean_lengths = []\n",
    "max_lengths = []\n",
    "min_lengths = []\n",
    "\n",
    "for tag in lyrics['tag'].unique():\n",
    "    tag_lengths = lyrics[lyrics['tag'] == tag]['n_tokens']\n",
    "\n",
    "    mean_len = tag_lengths.mean()\n",
    "    max_len = tag_lengths.max()\n",
    "    min_len = tag_lengths.min()\n",
    "\n",
    "    mean_lengths.append(mean_len)\n",
    "    max_lengths.append(max_len)\n",
    "    min_lengths.append(min_len)\n",
    "\n",
    "    print(f'{tag} -----------------------------------')\n",
    "    print(f'mean length: {mean_len}\\nmax length: {max_len}\\nmin length: {min_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [01:28<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_tokens(tokens):\n",
    "    tokens = [token.lower() for token in tokens]  # Convert tokens to lowercase\n",
    "    tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "    tokens = [''.join(char for char in token if char not in string.punctuation) for token in tokens]  # Remove punctuation\n",
    "    tokens = [token for token in tokens if token.strip() != \"\"]  # Remove empty tokens\n",
    "    return tokens\n",
    "\n",
    "lyrics['filtered_tokens'] = lyrics['tokens'].mapply(process_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [could, hold, hold, icecaps, melting, transist...\n",
       "1        [nothing, could, ask, nothing, need, world, ba...\n",
       "2        [gon, pop, xanny, brown, skin, fatty, dasani, ...\n",
       "3        [heroes, gone, come, great, land, america, sti...\n",
       "4        [cheated, lied, said, love, cheated, lied, sai...\n",
       "                               ...                        \n",
       "70173    [another, song, reasons, stressed, melody, get...\n",
       "70174    [yeah, people, rapper, right, could, ever, var...\n",
       "70175    [hollygrove, go, hollywood, uhh, hollygrove, h...\n",
       "70176    [seen, men, look, think, see, like, think, mak...\n",
       "70177    [lay, awake, night, oh, low, troubled, get, jo...\n",
       "Name: filtered_tokens, Length: 70178, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics['filtered_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:33<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock -----------------------------------\n",
      "mean length: 112.55311720698255\n",
      "max length: 1230\n",
      "min length: 6\n",
      "pop -----------------------------------\n",
      "mean length: 130.9160729552579\n",
      "max length: 3914\n",
      "min length: 6\n",
      "rb -----------------------------------\n",
      "mean length: 161.3851086569291\n",
      "max length: 7009\n",
      "min length: 4\n",
      "country -----------------------------------\n",
      "mean length: 125.64826161299516\n",
      "max length: 1577\n",
      "min length: 5\n",
      "rap -----------------------------------\n",
      "mean length: 275.18509546879454\n",
      "max length: 7051\n",
      "min length: 8\n"
     ]
    }
   ],
   "source": [
    "lyrics['n_filtered_tokens'] = lyrics['filtered_tokens'].mapply(lambda x: len(x))\n",
    "\n",
    "mean_lengths = []\n",
    "max_lengths = []\n",
    "min_lengths = []\n",
    "\n",
    "for tag in lyrics['tag'].unique():\n",
    "    tag_lengths = lyrics[lyrics['tag'] == tag]['n_filtered_tokens']\n",
    "\n",
    "    mean_len = tag_lengths.mean()\n",
    "    max_len = tag_lengths.max()\n",
    "    min_len = tag_lengths.min()\n",
    "\n",
    "    mean_lengths.append(mean_len)\n",
    "    max_lengths.append(max_len)\n",
    "    min_lengths.append(min_len)\n",
    "\n",
    "    print(f'{tag} -----------------------------------')\n",
    "    print(f'mean length: {mean_len}\\nmax length: {max_len}\\nmin length: {min_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:39<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "lyrics['filtered_unique_tokens'] = lyrics['filtered_tokens'].mapply(lambda x: list(set(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [lost, want, window, could, little, burning, p...\n",
       "1        [could, say, jesa, hard, asked, beliya, seen, ...\n",
       "2        [trust, go, want, us, always, brown, neck, sit...\n",
       "3        [last, fair, nontax, want, land, read, us, put...\n",
       "4        [oh, keep, nothing, lied, try, somethng, cheat...\n",
       "                               ...                        \n",
       "70173    [last, light, go, lead, want, song, reason, ca...\n",
       "70174    [diaper, uh, go, want, respect, could, putting...\n",
       "70175    [hahahaha, pt, go, range, gun, californicate, ...\n",
       "70176    [look, drinkin, friend, tell, single, actin, h...\n",
       "70177    [oh, generation, even, decent, go, lie, baby, ...\n",
       "Name: filtered_unique_tokens, Length: 70178, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics['filtered_unique_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:19<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock -----------------------------------\n",
      "mean length: 59.746704666904165\n",
      "max length: 498\n",
      "min length: 2\n",
      "pop -----------------------------------\n",
      "mean length: 60.81575947563408\n",
      "max length: 1594\n",
      "min length: 1\n",
      "rb -----------------------------------\n",
      "mean length: 69.7464196651229\n",
      "max length: 1741\n",
      "min length: 2\n",
      "country -----------------------------------\n",
      "mean length: 67.34212026218296\n",
      "max length: 700\n",
      "min length: 4\n",
      "rap -----------------------------------\n",
      "mean length: 156.57566258193216\n",
      "max length: 2987\n",
      "min length: 2\n"
     ]
    }
   ],
   "source": [
    "lyrics['n_filtered_unique_tokens'] = lyrics['filtered_unique_tokens'].mapply(lambda x: len(x))\n",
    "\n",
    "mean_lengths = []\n",
    "max_lengths = []\n",
    "min_lengths = []\n",
    "\n",
    "for tag in lyrics['tag'].unique():\n",
    "    tag_lengths = lyrics[lyrics['tag'] == tag]['n_filtered_unique_tokens']\n",
    "\n",
    "    mean_len = tag_lengths.mean()\n",
    "    max_len = tag_lengths.max()\n",
    "    min_len = tag_lengths.min()\n",
    "\n",
    "    mean_lengths.append(mean_len)\n",
    "    max_lengths.append(max_len)\n",
    "    min_lengths.append(min_len)\n",
    "\n",
    "    print(f'{tag} -----------------------------------')\n",
    "    print(f'mean length: {mean_len}\\nmax length: {max_len}\\nmin length: {min_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>tag</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>n_filtered_tokens</th>\n",
       "      <th>filtered_unique_tokens</th>\n",
       "      <th>n_filtered_unique_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>359018</td>\n",
       "      <td>Gamma Ray</td>\n",
       "      <td>Beck</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nIf I could hold\\nHold out for now\\nWith thes...</td>\n",
       "      <td>724</td>\n",
       "      <td>[If, I, could, hold, Hold, out, for, now, With...</td>\n",
       "      <td>140</td>\n",
       "      <td>[could, hold, hold, icecaps, melting, transist...</td>\n",
       "      <td>72</td>\n",
       "      <td>[lost, want, window, could, little, burning, p...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3610742</td>\n",
       "      <td>Beliya</td>\n",
       "      <td>The Vamps</td>\n",
       "      <td>pop</td>\n",
       "      <td>Nothing more I could ask for\\nNothing more tha...</td>\n",
       "      <td>1419</td>\n",
       "      <td>[Nothing, more, I, could, ask, for, Nothing, m...</td>\n",
       "      <td>297</td>\n",
       "      <td>[nothing, could, ask, nothing, need, world, ba...</td>\n",
       "      <td>176</td>\n",
       "      <td>[could, say, jesa, hard, asked, beliya, seen, ...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4007667</td>\n",
       "      <td>Titanic</td>\n",
       "      <td>FRVRFRIDAY</td>\n",
       "      <td>rb</td>\n",
       "      <td>\\nShe gon pop a xanny\\nBrown skin with a fatty...</td>\n",
       "      <td>1559</td>\n",
       "      <td>[She, gon, pop, a, xanny, Brown, skin, with, a...</td>\n",
       "      <td>368</td>\n",
       "      <td>[gon, pop, xanny, brown, skin, fatty, dasani, ...</td>\n",
       "      <td>157</td>\n",
       "      <td>[trust, go, want, us, always, brown, neck, sit...</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>937615</td>\n",
       "      <td>Where Have All Our Heroes Gone</td>\n",
       "      <td>Bill Anderson</td>\n",
       "      <td>country</td>\n",
       "      <td>Where have all our heroes gone\\nWhat's come ov...</td>\n",
       "      <td>2864</td>\n",
       "      <td>[Where, have, all, our, heroes, gone, What, 's...</td>\n",
       "      <td>587</td>\n",
       "      <td>[heroes, gone, come, great, land, america, sti...</td>\n",
       "      <td>266</td>\n",
       "      <td>[last, fair, nontax, want, land, read, us, put...</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1523825</td>\n",
       "      <td>You Cheated</td>\n",
       "      <td>The Shields (Soul)</td>\n",
       "      <td>pop</td>\n",
       "      <td>You cheated, you lied\\nYou said that you love ...</td>\n",
       "      <td>474</td>\n",
       "      <td>[You, cheated, ,, you, lied, You, said, that, ...</td>\n",
       "      <td>118</td>\n",
       "      <td>[cheated, lied, said, love, cheated, lied, sai...</td>\n",
       "      <td>43</td>\n",
       "      <td>[oh, keep, nothing, lied, try, somethng, cheat...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70173</th>\n",
       "      <td>758875</td>\n",
       "      <td>My Story</td>\n",
       "      <td>Addison Road</td>\n",
       "      <td>pop</td>\n",
       "      <td>This is not another song about the reasons I a...</td>\n",
       "      <td>1070</td>\n",
       "      <td>[This, is, not, another, song, about, the, rea...</td>\n",
       "      <td>242</td>\n",
       "      <td>[another, song, reasons, stressed, melody, get...</td>\n",
       "      <td>87</td>\n",
       "      <td>[last, light, go, lead, want, song, reason, ca...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70174</th>\n",
       "      <td>81170</td>\n",
       "      <td>8 Mile Road</td>\n",
       "      <td>Jon Connor</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nYeah\\nThe People's Rapper (That's right) Tha...</td>\n",
       "      <td>3744</td>\n",
       "      <td>[Yeah, The, People, 's, Rapper, (, That, 's, r...</td>\n",
       "      <td>857</td>\n",
       "      <td>[yeah, people, rapper, right, could, ever, var...</td>\n",
       "      <td>357</td>\n",
       "      <td>[diaper, uh, go, want, respect, could, putting...</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70175</th>\n",
       "      <td>685108</td>\n",
       "      <td>HollyWeezy</td>\n",
       "      <td>Lil Wayne</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nToo Hollygrove to go Hollywood\\nUhh\\n\\n\\nHol...</td>\n",
       "      <td>4316</td>\n",
       "      <td>[Too, Hollygrove, to, go, Hollywood, Uhh, Holl...</td>\n",
       "      <td>912</td>\n",
       "      <td>[hollygrove, go, hollywood, uhh, hollygrove, h...</td>\n",
       "      <td>419</td>\n",
       "      <td>[hahahaha, pt, go, range, gun, californicate, ...</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70176</th>\n",
       "      <td>1390290</td>\n",
       "      <td>Shes Actin Single Im Drinkin Doubles</td>\n",
       "      <td>Gary Stewart</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nI've seen men look at her before\\nAnd they t...</td>\n",
       "      <td>875</td>\n",
       "      <td>[I, 've, seen, men, look, at, her, before, And...</td>\n",
       "      <td>208</td>\n",
       "      <td>[seen, men, look, think, see, like, think, mak...</td>\n",
       "      <td>74</td>\n",
       "      <td>[look, drinkin, friend, tell, single, actin, h...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70177</th>\n",
       "      <td>333246</td>\n",
       "      <td>Double Trouble</td>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nLay awake at night\\nOh so low, just so troub...</td>\n",
       "      <td>597</td>\n",
       "      <td>[Lay, awake, at, night, Oh, so, low, ,, just, ...</td>\n",
       "      <td>134</td>\n",
       "      <td>[lay, awake, night, oh, low, troubled, get, jo...</td>\n",
       "      <td>60</td>\n",
       "      <td>[oh, generation, even, decent, go, lie, baby, ...</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70178 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                 title              artist  \\\n",
       "0       359018                             Gamma Ray                Beck   \n",
       "1      3610742                                Beliya           The Vamps   \n",
       "2      4007667                               Titanic          FRVRFRIDAY   \n",
       "3       937615        Where Have All Our Heroes Gone       Bill Anderson   \n",
       "4      1523825                           You Cheated  The Shields (Soul)   \n",
       "...        ...                                   ...                 ...   \n",
       "70173   758875                              My Story        Addison Road   \n",
       "70174    81170                           8 Mile Road          Jon Connor   \n",
       "70175   685108                            HollyWeezy           Lil Wayne   \n",
       "70176  1390290  Shes Actin Single Im Drinkin Doubles        Gary Stewart   \n",
       "70177   333246                        Double Trouble        Eric Clapton   \n",
       "\n",
       "           tag                                             lyrics  length  \\\n",
       "0         rock  \\nIf I could hold\\nHold out for now\\nWith thes...     724   \n",
       "1          pop  Nothing more I could ask for\\nNothing more tha...    1419   \n",
       "2           rb  \\nShe gon pop a xanny\\nBrown skin with a fatty...    1559   \n",
       "3      country  Where have all our heroes gone\\nWhat's come ov...    2864   \n",
       "4          pop  You cheated, you lied\\nYou said that you love ...     474   \n",
       "...        ...                                                ...     ...   \n",
       "70173      pop  This is not another song about the reasons I a...    1070   \n",
       "70174      rap  \\nYeah\\nThe People's Rapper (That's right) Tha...    3744   \n",
       "70175      rap  \\nToo Hollygrove to go Hollywood\\nUhh\\n\\n\\nHol...    4316   \n",
       "70176  country  \\nI've seen men look at her before\\nAnd they t...     875   \n",
       "70177     rock  \\nLay awake at night\\nOh so low, just so troub...     597   \n",
       "\n",
       "                                                  tokens  n_tokens  \\\n",
       "0      [If, I, could, hold, Hold, out, for, now, With...       140   \n",
       "1      [Nothing, more, I, could, ask, for, Nothing, m...       297   \n",
       "2      [She, gon, pop, a, xanny, Brown, skin, with, a...       368   \n",
       "3      [Where, have, all, our, heroes, gone, What, 's...       587   \n",
       "4      [You, cheated, ,, you, lied, You, said, that, ...       118   \n",
       "...                                                  ...       ...   \n",
       "70173  [This, is, not, another, song, about, the, rea...       242   \n",
       "70174  [Yeah, The, People, 's, Rapper, (, That, 's, r...       857   \n",
       "70175  [Too, Hollygrove, to, go, Hollywood, Uhh, Holl...       912   \n",
       "70176  [I, 've, seen, men, look, at, her, before, And...       208   \n",
       "70177  [Lay, awake, at, night, Oh, so, low, ,, just, ...       134   \n",
       "\n",
       "                                         filtered_tokens  n_filtered_tokens  \\\n",
       "0      [could, hold, hold, icecaps, melting, transist...                 72   \n",
       "1      [nothing, could, ask, nothing, need, world, ba...                176   \n",
       "2      [gon, pop, xanny, brown, skin, fatty, dasani, ...                157   \n",
       "3      [heroes, gone, come, great, land, america, sti...                266   \n",
       "4      [cheated, lied, said, love, cheated, lied, sai...                 43   \n",
       "...                                                  ...                ...   \n",
       "70173  [another, song, reasons, stressed, melody, get...                 87   \n",
       "70174  [yeah, people, rapper, right, could, ever, var...                357   \n",
       "70175  [hollygrove, go, hollywood, uhh, hollygrove, h...                419   \n",
       "70176  [seen, men, look, think, see, like, think, mak...                 74   \n",
       "70177  [lay, awake, night, oh, low, troubled, get, jo...                 60   \n",
       "\n",
       "                                  filtered_unique_tokens  \\\n",
       "0      [lost, want, window, could, little, burning, p...   \n",
       "1      [could, say, jesa, hard, asked, beliya, seen, ...   \n",
       "2      [trust, go, want, us, always, brown, neck, sit...   \n",
       "3      [last, fair, nontax, want, land, read, us, put...   \n",
       "4      [oh, keep, nothing, lied, try, somethng, cheat...   \n",
       "...                                                  ...   \n",
       "70173  [last, light, go, lead, want, song, reason, ca...   \n",
       "70174  [diaper, uh, go, want, respect, could, putting...   \n",
       "70175  [hahahaha, pt, go, range, gun, californicate, ...   \n",
       "70176  [look, drinkin, friend, tell, single, actin, h...   \n",
       "70177  [oh, generation, even, decent, go, lie, baby, ...   \n",
       "\n",
       "       n_filtered_unique_tokens  \n",
       "0                            48  \n",
       "1                            39  \n",
       "2                           107  \n",
       "3                           202  \n",
       "4                            17  \n",
       "...                         ...  \n",
       "70173                        57  \n",
       "70174                       199  \n",
       "70175                       247  \n",
       "70176                        44  \n",
       "70177                        33  \n",
       "\n",
       "[70178 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock:  0.2\n",
      "pop:  0.2\n",
      "rb:  0.2\n",
      "country:  0.2\n",
      "rap:  0.2\n"
     ]
    }
   ],
   "source": [
    "for tag in lyrics['tag'].unique():\n",
    "    tag_perc = len(lyrics[lyrics['tag'] == tag]) / len(lyrics)\n",
    "    print(f'{tag}: {tag_perc: .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the augmented dataset\n",
    "lyrics['tokens'] = lyrics['tokens'].apply(json.dumps)\n",
    "lyrics['filtered_tokens'] = lyrics['filtered_tokens'].apply(json.dumps)\n",
    "lyrics['filtered_unique_tokens'] = lyrics['filtered_unique_tokens'].apply(json.dumps)\n",
    "lyrics.to_csv('augmented_lyrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv('augmented_lyrics.csv')\n",
    "lyrics['tokens'] = lyrics['tokens'].apply(json.loads)\n",
    "lyrics['filtered_tokens'] = lyrics['filtered_tokens'].apply(json.loads)\n",
    "lyrics['filtered_unique_tokens'] = lyrics['filtered_unique_tokens'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a tutorial about the Transformers library in HuggingFace',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9194583296775818, 0.04611356183886528, 0.034428082406520844]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"valhalla/distilbart-mnli-12-3\")\n",
    "classifier(\n",
    "    \"This is a tutorial about the Transformers library in HuggingFace\",\n",
    "    candidate_labels=[\"politics\", \"education\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lyrics['lyrics'].to_list())\n",
    "type(list(lyrics['tag'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlyrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlyrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcandidate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlyrics\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m predictions\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:206\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline.__call__\u001b[0;34m(self, sequences, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to understand extra arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1187\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1184\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1185\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1186\u001b[0m     )\n\u001b[0;32m-> 1187\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:266\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 266\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/pipelines/base.py:1112\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1111\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1112\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/pipelines/zero_shot_classification.py:229\u001b[0m, in \u001b[0;36mZeroShotClassificationPipeline._forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    228\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_label\u001b[39m\u001b[38;5;124m\"\u001b[39m: candidate_label,\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sequence,\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moutputs,\n\u001b[1;32m    236\u001b[0m }\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1891\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1888\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing input embeddings is currently not supported for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1889\u001b[0m     )\n\u001b[0;32m-> 1891\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1901\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1902\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1903\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1904\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1907\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# last hidden state\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m eos_mask \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meos_token_id)\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1599\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1596\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1599\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:1207\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1200\u001b[0m             encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1201\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1204\u001b[0m             output_attentions,\n\u001b[1;32m   1205\u001b[0m         )\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1207\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:662\u001b[0m, in \u001b[0;36mBartEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;124;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    661\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 662\u001b[0m hidden_states, attn_weights, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    668\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    669\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py:612\u001b[0m, in \u001b[0;36mBartSdpaAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m--> 612\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/HLT/project/code/hlt_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = classifier(\n",
    "    lyrics['lyrics'].to_list(),\n",
    "    candidate_labels=lyrics['tag'].unique(),\n",
    ")\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING SET:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42106 entries, 0 to 42105\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   id                        42106 non-null  int64 \n",
      " 1   title                     42106 non-null  object\n",
      " 2   artist                    42106 non-null  object\n",
      " 3   tag                       42106 non-null  object\n",
      " 4   lyrics                    42106 non-null  object\n",
      " 5   length                    42106 non-null  int64 \n",
      " 6   tokens                    42106 non-null  object\n",
      " 7   n_tokens                  42106 non-null  int64 \n",
      " 8   filtered_tokens           42106 non-null  object\n",
      " 9   n_filtered_tokens         42106 non-null  int64 \n",
      " 10  filtered_unique_tokens    42106 non-null  object\n",
      " 11  n_filtered_unique_tokens  42106 non-null  int64 \n",
      "dtypes: int64(5), object(7)\n",
      "memory usage: 3.9+ MB\n",
      "None\n",
      "\n",
      "VALIDATION SET:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14036 entries, 0 to 14035\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   id                        14036 non-null  int64 \n",
      " 1   title                     14036 non-null  object\n",
      " 2   artist                    14036 non-null  object\n",
      " 3   tag                       14036 non-null  object\n",
      " 4   lyrics                    14036 non-null  object\n",
      " 5   length                    14036 non-null  int64 \n",
      " 6   tokens                    14036 non-null  object\n",
      " 7   n_tokens                  14036 non-null  int64 \n",
      " 8   filtered_tokens           14036 non-null  object\n",
      " 9   n_filtered_tokens         14036 non-null  int64 \n",
      " 10  filtered_unique_tokens    14036 non-null  object\n",
      " 11  n_filtered_unique_tokens  14036 non-null  int64 \n",
      "dtypes: int64(5), object(7)\n",
      "memory usage: 1.3+ MB\n",
      "None\n",
      "\n",
      "TEST SET:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14036 entries, 0 to 14035\n",
      "Data columns (total 12 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   id                        14036 non-null  int64 \n",
      " 1   title                     14036 non-null  object\n",
      " 2   artist                    14036 non-null  object\n",
      " 3   tag                       14036 non-null  object\n",
      " 4   lyrics                    14036 non-null  object\n",
      " 5   length                    14036 non-null  int64 \n",
      " 6   tokens                    14036 non-null  object\n",
      " 7   n_tokens                  14036 non-null  int64 \n",
      " 8   filtered_tokens           14036 non-null  object\n",
      " 9   n_filtered_tokens         14036 non-null  int64 \n",
      " 10  filtered_unique_tokens    14036 non-null  object\n",
      " 11  n_filtered_unique_tokens  14036 non-null  int64 \n",
      "dtypes: int64(5), object(7)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lyrics = lyrics.sample(frac=1, replace=False).reset_index(drop=True)\n",
    "nrows = len(lyrics)\n",
    "split1 = int(0.6 * nrows)\n",
    "split2 = int(0.8 * nrows)\n",
    "train_set = lyrics.iloc[:split1].reset_index(drop=True)\n",
    "val_set = lyrics.iloc[split1:split2].reset_index(drop=True)\n",
    "test_set = lyrics.iloc[split2:].reset_index(drop=True)\n",
    "print('\\nTRAINING SET:')\n",
    "print(train_set.info())\n",
    "print('\\nVALIDATION SET:')\n",
    "print(val_set.info())\n",
    "print('\\nTEST SET:')\n",
    "print(test_set.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>tag</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>n_filtered_tokens</th>\n",
       "      <th>filtered_unique_tokens</th>\n",
       "      <th>n_filtered_unique_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>681241</td>\n",
       "      <td>The Cape</td>\n",
       "      <td>Guy Clark</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nEight years old with a flour sack cape\\nTied...</td>\n",
       "      <td>1312</td>\n",
       "      <td>[Eight, years, old, with, a, flour, sack, cape...</td>\n",
       "      <td>289</td>\n",
       "      <td>[eight, years, old, flour, sack, cape, tied, a...</td>\n",
       "      <td>126</td>\n",
       "      <td>[sure, trust, tied, years, screwed, arms, fing...</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6115912</td>\n",
       "      <td>​heartbreak of america</td>\n",
       "      <td>lovelytheband</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nThis is the heartbreak\\nThis is the heartbre...</td>\n",
       "      <td>1784</td>\n",
       "      <td>[This, is, the, heartbreak, This, is, the, hea...</td>\n",
       "      <td>389</td>\n",
       "      <td>[heartbreak, heartbreak, heartbreak, wipe, fee...</td>\n",
       "      <td>175</td>\n",
       "      <td>[steppеd, gun, freedom, land, us, bruv, reason...</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3277190</td>\n",
       "      <td>I Am Who I Am</td>\n",
       "      <td>Baka Not Nice</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nNiggas wanna hang, yeah (hang, nigga)\\nCause...</td>\n",
       "      <td>2486</td>\n",
       "      <td>[Niggas, wan, na, hang, ,, yeah, (, hang, ,, n...</td>\n",
       "      <td>721</td>\n",
       "      <td>[niggas, wan, na, hang, yeah, hang, nigga, cau...</td>\n",
       "      <td>267</td>\n",
       "      <td>[last, go, want, two, hang, trend, pipes, stas...</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3007450</td>\n",
       "      <td>Drive</td>\n",
       "      <td>Chris Travis</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nWater, water\\nFuck a damn job, Imma' get min...</td>\n",
       "      <td>1500</td>\n",
       "      <td>[Water, ,, water, Fuck, a, damn, job, ,, Imma,...</td>\n",
       "      <td>391</td>\n",
       "      <td>[water, water, fuck, damn, job, imma, get, min...</td>\n",
       "      <td>181</td>\n",
       "      <td>[came, bro, hi, yuh, liked, huh, really, real,...</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6013803</td>\n",
       "      <td>One More Hallelujah</td>\n",
       "      <td>Brennley Brown</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nSometimes, life ain't easy\\nSometimes, you'r...</td>\n",
       "      <td>1696</td>\n",
       "      <td>[Sometimes, ,, life, ai, n't, easy, Sometimes,...</td>\n",
       "      <td>330</td>\n",
       "      <td>[sometimes, life, ai, easy, sometimes, praying...</td>\n",
       "      <td>156</td>\n",
       "      <td>[go, window, could, thankful, staying, whisper...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42101</th>\n",
       "      <td>2456105</td>\n",
       "      <td>Hey hey yeah</td>\n",
       "      <td>RavenEye</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\n\\nVerse 1:\\nYes I know that she's no good\\nO...</td>\n",
       "      <td>1554</td>\n",
       "      <td>[Verse, 1, :, Yes, I, know, that, she, 's, no,...</td>\n",
       "      <td>380</td>\n",
       "      <td>[verse, yes, know, good, gon, na, leave, pain,...</td>\n",
       "      <td>180</td>\n",
       "      <td>[good, even, look, oooohohhhhhooooh, one, want...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42102</th>\n",
       "      <td>99502</td>\n",
       "      <td>Hilikus</td>\n",
       "      <td>Incubus</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nAbout a hundred years ago now\\nThought I was...</td>\n",
       "      <td>1552</td>\n",
       "      <td>[About, a, hundred, years, ago, now, Thought, ...</td>\n",
       "      <td>342</td>\n",
       "      <td>[hundred, years, ago, thought, left, dead, sol...</td>\n",
       "      <td>121</td>\n",
       "      <td>[block, years, good, go, friend, could, knowle...</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42103</th>\n",
       "      <td>1789633</td>\n",
       "      <td>Check</td>\n",
       "      <td>Fetty Wap &amp; DRiivE</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nBaby call me Xoovier thats my name (thats my...</td>\n",
       "      <td>3072</td>\n",
       "      <td>[Baby, call, me, Xoovier, thats, my, name, (, ...</td>\n",
       "      <td>690</td>\n",
       "      <td>[baby, call, xoovier, thats, name, thats, name...</td>\n",
       "      <td>344</td>\n",
       "      <td>[go, wash, want, links, beamer, could, respect...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42104</th>\n",
       "      <td>449564</td>\n",
       "      <td>Family Tradition</td>\n",
       "      <td>Hank Williams Jr.</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nCountry music singers have always been a rea...</td>\n",
       "      <td>1442</td>\n",
       "      <td>[Country, music, singers, have, always, been, ...</td>\n",
       "      <td>332</td>\n",
       "      <td>[country, music, singers, always, real, close,...</td>\n",
       "      <td>127</td>\n",
       "      <td>[smoke, unique, although, guess, want, others,...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42105</th>\n",
       "      <td>111736</td>\n",
       "      <td>Traffic Remix</td>\n",
       "      <td>Lil Reese</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nWe ain't really with that talking, bitch we ...</td>\n",
       "      <td>3063</td>\n",
       "      <td>[We, ai, n't, really, with, that, talking, ,, ...</td>\n",
       "      <td>689</td>\n",
       "      <td>[ai, really, talking, bitch, action, life, man...</td>\n",
       "      <td>286</td>\n",
       "      <td>[go, normal, want, could, keef, kicked, two, b...</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42106 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                   title              artist      tag  \\\n",
       "0       681241                The Cape           Guy Clark  country   \n",
       "1      6115912  ​heartbreak of america       lovelytheband     rock   \n",
       "2      3277190           I Am Who I Am       Baka Not Nice      rap   \n",
       "3      3007450                   Drive        Chris Travis      rap   \n",
       "4      6013803     One More Hallelujah      Brennley Brown  country   \n",
       "...        ...                     ...                 ...      ...   \n",
       "42101  2456105            Hey hey yeah            RavenEye     rock   \n",
       "42102    99502                 Hilikus             Incubus     rock   \n",
       "42103  1789633                   Check  Fetty Wap & DRiivE      rap   \n",
       "42104   449564        Family Tradition   Hank Williams Jr.  country   \n",
       "42105   111736           Traffic Remix           Lil Reese      rap   \n",
       "\n",
       "                                                  lyrics  length  \\\n",
       "0      \\nEight years old with a flour sack cape\\nTied...    1312   \n",
       "1      \\nThis is the heartbreak\\nThis is the heartbre...    1784   \n",
       "2      \\nNiggas wanna hang, yeah (hang, nigga)\\nCause...    2486   \n",
       "3      \\nWater, water\\nFuck a damn job, Imma' get min...    1500   \n",
       "4      \\nSometimes, life ain't easy\\nSometimes, you'r...    1696   \n",
       "...                                                  ...     ...   \n",
       "42101  \\n\\nVerse 1:\\nYes I know that she's no good\\nO...    1554   \n",
       "42102  \\nAbout a hundred years ago now\\nThought I was...    1552   \n",
       "42103  \\nBaby call me Xoovier thats my name (thats my...    3072   \n",
       "42104  \\nCountry music singers have always been a rea...    1442   \n",
       "42105  \\nWe ain't really with that talking, bitch we ...    3063   \n",
       "\n",
       "                                                  tokens  n_tokens  \\\n",
       "0      [Eight, years, old, with, a, flour, sack, cape...       289   \n",
       "1      [This, is, the, heartbreak, This, is, the, hea...       389   \n",
       "2      [Niggas, wan, na, hang, ,, yeah, (, hang, ,, n...       721   \n",
       "3      [Water, ,, water, Fuck, a, damn, job, ,, Imma,...       391   \n",
       "4      [Sometimes, ,, life, ai, n't, easy, Sometimes,...       330   \n",
       "...                                                  ...       ...   \n",
       "42101  [Verse, 1, :, Yes, I, know, that, she, 's, no,...       380   \n",
       "42102  [About, a, hundred, years, ago, now, Thought, ...       342   \n",
       "42103  [Baby, call, me, Xoovier, thats, my, name, (, ...       690   \n",
       "42104  [Country, music, singers, have, always, been, ...       332   \n",
       "42105  [We, ai, n't, really, with, that, talking, ,, ...       689   \n",
       "\n",
       "                                         filtered_tokens  n_filtered_tokens  \\\n",
       "0      [eight, years, old, flour, sack, cape, tied, a...                126   \n",
       "1      [heartbreak, heartbreak, heartbreak, wipe, fee...                175   \n",
       "2      [niggas, wan, na, hang, yeah, hang, nigga, cau...                267   \n",
       "3      [water, water, fuck, damn, job, imma, get, min...                181   \n",
       "4      [sometimes, life, ai, easy, sometimes, praying...                156   \n",
       "...                                                  ...                ...   \n",
       "42101  [verse, yes, know, good, gon, na, leave, pain,...                180   \n",
       "42102  [hundred, years, ago, thought, left, dead, sol...                121   \n",
       "42103  [baby, call, xoovier, thats, name, thats, name...                344   \n",
       "42104  [country, music, singers, always, real, close,...                127   \n",
       "42105  [ai, really, talking, bitch, action, life, man...                286   \n",
       "\n",
       "                                  filtered_unique_tokens  \\\n",
       "0      [sure, trust, tied, years, screwed, arms, fing...   \n",
       "1      [steppеd, gun, freedom, land, us, bruv, reason...   \n",
       "2      [last, go, want, two, hang, trend, pipes, stas...   \n",
       "3      [came, bro, hi, yuh, liked, huh, really, real,...   \n",
       "4      [go, window, could, thankful, staying, whisper...   \n",
       "...                                                  ...   \n",
       "42101  [good, even, look, oooohohhhhhooooh, one, want...   \n",
       "42102  [block, years, good, go, friend, could, knowle...   \n",
       "42103  [go, wash, want, links, beamer, could, respect...   \n",
       "42104  [smoke, unique, although, guess, want, others,...   \n",
       "42105  [go, normal, want, could, keef, kicked, two, b...   \n",
       "\n",
       "       n_filtered_unique_tokens  \n",
       "0                            71  \n",
       "1                            93  \n",
       "2                            88  \n",
       "3                           109  \n",
       "4                            79  \n",
       "...                         ...  \n",
       "42101                        73  \n",
       "42102                        57  \n",
       "42103                       154  \n",
       "42104                        74  \n",
       "42105                       171  \n",
       "\n",
       "[42106 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>tag</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>n_filtered_tokens</th>\n",
       "      <th>filtered_unique_tokens</th>\n",
       "      <th>n_filtered_unique_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3338936</td>\n",
       "      <td>What Went Wrong</td>\n",
       "      <td>Rusty Machines</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nWell looked how this turned out for the both...</td>\n",
       "      <td>714</td>\n",
       "      <td>[Well, looked, how, this, turned, out, for, th...</td>\n",
       "      <td>159</td>\n",
       "      <td>[well, looked, turned, us, decided, stay, forg...</td>\n",
       "      <td>60</td>\n",
       "      <td>[different, sorry, us, well, came, much, said,...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37298</td>\n",
       "      <td>Crownin Me</td>\n",
       "      <td>Playa Fly</td>\n",
       "      <td>rap</td>\n",
       "      <td>Nigga's ask me.. you know what I'm saying..\\nI...</td>\n",
       "      <td>3592</td>\n",
       "      <td>[Nigga, 's, ask, me, .., you, know, what, I, '...</td>\n",
       "      <td>776</td>\n",
       "      <td>[nigga, ask, know, saying, ever, reach, top, f...</td>\n",
       "      <td>382</td>\n",
       "      <td>[slugs, smoke, flizy, shelter, always, suckas,...</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2272377</td>\n",
       "      <td>Chartered Trips</td>\n",
       "      <td>Hsker D</td>\n",
       "      <td>rock</td>\n",
       "      <td>I packed up my belongings\\nIn a nylon carry-al...</td>\n",
       "      <td>1528</td>\n",
       "      <td>[I, packed, up, my, belongings, In, a, nylon, ...</td>\n",
       "      <td>335</td>\n",
       "      <td>[packed, belongings, nylon, hear, porter, call...</td>\n",
       "      <td>150</td>\n",
       "      <td>[go, guess, trees, call, nylon, said, ever, ho...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6135525</td>\n",
       "      <td>Good Girl</td>\n",
       "      <td>Charlotte Cardin</td>\n",
       "      <td>pop</td>\n",
       "      <td>\\nYou spent all my money playing poker with yo...</td>\n",
       "      <td>1102</td>\n",
       "      <td>[You, spent, all, my, money, playing, poker, w...</td>\n",
       "      <td>285</td>\n",
       "      <td>[spent, money, playing, poker, buddies, mind, ...</td>\n",
       "      <td>99</td>\n",
       "      <td>[oh, good, fake, let, buddies, tell, forgive, ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3093833</td>\n",
       "      <td>Heaven Sent // Hell Bound</td>\n",
       "      <td>Vanish</td>\n",
       "      <td>rock</td>\n",
       "      <td>The wolves in the street they start to retreat...</td>\n",
       "      <td>1798</td>\n",
       "      <td>[The, wolves, in, the, street, they, start, to...</td>\n",
       "      <td>391</td>\n",
       "      <td>[wolves, street, start, retreat, sunshine, awa...</td>\n",
       "      <td>167</td>\n",
       "      <td>[last, surely, smoke, go, putting, well, death...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14031</th>\n",
       "      <td>1638232</td>\n",
       "      <td>If You Say Go</td>\n",
       "      <td>Rita Springer</td>\n",
       "      <td>pop</td>\n",
       "      <td>Playlist: Sweetly Broken album version:\\nOh, y...</td>\n",
       "      <td>1273</td>\n",
       "      <td>[Playlist, :, Sweetly, Broken, album, version,...</td>\n",
       "      <td>333</td>\n",
       "      <td>[playlist, sweetly, broken, album, version, oh...</td>\n",
       "      <td>117</td>\n",
       "      <td>[oh, good, look, withdraw, go, album, water, u...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14032</th>\n",
       "      <td>1302888</td>\n",
       "      <td>Dusty Old Dust So Long Its Been Good to Know Yuh</td>\n",
       "      <td>Woody Guthrie</td>\n",
       "      <td>country</td>\n",
       "      <td>I've sung this song, but I'll sing it again\\nO...</td>\n",
       "      <td>1336</td>\n",
       "      <td>[I, 've, sung, this, song, ,, but, I, 'll, sin...</td>\n",
       "      <td>323</td>\n",
       "      <td>[sung, song, sing, place, lived, wild, windy, ...</td>\n",
       "      <td>126</td>\n",
       "      <td>[last, us, read, song, could, dust, jumped, si...</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14033</th>\n",
       "      <td>103771</td>\n",
       "      <td>Lounge Fly</td>\n",
       "      <td>Stone Temple Pilots</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nIt's okay, it's okay\\nOkay, believe me\\nPins...</td>\n",
       "      <td>1722</td>\n",
       "      <td>[It, 's, okay, ,, it, 's, okay, Okay, ,, belie...</td>\n",
       "      <td>446</td>\n",
       "      <td>[okay, okay, okay, believe, pins, pins, kill, ...</td>\n",
       "      <td>153</td>\n",
       "      <td>[let, want, empty, always, kill, live, said, o...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14034</th>\n",
       "      <td>707764</td>\n",
       "      <td>No One Understands</td>\n",
       "      <td>Bayside</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nI've always played the fool around here\\nI'm...</td>\n",
       "      <td>1627</td>\n",
       "      <td>[I, 've, always, played, the, fool, around, he...</td>\n",
       "      <td>358</td>\n",
       "      <td>[always, played, fool, around, starting, worry...</td>\n",
       "      <td>147</td>\n",
       "      <td>[oh, one, always, could, felt, best, much, foo...</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14035</th>\n",
       "      <td>37819</td>\n",
       "      <td>ABC</td>\n",
       "      <td>Prodigy of Mobb Deep</td>\n",
       "      <td>rap</td>\n",
       "      <td>(Young P)\\nAyo daddy show these mothafucka's h...</td>\n",
       "      <td>1917</td>\n",
       "      <td>[(, Young, P, ), Ayo, daddy, show, these, moth...</td>\n",
       "      <td>469</td>\n",
       "      <td>[young, p, ayo, daddy, show, mothafucka, p, p,...</td>\n",
       "      <td>222</td>\n",
       "      <td>[gun, cd, could, uno, came, much, squala, two,...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14036 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                             title  \\\n",
       "0      3338936                                   What Went Wrong   \n",
       "1        37298                                        Crownin Me   \n",
       "2      2272377                                   Chartered Trips   \n",
       "3      6135525                                         Good Girl   \n",
       "4      3093833                         Heaven Sent // Hell Bound   \n",
       "...        ...                                               ...   \n",
       "14031  1638232                                     If You Say Go   \n",
       "14032  1302888  Dusty Old Dust So Long Its Been Good to Know Yuh   \n",
       "14033   103771                                        Lounge Fly   \n",
       "14034   707764                                No One Understands   \n",
       "14035    37819                                               ABC   \n",
       "\n",
       "                     artist      tag  \\\n",
       "0            Rusty Machines     rock   \n",
       "1                 Playa Fly      rap   \n",
       "2                   Hsker D     rock   \n",
       "3          Charlotte Cardin      pop   \n",
       "4                    Vanish     rock   \n",
       "...                     ...      ...   \n",
       "14031         Rita Springer      pop   \n",
       "14032         Woody Guthrie  country   \n",
       "14033   Stone Temple Pilots     rock   \n",
       "14034               Bayside     rock   \n",
       "14035  Prodigy of Mobb Deep      rap   \n",
       "\n",
       "                                                  lyrics  length  \\\n",
       "0      \\nWell looked how this turned out for the both...     714   \n",
       "1      Nigga's ask me.. you know what I'm saying..\\nI...    3592   \n",
       "2      I packed up my belongings\\nIn a nylon carry-al...    1528   \n",
       "3      \\nYou spent all my money playing poker with yo...    1102   \n",
       "4      The wolves in the street they start to retreat...    1798   \n",
       "...                                                  ...     ...   \n",
       "14031  Playlist: Sweetly Broken album version:\\nOh, y...    1273   \n",
       "14032  I've sung this song, but I'll sing it again\\nO...    1336   \n",
       "14033  \\nIt's okay, it's okay\\nOkay, believe me\\nPins...    1722   \n",
       "14034  \\nI've always played the fool around here\\nI'm...    1627   \n",
       "14035  (Young P)\\nAyo daddy show these mothafucka's h...    1917   \n",
       "\n",
       "                                                  tokens  n_tokens  \\\n",
       "0      [Well, looked, how, this, turned, out, for, th...       159   \n",
       "1      [Nigga, 's, ask, me, .., you, know, what, I, '...       776   \n",
       "2      [I, packed, up, my, belongings, In, a, nylon, ...       335   \n",
       "3      [You, spent, all, my, money, playing, poker, w...       285   \n",
       "4      [The, wolves, in, the, street, they, start, to...       391   \n",
       "...                                                  ...       ...   \n",
       "14031  [Playlist, :, Sweetly, Broken, album, version,...       333   \n",
       "14032  [I, 've, sung, this, song, ,, but, I, 'll, sin...       323   \n",
       "14033  [It, 's, okay, ,, it, 's, okay, Okay, ,, belie...       446   \n",
       "14034  [I, 've, always, played, the, fool, around, he...       358   \n",
       "14035  [(, Young, P, ), Ayo, daddy, show, these, moth...       469   \n",
       "\n",
       "                                         filtered_tokens  n_filtered_tokens  \\\n",
       "0      [well, looked, turned, us, decided, stay, forg...                 60   \n",
       "1      [nigga, ask, know, saying, ever, reach, top, f...                382   \n",
       "2      [packed, belongings, nylon, hear, porter, call...                150   \n",
       "3      [spent, money, playing, poker, buddies, mind, ...                 99   \n",
       "4      [wolves, street, start, retreat, sunshine, awa...                167   \n",
       "...                                                  ...                ...   \n",
       "14031  [playlist, sweetly, broken, album, version, oh...                117   \n",
       "14032  [sung, song, sing, place, lived, wild, windy, ...                126   \n",
       "14033  [okay, okay, okay, believe, pins, pins, kill, ...                153   \n",
       "14034  [always, played, fool, around, starting, worry...                147   \n",
       "14035  [young, p, ayo, daddy, show, mothafucka, p, p,...                222   \n",
       "\n",
       "                                  filtered_unique_tokens  \\\n",
       "0      [different, sorry, us, well, came, much, said,...   \n",
       "1      [slugs, smoke, flizy, shelter, always, suckas,...   \n",
       "2      [go, guess, trees, call, nylon, said, ever, ho...   \n",
       "3      [oh, good, fake, let, buddies, tell, forgive, ...   \n",
       "4      [last, surely, smoke, go, putting, well, death...   \n",
       "...                                                  ...   \n",
       "14031  [oh, good, look, withdraw, go, album, water, u...   \n",
       "14032  [last, us, read, song, could, dust, jumped, si...   \n",
       "14033  [let, want, empty, always, kill, live, said, o...   \n",
       "14034  [oh, one, always, could, felt, best, much, foo...   \n",
       "14035  [gun, cd, could, uno, came, much, squala, two,...   \n",
       "\n",
       "       n_filtered_unique_tokens  \n",
       "0                            36  \n",
       "1                           243  \n",
       "2                            39  \n",
       "3                            52  \n",
       "4                           117  \n",
       "...                         ...  \n",
       "14031                        31  \n",
       "14032                        86  \n",
       "14033                        41  \n",
       "14034                        73  \n",
       "14035                       153  \n",
       "\n",
       "[14036 rows x 12 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>tag</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>length</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>filtered_tokens</th>\n",
       "      <th>n_filtered_tokens</th>\n",
       "      <th>filtered_unique_tokens</th>\n",
       "      <th>n_filtered_unique_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>896842</td>\n",
       "      <td>Next Ex Kick Em Out</td>\n",
       "      <td>Beyonc</td>\n",
       "      <td>rb</td>\n",
       "      <td>\\nWoah-ho ooh\\n\\n\\nI know that you treat me ri...</td>\n",
       "      <td>2108</td>\n",
       "      <td>[Woah-ho, ooh, I, know, that, you, treat, me, ...</td>\n",
       "      <td>521</td>\n",
       "      <td>[ooh, know, treat, right, still, think, got, t...</td>\n",
       "      <td>231</td>\n",
       "      <td>[oh, last, tied, woah, look, move, sorry, boy,...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100615</td>\n",
       "      <td>Stand</td>\n",
       "      <td>Sly and the Family Stone</td>\n",
       "      <td>rb</td>\n",
       "      <td>\\nStand\\nIn the end you'll still be you\\nOne t...</td>\n",
       "      <td>768</td>\n",
       "      <td>[Stand, In, the, end, you, 'll, still, be, you...</td>\n",
       "      <td>164</td>\n",
       "      <td>[stand, end, still, one, done, things, set, st...</td>\n",
       "      <td>68</td>\n",
       "      <td>[things, go, crease, one, want, fall, deal, cr...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306029</td>\n",
       "      <td>Roots Rock Reggae</td>\n",
       "      <td>Bob Marley &amp; The Wailers</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nPlay I some music (This a Reggae music)\\nPla...</td>\n",
       "      <td>1190</td>\n",
       "      <td>[Play, I, some, music, (, This, a, Reggae, mus...</td>\n",
       "      <td>275</td>\n",
       "      <td>[play, music, reggae, music, play, music, regg...</td>\n",
       "      <td>112</td>\n",
       "      <td>[sure, good, want, one, refuse, play, hundred,...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3035542</td>\n",
       "      <td>All In</td>\n",
       "      <td>Empire Cast</td>\n",
       "      <td>rb</td>\n",
       "      <td>\\nYou and I, you and I, had our time, had our ...</td>\n",
       "      <td>1355</td>\n",
       "      <td>[You, and, I, ,, you, and, I, ,, had, our, tim...</td>\n",
       "      <td>345</td>\n",
       "      <td>[time, time, moments, start, fade, away, baby,...</td>\n",
       "      <td>119</td>\n",
       "      <td>[last, trust, adore, go, us, cup, budge, two, ...</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>695880</td>\n",
       "      <td>Go To Dumb</td>\n",
       "      <td>Lil B</td>\n",
       "      <td>rock</td>\n",
       "      <td>\\nIt's your boy Lil B\\nBased freestyle in New ...</td>\n",
       "      <td>2274</td>\n",
       "      <td>[It, 's, your, boy, Lil, B, Based, freestyle, ...</td>\n",
       "      <td>509</td>\n",
       "      <td>[boy, lil, b, based, freestyle, new, york, cit...</td>\n",
       "      <td>244</td>\n",
       "      <td>[smoke, go, want, song, stay, blunts, pretty, ...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14031</th>\n",
       "      <td>2986575</td>\n",
       "      <td>God I’m Sorry</td>\n",
       "      <td>Boosie Badazz &amp; C-Murder</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nC Rhythm \\n\\n\\nFirst off, I wanna apologize ...</td>\n",
       "      <td>3293</td>\n",
       "      <td>[C, Rhythm, First, off, ,, I, wan, na, apologi...</td>\n",
       "      <td>833</td>\n",
       "      <td>[c, rhythm, first, wan, na, apologize, every, ...</td>\n",
       "      <td>302</td>\n",
       "      <td>[uh, keeping, destined, much, forgiveness, sta...</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14032</th>\n",
       "      <td>1262205</td>\n",
       "      <td>I Still Love You</td>\n",
       "      <td>Next</td>\n",
       "      <td>rb</td>\n",
       "      <td>\\nI still love you, babe\\n(It ain't ever gonna...</td>\n",
       "      <td>2553</td>\n",
       "      <td>[I, still, love, you, ,, babe, (, It, ai, n't,...</td>\n",
       "      <td>693</td>\n",
       "      <td>[still, love, babe, ai, ever, gon, na, change,...</td>\n",
       "      <td>237</td>\n",
       "      <td>[prove, last, oh, even, let, one, want, always...</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14033</th>\n",
       "      <td>3176659</td>\n",
       "      <td>Queen of the New World</td>\n",
       "      <td>Pre Kai Ro</td>\n",
       "      <td>rb</td>\n",
       "      <td>It’s what you doin’ oh no \\n\\nSad royalty girl...</td>\n",
       "      <td>2573</td>\n",
       "      <td>[It, ’, s, what, you, doin, ’, oh, no, Sad, ro...</td>\n",
       "      <td>671</td>\n",
       "      <td>[doin, oh, sad, royalty, girl, dont, spoil, gi...</td>\n",
       "      <td>214</td>\n",
       "      <td>[go, want, song, taking, cool, still, codeine,...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14034</th>\n",
       "      <td>28150</td>\n",
       "      <td>Don’t Mess wit Texas</td>\n",
       "      <td>Lil Keke</td>\n",
       "      <td>rap</td>\n",
       "      <td>\\nPut your stones in dust and worn, South coas...</td>\n",
       "      <td>3638</td>\n",
       "      <td>[Put, your, stones, in, dust, and, worn, ,, So...</td>\n",
       "      <td>835</td>\n",
       "      <td>[put, stones, dust, worn, south, coast, raised...</td>\n",
       "      <td>402</td>\n",
       "      <td>[ill, trust, uh, playahaters, everythang, keke...</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14035</th>\n",
       "      <td>5726635</td>\n",
       "      <td>​get oveR you</td>\n",
       "      <td>Lindsay Ell</td>\n",
       "      <td>country</td>\n",
       "      <td>\\nI walked away with tears running down my fac...</td>\n",
       "      <td>1079</td>\n",
       "      <td>[I, walked, away, with, tears, running, down, ...</td>\n",
       "      <td>253</td>\n",
       "      <td>[walked, away, tears, running, face, say, know...</td>\n",
       "      <td>79</td>\n",
       "      <td>[things, go, want, move, call, well, say, know...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14036 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                   title                    artist      tag  \\\n",
       "0       896842     Next Ex Kick Em Out                    Beyonc       rb   \n",
       "1       100615                   Stand  Sly and the Family Stone       rb   \n",
       "2       306029       Roots Rock Reggae  Bob Marley & The Wailers     rock   \n",
       "3      3035542                  All In               Empire Cast       rb   \n",
       "4       695880              Go To Dumb                     Lil B     rock   \n",
       "...        ...                     ...                       ...      ...   \n",
       "14031  2986575           God I’m Sorry  Boosie Badazz & C-Murder      rap   \n",
       "14032  1262205        I Still Love You                      Next       rb   \n",
       "14033  3176659  Queen of the New World                Pre Kai Ro       rb   \n",
       "14034    28150    Don’t Mess wit Texas                  Lil Keke      rap   \n",
       "14035  5726635           ​get oveR you               Lindsay Ell  country   \n",
       "\n",
       "                                                  lyrics  length  \\\n",
       "0      \\nWoah-ho ooh\\n\\n\\nI know that you treat me ri...    2108   \n",
       "1      \\nStand\\nIn the end you'll still be you\\nOne t...     768   \n",
       "2      \\nPlay I some music (This a Reggae music)\\nPla...    1190   \n",
       "3      \\nYou and I, you and I, had our time, had our ...    1355   \n",
       "4      \\nIt's your boy Lil B\\nBased freestyle in New ...    2274   \n",
       "...                                                  ...     ...   \n",
       "14031  \\nC Rhythm \\n\\n\\nFirst off, I wanna apologize ...    3293   \n",
       "14032  \\nI still love you, babe\\n(It ain't ever gonna...    2553   \n",
       "14033  It’s what you doin’ oh no \\n\\nSad royalty girl...    2573   \n",
       "14034  \\nPut your stones in dust and worn, South coas...    3638   \n",
       "14035  \\nI walked away with tears running down my fac...    1079   \n",
       "\n",
       "                                                  tokens  n_tokens  \\\n",
       "0      [Woah-ho, ooh, I, know, that, you, treat, me, ...       521   \n",
       "1      [Stand, In, the, end, you, 'll, still, be, you...       164   \n",
       "2      [Play, I, some, music, (, This, a, Reggae, mus...       275   \n",
       "3      [You, and, I, ,, you, and, I, ,, had, our, tim...       345   \n",
       "4      [It, 's, your, boy, Lil, B, Based, freestyle, ...       509   \n",
       "...                                                  ...       ...   \n",
       "14031  [C, Rhythm, First, off, ,, I, wan, na, apologi...       833   \n",
       "14032  [I, still, love, you, ,, babe, (, It, ai, n't,...       693   \n",
       "14033  [It, ’, s, what, you, doin, ’, oh, no, Sad, ro...       671   \n",
       "14034  [Put, your, stones, in, dust, and, worn, ,, So...       835   \n",
       "14035  [I, walked, away, with, tears, running, down, ...       253   \n",
       "\n",
       "                                         filtered_tokens  n_filtered_tokens  \\\n",
       "0      [ooh, know, treat, right, still, think, got, t...                231   \n",
       "1      [stand, end, still, one, done, things, set, st...                 68   \n",
       "2      [play, music, reggae, music, play, music, regg...                112   \n",
       "3      [time, time, moments, start, fade, away, baby,...                119   \n",
       "4      [boy, lil, b, based, freestyle, new, york, cit...                244   \n",
       "...                                                  ...                ...   \n",
       "14031  [c, rhythm, first, wan, na, apologize, every, ...                302   \n",
       "14032  [still, love, babe, ai, ever, gon, na, change,...                237   \n",
       "14033  [doin, oh, sad, royalty, girl, dont, spoil, gi...                214   \n",
       "14034  [put, stones, dust, worn, south, coast, raised...                402   \n",
       "14035  [walked, away, tears, running, face, say, know...                 79   \n",
       "\n",
       "                                  filtered_unique_tokens  \\\n",
       "0      [oh, last, tied, woah, look, move, sorry, boy,...   \n",
       "1      [things, go, crease, one, want, fall, deal, cr...   \n",
       "2      [sure, good, want, one, refuse, play, hundred,...   \n",
       "3      [last, trust, adore, go, us, cup, budge, two, ...   \n",
       "4      [smoke, go, want, song, stay, blunts, pretty, ...   \n",
       "...                                                  ...   \n",
       "14031  [uh, keeping, destined, much, forgiveness, sta...   \n",
       "14032  [prove, last, oh, even, let, one, want, always...   \n",
       "14033  [go, want, song, taking, cool, still, codeine,...   \n",
       "14034  [ill, trust, uh, playahaters, everythang, keke...   \n",
       "14035  [things, go, want, move, call, well, say, know...   \n",
       "\n",
       "       n_filtered_unique_tokens  \n",
       "0                            66  \n",
       "1                            43  \n",
       "2                            28  \n",
       "3                            79  \n",
       "4                            85  \n",
       "...                         ...  \n",
       "14031                       191  \n",
       "14032                        68  \n",
       "14033                        82  \n",
       "14034                       239  \n",
       "14035                        38  \n",
       "\n",
       "[14036 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set:\n",
      "country:  0.2\n",
      "rock:  0.2\n",
      "rap:  0.2\n",
      "rb:  0.2\n",
      "pop:  0.2\n",
      "\n",
      "Validation set:\n",
      "rock:  0.2\n",
      "rap:  0.2\n",
      "pop:  0.2\n",
      "country:  0.2\n",
      "rb:  0.2\n",
      "\n",
      "Test set:\n",
      "rb:  0.2\n",
      "rock:  0.2\n",
      "rap:  0.2\n",
      "country:  0.2\n",
      "pop:  0.2\n"
     ]
    }
   ],
   "source": [
    "for df, s in zip([train_set, val_set, test_set], ['Training set', 'Validation set', 'Test set']):\n",
    "    print(f'\\n{s}:')\n",
    "    for tag in df['tag'].unique():\n",
    "        tag_perc = len(df[df['tag'] == tag]) / len(df)\n",
    "        print(f'{tag}: {tag_perc: .1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the splits\n",
    "for df, s in zip([train_set, val_set, test_set], ['train_set', 'val_set', 'test_set']):\n",
    "    df['tokens'] = df['tokens'].apply(json.dumps)\n",
    "    df['filtered_tokens'] = df['filtered_tokens'].apply(json.dumps)\n",
    "    df['filtered_unique_tokens'] = df['filtered_unique_tokens'].apply(json.dumps)\n",
    "    df.to_csv(f'{s}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/claudia/HLT/project/code/hlt_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mapply\n",
    "# set mapply\n",
    "mapply.init(n_workers=-1, progressbar=True)\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set\n",
    "train_set = pd.read_csv('train_set.csv')\n",
    "\n",
    "train_set['tokens'] = train_set['tokens'].apply(json.loads)\n",
    "train_set['filtered_tokens'] = train_set['filtered_tokens'].apply(json.loads)\n",
    "train_set['filtered_unique_tokens'] = train_set['filtered_unique_tokens'].apply(json.loads)\n",
    "\n",
    "# Load validation set\n",
    "val_set = pd.read_csv('val_set.csv')\n",
    "\n",
    "val_set['tokens'] = val_set['tokens'].apply(json.loads)\n",
    "val_set['filtered_tokens'] = val_set['filtered_tokens'].apply(json.loads)\n",
    "val_set['filtered_unique_tokens'] = val_set['filtered_unique_tokens'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "  2%|▎         | 1/40 [00:01<01:14,  1.90s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "  5%|▌         | 2/40 [00:02<00:52,  1.39s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 35%|███▌      | 14/40 [00:16<00:29,  1.14s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 38%|███▊      | 15/40 [00:19<00:32,  1.28s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 45%|████▌     | 18/40 [00:20<00:24,  1.10s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 48%|████▊     | 19/40 [00:21<00:23,  1.12s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "100%|██████████| 40/40 [00:36<00:00,  1.09it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "  2%|▎         | 1/40 [00:00<00:22,  1.72it/s]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 40%|████      | 16/40 [00:18<00:27,  1.15s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 42%|████▎     | 17/40 [00:20<00:27,  1.19s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 45%|████▌     | 18/40 [00:21<00:26,  1.21s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      " 48%|████▊     | 19/40 [00:22<00:25,  1.19s/it]/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "/tmp/ipykernel_8843/2334475650.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(lyric_vector)\n",
      "100%|██████████| 40/40 [00:38<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "# Load FastText embeddings\n",
    "fasttext_embeddings = vocab.FastText('simple')\n",
    "\n",
    "def vectorize(lyric_tokens):\n",
    "    lyric_tokens = [token for token in lyric_tokens if token in fasttext_embeddings.stoi]\n",
    "    vectors = [fasttext_embeddings[token] for token in lyric_tokens]\n",
    "    lyric_vector = torch.mean(torch.stack(vectors), dim=0)\n",
    "    return torch.tensor(lyric_vector)\n",
    "\n",
    "# Compute FastText embeddings\n",
    "train_set['FastText_embedding'] = train_set['filtered_tokens'].mapply(vectorize)\n",
    "val_set['FastText_embedding'] = val_set['filtered_tokens'].mapply(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42106"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set['FastText_embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.55it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 50.14it/s]\n"
     ]
    }
   ],
   "source": [
    "label = {\n",
    "    'rap': torch.tensor(0, dtype=torch.long),\n",
    "    'rb': torch.tensor(1, dtype=torch.long),\n",
    "    'pop': torch.tensor(2, dtype=torch.long),\n",
    "    'rock': torch.tensor(3, dtype=torch.long),\n",
    "    'country': torch.tensor(4, dtype=torch.long)\n",
    "}\n",
    "\n",
    "train_set['label'] = train_set['tag'].mapply(lambda x: label[x])\n",
    "val_set['label'] = val_set['tag'].mapply(lambda x: label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics_df):\n",
    "        \n",
    "        self.x = train_set['FastText_embedding'].values\n",
    "        self.y = train_set['label'].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "training_set = LyricsDataset(train_set)\n",
    "train_dataloader = DataLoader(training_set, batch_size=50000, shuffle=True)\n",
    "\n",
    "validation_set = LyricsDataset(val_set)\n",
    "val_dataloader = DataLoader(validation_set, batch_size=50000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1000),#128\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, output_dim),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(300, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "model = NeuralNetwork(input_dim = 300, output_dim = 5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 1.6115\n",
      "Epoch [2/1000], Loss: 1.6002\n",
      "Epoch [3/1000], Loss: 1.5905\n",
      "Epoch [4/1000], Loss: 1.5804\n",
      "Epoch [5/1000], Loss: 1.5697\n",
      "Epoch [6/1000], Loss: 1.5583\n",
      "Epoch [7/1000], Loss: 1.5463\n",
      "Epoch [8/1000], Loss: 1.5335\n",
      "Epoch [9/1000], Loss: 1.5200\n",
      "Epoch [10/1000], Loss: 1.5059\n",
      "Epoch [11/1000], Loss: 1.4912\n",
      "Epoch [12/1000], Loss: 1.4760\n",
      "Epoch [13/1000], Loss: 1.4606\n",
      "Epoch [14/1000], Loss: 1.4451\n",
      "Epoch [15/1000], Loss: 1.4297\n",
      "Epoch [16/1000], Loss: 1.4144\n",
      "Epoch [17/1000], Loss: 1.3996\n",
      "Epoch [18/1000], Loss: 1.3853\n",
      "Epoch [19/1000], Loss: 1.3718\n",
      "Epoch [20/1000], Loss: 1.3590\n",
      "Epoch [21/1000], Loss: 1.3472\n",
      "Epoch [22/1000], Loss: 1.3362\n",
      "Epoch [23/1000], Loss: 1.3260\n",
      "Epoch [24/1000], Loss: 1.3166\n",
      "Epoch [25/1000], Loss: 1.3079\n",
      "Epoch [26/1000], Loss: 1.2998\n",
      "Epoch [27/1000], Loss: 1.2922\n",
      "Epoch [28/1000], Loss: 1.2850\n",
      "Epoch [29/1000], Loss: 1.2782\n",
      "Epoch [30/1000], Loss: 1.2717\n",
      "Epoch [31/1000], Loss: 1.2654\n",
      "Epoch [32/1000], Loss: 1.2594\n",
      "Epoch [33/1000], Loss: 1.2536\n",
      "Epoch [34/1000], Loss: 1.2481\n",
      "Epoch [35/1000], Loss: 1.2428\n",
      "Epoch [36/1000], Loss: 1.2377\n",
      "Epoch [37/1000], Loss: 1.2329\n",
      "Epoch [38/1000], Loss: 1.2283\n",
      "Epoch [39/1000], Loss: 1.2240\n",
      "Epoch [40/1000], Loss: 1.2198\n",
      "Epoch [41/1000], Loss: 1.2157\n",
      "Epoch [42/1000], Loss: 1.2119\n",
      "Epoch [43/1000], Loss: 1.2082\n",
      "Epoch [44/1000], Loss: 1.2046\n",
      "Epoch [45/1000], Loss: 1.2012\n",
      "Epoch [46/1000], Loss: 1.1979\n",
      "Epoch [47/1000], Loss: 1.1947\n",
      "Epoch [48/1000], Loss: 1.1915\n",
      "Epoch [49/1000], Loss: 1.1885\n",
      "Epoch [50/1000], Loss: 1.1855\n",
      "Epoch [51/1000], Loss: 1.1827\n",
      "Epoch [52/1000], Loss: 1.1799\n",
      "Epoch [53/1000], Loss: 1.1772\n",
      "Epoch [54/1000], Loss: 1.1746\n",
      "Epoch [55/1000], Loss: 1.1721\n",
      "Epoch [56/1000], Loss: 1.1698\n",
      "Epoch [57/1000], Loss: 1.1681\n",
      "Epoch [58/1000], Loss: 1.1659\n",
      "Epoch [59/1000], Loss: 1.1631\n",
      "Epoch [60/1000], Loss: 1.1613\n",
      "Epoch [61/1000], Loss: 1.1597\n",
      "Epoch [62/1000], Loss: 1.1573\n",
      "Epoch [63/1000], Loss: 1.1557\n",
      "Epoch [64/1000], Loss: 1.1540\n",
      "Epoch [65/1000], Loss: 1.1520\n",
      "Epoch [66/1000], Loss: 1.1505\n",
      "Epoch [67/1000], Loss: 1.1488\n",
      "Epoch [68/1000], Loss: 1.1471\n",
      "Epoch [69/1000], Loss: 1.1457\n",
      "Epoch [70/1000], Loss: 1.1440\n",
      "Epoch [71/1000], Loss: 1.1425\n",
      "Epoch [72/1000], Loss: 1.1411\n",
      "Epoch [73/1000], Loss: 1.1395\n",
      "Epoch [74/1000], Loss: 1.1381\n",
      "Epoch [75/1000], Loss: 1.1367\n",
      "Epoch [76/1000], Loss: 1.1353\n",
      "Epoch [77/1000], Loss: 1.1339\n",
      "Epoch [78/1000], Loss: 1.1326\n",
      "Epoch [79/1000], Loss: 1.1312\n",
      "Epoch [80/1000], Loss: 1.1299\n",
      "Epoch [81/1000], Loss: 1.1287\n",
      "Epoch [82/1000], Loss: 1.1274\n",
      "Epoch [83/1000], Loss: 1.1261\n",
      "Epoch [84/1000], Loss: 1.1249\n",
      "Epoch [85/1000], Loss: 1.1237\n",
      "Epoch [86/1000], Loss: 1.1224\n",
      "Epoch [87/1000], Loss: 1.1212\n",
      "Epoch [88/1000], Loss: 1.1200\n",
      "Epoch [89/1000], Loss: 1.1189\n",
      "Epoch [90/1000], Loss: 1.1177\n",
      "Epoch [91/1000], Loss: 1.1165\n",
      "Epoch [92/1000], Loss: 1.1154\n",
      "Epoch [93/1000], Loss: 1.1142\n",
      "Epoch [94/1000], Loss: 1.1131\n",
      "Epoch [95/1000], Loss: 1.1120\n",
      "Epoch [96/1000], Loss: 1.1109\n",
      "Epoch [97/1000], Loss: 1.1098\n",
      "Epoch [98/1000], Loss: 1.1087\n",
      "Epoch [99/1000], Loss: 1.1077\n",
      "Epoch [100/1000], Loss: 1.1066\n",
      "Epoch [101/1000], Loss: 1.1056\n",
      "Epoch [102/1000], Loss: 1.1045\n",
      "Epoch [103/1000], Loss: 1.1035\n",
      "Epoch [104/1000], Loss: 1.1025\n",
      "Epoch [105/1000], Loss: 1.1016\n",
      "Epoch [106/1000], Loss: 1.1007\n",
      "Epoch [107/1000], Loss: 1.1001\n",
      "Epoch [108/1000], Loss: 1.0998\n",
      "Epoch [109/1000], Loss: 1.0994\n",
      "Epoch [110/1000], Loss: 1.0975\n",
      "Epoch [111/1000], Loss: 1.0958\n",
      "Epoch [112/1000], Loss: 1.0957\n",
      "Epoch [113/1000], Loss: 1.0950\n",
      "Epoch [114/1000], Loss: 1.0934\n",
      "Epoch [115/1000], Loss: 1.0926\n",
      "Epoch [116/1000], Loss: 1.0924\n",
      "Epoch [117/1000], Loss: 1.0910\n",
      "Epoch [118/1000], Loss: 1.0899\n",
      "Epoch [119/1000], Loss: 1.0896\n",
      "Epoch [120/1000], Loss: 1.0886\n",
      "Epoch [121/1000], Loss: 1.0875\n",
      "Epoch [122/1000], Loss: 1.0870\n",
      "Epoch [123/1000], Loss: 1.0863\n",
      "Epoch [124/1000], Loss: 1.0851\n",
      "Epoch [125/1000], Loss: 1.0845\n",
      "Epoch [126/1000], Loss: 1.0839\n",
      "Epoch [127/1000], Loss: 1.0829\n",
      "Epoch [128/1000], Loss: 1.0821\n",
      "Epoch [129/1000], Loss: 1.0815\n",
      "Epoch [130/1000], Loss: 1.0806\n",
      "Epoch [131/1000], Loss: 1.0797\n",
      "Epoch [132/1000], Loss: 1.0791\n",
      "Epoch [133/1000], Loss: 1.0784\n",
      "Epoch [134/1000], Loss: 1.0775\n",
      "Epoch [135/1000], Loss: 1.0767\n",
      "Epoch [136/1000], Loss: 1.0760\n",
      "Epoch [137/1000], Loss: 1.0753\n",
      "Epoch [138/1000], Loss: 1.0745\n",
      "Epoch [139/1000], Loss: 1.0737\n",
      "Epoch [140/1000], Loss: 1.0730\n",
      "Epoch [141/1000], Loss: 1.0722\n",
      "Epoch [142/1000], Loss: 1.0715\n",
      "Epoch [143/1000], Loss: 1.0707\n",
      "Epoch [144/1000], Loss: 1.0700\n",
      "Epoch [145/1000], Loss: 1.0692\n",
      "Epoch [146/1000], Loss: 1.0685\n",
      "Epoch [147/1000], Loss: 1.0677\n",
      "Epoch [148/1000], Loss: 1.0670\n",
      "Epoch [149/1000], Loss: 1.0662\n",
      "Epoch [150/1000], Loss: 1.0655\n",
      "Epoch [151/1000], Loss: 1.0648\n",
      "Epoch [152/1000], Loss: 1.0641\n",
      "Epoch [153/1000], Loss: 1.0634\n",
      "Epoch [154/1000], Loss: 1.0627\n",
      "Epoch [155/1000], Loss: 1.0624\n",
      "Epoch [156/1000], Loss: 1.0626\n",
      "Epoch [157/1000], Loss: 1.0630\n",
      "Epoch [158/1000], Loss: 1.0614\n",
      "Epoch [159/1000], Loss: 1.0593\n",
      "Epoch [160/1000], Loss: 1.0596\n",
      "Epoch [161/1000], Loss: 1.0590\n",
      "Epoch [162/1000], Loss: 1.0572\n",
      "Epoch [163/1000], Loss: 1.0573\n",
      "Epoch [164/1000], Loss: 1.0567\n",
      "Epoch [165/1000], Loss: 1.0552\n",
      "Epoch [166/1000], Loss: 1.0552\n",
      "Epoch [167/1000], Loss: 1.0543\n",
      "Epoch [168/1000], Loss: 1.0533\n",
      "Epoch [169/1000], Loss: 1.0533\n",
      "Epoch [170/1000], Loss: 1.0522\n",
      "Epoch [171/1000], Loss: 1.0514\n",
      "Epoch [172/1000], Loss: 1.0512\n",
      "Epoch [173/1000], Loss: 1.0501\n",
      "Epoch [174/1000], Loss: 1.0496\n",
      "Epoch [175/1000], Loss: 1.0492\n",
      "Epoch [176/1000], Loss: 1.0482\n",
      "Epoch [177/1000], Loss: 1.0478\n",
      "Epoch [178/1000], Loss: 1.0471\n",
      "Epoch [179/1000], Loss: 1.0463\n",
      "Epoch [180/1000], Loss: 1.0458\n",
      "Epoch [181/1000], Loss: 1.0451\n",
      "Epoch [182/1000], Loss: 1.0444\n",
      "Epoch [183/1000], Loss: 1.0439\n",
      "Epoch [184/1000], Loss: 1.0432\n",
      "Epoch [185/1000], Loss: 1.0425\n",
      "Epoch [186/1000], Loss: 1.0420\n",
      "Epoch [187/1000], Loss: 1.0413\n",
      "Epoch [188/1000], Loss: 1.0406\n",
      "Epoch [189/1000], Loss: 1.0401\n",
      "Epoch [190/1000], Loss: 1.0395\n",
      "Epoch [191/1000], Loss: 1.0390\n",
      "Epoch [192/1000], Loss: 1.0388\n",
      "Epoch [193/1000], Loss: 1.0388\n",
      "Epoch [194/1000], Loss: 1.0388\n",
      "Epoch [195/1000], Loss: 1.0384\n",
      "Epoch [196/1000], Loss: 1.0366\n",
      "Epoch [197/1000], Loss: 1.0351\n",
      "Epoch [198/1000], Loss: 1.0350\n",
      "Epoch [199/1000], Loss: 1.0351\n",
      "Epoch [200/1000], Loss: 1.0342\n",
      "Epoch [201/1000], Loss: 1.0329\n",
      "Epoch [202/1000], Loss: 1.0325\n",
      "Epoch [203/1000], Loss: 1.0324\n",
      "Epoch [204/1000], Loss: 1.0316\n",
      "Epoch [205/1000], Loss: 1.0306\n",
      "Epoch [206/1000], Loss: 1.0301\n",
      "Epoch [207/1000], Loss: 1.0298\n",
      "Epoch [208/1000], Loss: 1.0292\n",
      "Epoch [209/1000], Loss: 1.0283\n",
      "Epoch [210/1000], Loss: 1.0278\n",
      "Epoch [211/1000], Loss: 1.0275\n",
      "Epoch [212/1000], Loss: 1.0267\n",
      "Epoch [213/1000], Loss: 1.0260\n",
      "Epoch [214/1000], Loss: 1.0254\n",
      "Epoch [215/1000], Loss: 1.0250\n",
      "Epoch [216/1000], Loss: 1.0244\n",
      "Epoch [217/1000], Loss: 1.0237\n",
      "Epoch [218/1000], Loss: 1.0231\n",
      "Epoch [219/1000], Loss: 1.0226\n",
      "Epoch [220/1000], Loss: 1.0221\n",
      "Epoch [221/1000], Loss: 1.0215\n",
      "Epoch [222/1000], Loss: 1.0208\n",
      "Epoch [223/1000], Loss: 1.0202\n",
      "Epoch [224/1000], Loss: 1.0197\n",
      "Epoch [225/1000], Loss: 1.0192\n",
      "Epoch [226/1000], Loss: 1.0186\n",
      "Epoch [227/1000], Loss: 1.0180\n",
      "Epoch [228/1000], Loss: 1.0174\n",
      "Epoch [229/1000], Loss: 1.0168\n",
      "Epoch [230/1000], Loss: 1.0163\n",
      "Epoch [231/1000], Loss: 1.0157\n",
      "Epoch [232/1000], Loss: 1.0152\n",
      "Epoch [233/1000], Loss: 1.0146\n",
      "Epoch [234/1000], Loss: 1.0140\n",
      "Epoch [235/1000], Loss: 1.0134\n",
      "Epoch [236/1000], Loss: 1.0128\n",
      "Epoch [237/1000], Loss: 1.0123\n",
      "Epoch [238/1000], Loss: 1.0118\n",
      "Epoch [239/1000], Loss: 1.0114\n",
      "Epoch [240/1000], Loss: 1.0111\n",
      "Epoch [241/1000], Loss: 1.0109\n",
      "Epoch [242/1000], Loss: 1.0106\n",
      "Epoch [243/1000], Loss: 1.0100\n",
      "Epoch [244/1000], Loss: 1.0092\n",
      "Epoch [245/1000], Loss: 1.0084\n",
      "Epoch [246/1000], Loss: 1.0077\n",
      "Epoch [247/1000], Loss: 1.0073\n",
      "Epoch [248/1000], Loss: 1.0067\n",
      "Epoch [249/1000], Loss: 1.0059\n",
      "Epoch [250/1000], Loss: 1.0051\n",
      "Epoch [251/1000], Loss: 1.0047\n",
      "Epoch [252/1000], Loss: 1.0044\n",
      "Epoch [253/1000], Loss: 1.0040\n",
      "Epoch [254/1000], Loss: 1.0032\n",
      "Epoch [255/1000], Loss: 1.0023\n",
      "Epoch [256/1000], Loss: 1.0017\n",
      "Epoch [257/1000], Loss: 1.0014\n",
      "Epoch [258/1000], Loss: 1.0009\n",
      "Epoch [259/1000], Loss: 1.0003\n",
      "Epoch [260/1000], Loss: 0.9996\n",
      "Epoch [261/1000], Loss: 0.9990\n",
      "Epoch [262/1000], Loss: 0.9986\n",
      "Epoch [263/1000], Loss: 0.9981\n",
      "Epoch [264/1000], Loss: 0.9977\n",
      "Epoch [265/1000], Loss: 0.9970\n",
      "Epoch [266/1000], Loss: 0.9964\n",
      "Epoch [267/1000], Loss: 0.9958\n",
      "Epoch [268/1000], Loss: 0.9953\n",
      "Epoch [269/1000], Loss: 0.9949\n",
      "Epoch [270/1000], Loss: 0.9944\n",
      "Epoch [271/1000], Loss: 0.9939\n",
      "Epoch [272/1000], Loss: 0.9935\n",
      "Epoch [273/1000], Loss: 0.9930\n",
      "Epoch [274/1000], Loss: 0.9925\n",
      "Epoch [275/1000], Loss: 0.9920\n",
      "Epoch [276/1000], Loss: 0.9914\n",
      "Epoch [277/1000], Loss: 0.9908\n",
      "Epoch [278/1000], Loss: 0.9902\n",
      "Epoch [279/1000], Loss: 0.9896\n",
      "Epoch [280/1000], Loss: 0.9890\n",
      "Epoch [281/1000], Loss: 0.9883\n",
      "Epoch [282/1000], Loss: 0.9876\n",
      "Epoch [283/1000], Loss: 0.9871\n",
      "Epoch [284/1000], Loss: 0.9867\n",
      "Epoch [285/1000], Loss: 0.9864\n",
      "Epoch [286/1000], Loss: 0.9860\n",
      "Epoch [287/1000], Loss: 0.9855\n",
      "Epoch [288/1000], Loss: 0.9850\n",
      "Epoch [289/1000], Loss: 0.9844\n",
      "Epoch [290/1000], Loss: 0.9839\n",
      "Epoch [291/1000], Loss: 0.9833\n",
      "Epoch [292/1000], Loss: 0.9826\n",
      "Epoch [293/1000], Loss: 0.9818\n",
      "Epoch [294/1000], Loss: 0.9812\n",
      "Epoch [295/1000], Loss: 0.9807\n",
      "Epoch [296/1000], Loss: 0.9804\n",
      "Epoch [297/1000], Loss: 0.9801\n",
      "Epoch [298/1000], Loss: 0.9798\n",
      "Epoch [299/1000], Loss: 0.9793\n",
      "Epoch [300/1000], Loss: 0.9789\n",
      "Epoch [301/1000], Loss: 0.9783\n",
      "Epoch [302/1000], Loss: 0.9778\n",
      "Epoch [303/1000], Loss: 0.9773\n",
      "Epoch [304/1000], Loss: 0.9768\n",
      "Epoch [305/1000], Loss: 0.9763\n",
      "Epoch [306/1000], Loss: 0.9761\n",
      "Epoch [307/1000], Loss: 0.9758\n",
      "Epoch [308/1000], Loss: 0.9752\n",
      "Epoch [309/1000], Loss: 0.9741\n",
      "Epoch [310/1000], Loss: 0.9729\n",
      "Epoch [311/1000], Loss: 0.9723\n",
      "Epoch [312/1000], Loss: 0.9722\n",
      "Epoch [313/1000], Loss: 0.9721\n",
      "Epoch [314/1000], Loss: 0.9716\n",
      "Epoch [315/1000], Loss: 0.9708\n",
      "Epoch [316/1000], Loss: 0.9700\n",
      "Epoch [317/1000], Loss: 0.9695\n",
      "Epoch [318/1000], Loss: 0.9691\n",
      "Epoch [319/1000], Loss: 0.9686\n",
      "Epoch [320/1000], Loss: 0.9680\n",
      "Epoch [321/1000], Loss: 0.9674\n",
      "Epoch [322/1000], Loss: 0.9670\n",
      "Epoch [323/1000], Loss: 0.9667\n",
      "Epoch [324/1000], Loss: 0.9662\n",
      "Epoch [325/1000], Loss: 0.9655\n",
      "Epoch [326/1000], Loss: 0.9648\n",
      "Epoch [327/1000], Loss: 0.9642\n",
      "Epoch [328/1000], Loss: 0.9639\n",
      "Epoch [329/1000], Loss: 0.9636\n",
      "Epoch [330/1000], Loss: 0.9633\n",
      "Epoch [331/1000], Loss: 0.9630\n",
      "Epoch [332/1000], Loss: 0.9627\n",
      "Epoch [333/1000], Loss: 0.9627\n",
      "Epoch [334/1000], Loss: 0.9624\n",
      "Epoch [335/1000], Loss: 0.9616\n",
      "Epoch [336/1000], Loss: 0.9603\n",
      "Epoch [337/1000], Loss: 0.9592\n",
      "Epoch [338/1000], Loss: 0.9587\n",
      "Epoch [339/1000], Loss: 0.9586\n",
      "Epoch [340/1000], Loss: 0.9585\n",
      "Epoch [341/1000], Loss: 0.9580\n",
      "Epoch [342/1000], Loss: 0.9573\n",
      "Epoch [343/1000], Loss: 0.9565\n",
      "Epoch [344/1000], Loss: 0.9560\n",
      "Epoch [345/1000], Loss: 0.9556\n",
      "Epoch [346/1000], Loss: 0.9550\n",
      "Epoch [347/1000], Loss: 0.9546\n",
      "Epoch [348/1000], Loss: 0.9545\n",
      "Epoch [349/1000], Loss: 0.9544\n",
      "Epoch [350/1000], Loss: 0.9541\n",
      "Epoch [351/1000], Loss: 0.9534\n",
      "Epoch [352/1000], Loss: 0.9527\n",
      "Epoch [353/1000], Loss: 0.9522\n",
      "Epoch [354/1000], Loss: 0.9518\n",
      "Epoch [355/1000], Loss: 0.9510\n",
      "Epoch [356/1000], Loss: 0.9500\n",
      "Epoch [357/1000], Loss: 0.9493\n",
      "Epoch [358/1000], Loss: 0.9491\n",
      "Epoch [359/1000], Loss: 0.9488\n",
      "Epoch [360/1000], Loss: 0.9483\n",
      "Epoch [361/1000], Loss: 0.9478\n",
      "Epoch [362/1000], Loss: 0.9473\n",
      "Epoch [363/1000], Loss: 0.9468\n",
      "Epoch [364/1000], Loss: 0.9461\n",
      "Epoch [365/1000], Loss: 0.9454\n",
      "Epoch [366/1000], Loss: 0.9448\n",
      "Epoch [367/1000], Loss: 0.9444\n",
      "Epoch [368/1000], Loss: 0.9441\n",
      "Epoch [369/1000], Loss: 0.9438\n",
      "Epoch [370/1000], Loss: 0.9433\n",
      "Epoch [371/1000], Loss: 0.9428\n",
      "Epoch [372/1000], Loss: 0.9423\n",
      "Epoch [373/1000], Loss: 0.9419\n",
      "Epoch [374/1000], Loss: 0.9413\n",
      "Epoch [375/1000], Loss: 0.9406\n",
      "Epoch [376/1000], Loss: 0.9399\n",
      "Epoch [377/1000], Loss: 0.9394\n",
      "Epoch [378/1000], Loss: 0.9389\n",
      "Epoch [379/1000], Loss: 0.9386\n",
      "Epoch [380/1000], Loss: 0.9383\n",
      "Epoch [381/1000], Loss: 0.9381\n",
      "Epoch [382/1000], Loss: 0.9379\n",
      "Epoch [383/1000], Loss: 0.9380\n",
      "Epoch [384/1000], Loss: 0.9379\n",
      "Epoch [385/1000], Loss: 0.9377\n",
      "Epoch [386/1000], Loss: 0.9367\n",
      "Epoch [387/1000], Loss: 0.9353\n",
      "Epoch [388/1000], Loss: 0.9342\n",
      "Epoch [389/1000], Loss: 0.9338\n",
      "Epoch [390/1000], Loss: 0.9341\n",
      "Epoch [391/1000], Loss: 0.9342\n",
      "Epoch [392/1000], Loss: 0.9341\n",
      "Epoch [393/1000], Loss: 0.9329\n",
      "Epoch [394/1000], Loss: 0.9318\n",
      "Epoch [395/1000], Loss: 0.9312\n",
      "Epoch [396/1000], Loss: 0.9311\n",
      "Epoch [397/1000], Loss: 0.9310\n",
      "Epoch [398/1000], Loss: 0.9303\n",
      "Epoch [399/1000], Loss: 0.9294\n",
      "Epoch [400/1000], Loss: 0.9286\n",
      "Epoch [401/1000], Loss: 0.9280\n",
      "Epoch [402/1000], Loss: 0.9276\n",
      "Epoch [403/1000], Loss: 0.9272\n",
      "Epoch [404/1000], Loss: 0.9266\n",
      "Epoch [405/1000], Loss: 0.9262\n",
      "Epoch [406/1000], Loss: 0.9258\n",
      "Epoch [407/1000], Loss: 0.9254\n",
      "Epoch [408/1000], Loss: 0.9249\n",
      "Epoch [409/1000], Loss: 0.9243\n",
      "Epoch [410/1000], Loss: 0.9239\n",
      "Epoch [411/1000], Loss: 0.9238\n",
      "Epoch [412/1000], Loss: 0.9239\n",
      "Epoch [413/1000], Loss: 0.9240\n",
      "Epoch [414/1000], Loss: 0.9238\n",
      "Epoch [415/1000], Loss: 0.9229\n",
      "Epoch [416/1000], Loss: 0.9220\n",
      "Epoch [417/1000], Loss: 0.9213\n",
      "Epoch [418/1000], Loss: 0.9206\n",
      "Epoch [419/1000], Loss: 0.9200\n",
      "Epoch [420/1000], Loss: 0.9193\n",
      "Epoch [421/1000], Loss: 0.9189\n",
      "Epoch [422/1000], Loss: 0.9187\n",
      "Epoch [423/1000], Loss: 0.9183\n",
      "Epoch [424/1000], Loss: 0.9177\n",
      "Epoch [425/1000], Loss: 0.9169\n",
      "Epoch [426/1000], Loss: 0.9163\n",
      "Epoch [427/1000], Loss: 0.9160\n",
      "Epoch [428/1000], Loss: 0.9157\n",
      "Epoch [429/1000], Loss: 0.9151\n",
      "Epoch [430/1000], Loss: 0.9143\n",
      "Epoch [431/1000], Loss: 0.9137\n",
      "Epoch [432/1000], Loss: 0.9132\n",
      "Epoch [433/1000], Loss: 0.9130\n",
      "Epoch [434/1000], Loss: 0.9126\n",
      "Epoch [435/1000], Loss: 0.9122\n",
      "Epoch [436/1000], Loss: 0.9117\n",
      "Epoch [437/1000], Loss: 0.9113\n",
      "Epoch [438/1000], Loss: 0.9110\n",
      "Epoch [439/1000], Loss: 0.9107\n",
      "Epoch [440/1000], Loss: 0.9101\n",
      "Epoch [441/1000], Loss: 0.9095\n",
      "Epoch [442/1000], Loss: 0.9087\n",
      "Epoch [443/1000], Loss: 0.9081\n",
      "Epoch [444/1000], Loss: 0.9076\n",
      "Epoch [445/1000], Loss: 0.9072\n",
      "Epoch [446/1000], Loss: 0.9068\n",
      "Epoch [447/1000], Loss: 0.9064\n",
      "Epoch [448/1000], Loss: 0.9061\n",
      "Epoch [449/1000], Loss: 0.9060\n",
      "Epoch [450/1000], Loss: 0.9062\n",
      "Epoch [451/1000], Loss: 0.9069\n",
      "Epoch [452/1000], Loss: 0.9074\n",
      "Epoch [453/1000], Loss: 0.9076\n",
      "Epoch [454/1000], Loss: 0.9057\n",
      "Epoch [455/1000], Loss: 0.9034\n",
      "Epoch [456/1000], Loss: 0.9021\n",
      "Epoch [457/1000], Loss: 0.9024\n",
      "Epoch [458/1000], Loss: 0.9035\n",
      "Epoch [459/1000], Loss: 0.9031\n",
      "Epoch [460/1000], Loss: 0.9022\n",
      "Epoch [461/1000], Loss: 0.9004\n",
      "Epoch [462/1000], Loss: 0.8997\n",
      "Epoch [463/1000], Loss: 0.9000\n",
      "Epoch [464/1000], Loss: 0.9000\n",
      "Epoch [465/1000], Loss: 0.8992\n",
      "Epoch [466/1000], Loss: 0.8979\n",
      "Epoch [467/1000], Loss: 0.8971\n",
      "Epoch [468/1000], Loss: 0.8971\n",
      "Epoch [469/1000], Loss: 0.8969\n",
      "Epoch [470/1000], Loss: 0.8964\n",
      "Epoch [471/1000], Loss: 0.8954\n",
      "Epoch [472/1000], Loss: 0.8948\n",
      "Epoch [473/1000], Loss: 0.8946\n",
      "Epoch [474/1000], Loss: 0.8944\n",
      "Epoch [475/1000], Loss: 0.8940\n",
      "Epoch [476/1000], Loss: 0.8933\n",
      "Epoch [477/1000], Loss: 0.8927\n",
      "Epoch [478/1000], Loss: 0.8925\n",
      "Epoch [479/1000], Loss: 0.8926\n",
      "Epoch [480/1000], Loss: 0.8930\n",
      "Epoch [481/1000], Loss: 0.8933\n",
      "Epoch [482/1000], Loss: 0.8935\n",
      "Epoch [483/1000], Loss: 0.8933\n",
      "Epoch [484/1000], Loss: 0.8922\n",
      "Epoch [485/1000], Loss: 0.8908\n",
      "Epoch [486/1000], Loss: 0.8894\n",
      "Epoch [487/1000], Loss: 0.8887\n",
      "Epoch [488/1000], Loss: 0.8887\n",
      "Epoch [489/1000], Loss: 0.8889\n",
      "Epoch [490/1000], Loss: 0.8885\n",
      "Epoch [491/1000], Loss: 0.8873\n",
      "Epoch [492/1000], Loss: 0.8862\n",
      "Epoch [493/1000], Loss: 0.8857\n",
      "Epoch [494/1000], Loss: 0.8858\n",
      "Epoch [495/1000], Loss: 0.8859\n",
      "Epoch [496/1000], Loss: 0.8851\n",
      "Epoch [497/1000], Loss: 0.8840\n",
      "Epoch [498/1000], Loss: 0.8832\n",
      "Epoch [499/1000], Loss: 0.8829\n",
      "Epoch [500/1000], Loss: 0.8829\n",
      "Epoch [501/1000], Loss: 0.8827\n",
      "Epoch [502/1000], Loss: 0.8821\n",
      "Epoch [503/1000], Loss: 0.8813\n",
      "Epoch [504/1000], Loss: 0.8808\n",
      "Epoch [505/1000], Loss: 0.8805\n",
      "Epoch [506/1000], Loss: 0.8803\n",
      "Epoch [507/1000], Loss: 0.8802\n",
      "Epoch [508/1000], Loss: 0.8798\n",
      "Epoch [509/1000], Loss: 0.8795\n",
      "Epoch [510/1000], Loss: 0.8792\n",
      "Epoch [511/1000], Loss: 0.8791\n",
      "Epoch [512/1000], Loss: 0.8788\n",
      "Epoch [513/1000], Loss: 0.8783\n",
      "Epoch [514/1000], Loss: 0.8777\n",
      "Epoch [515/1000], Loss: 0.8769\n",
      "Epoch [516/1000], Loss: 0.8762\n",
      "Epoch [517/1000], Loss: 0.8756\n",
      "Epoch [518/1000], Loss: 0.8749\n",
      "Epoch [519/1000], Loss: 0.8744\n",
      "Epoch [520/1000], Loss: 0.8740\n",
      "Epoch [521/1000], Loss: 0.8737\n",
      "Epoch [522/1000], Loss: 0.8736\n",
      "Epoch [523/1000], Loss: 0.8735\n",
      "Epoch [524/1000], Loss: 0.8733\n",
      "Epoch [525/1000], Loss: 0.8728\n",
      "Epoch [526/1000], Loss: 0.8720\n",
      "Epoch [527/1000], Loss: 0.8711\n",
      "Epoch [528/1000], Loss: 0.8703\n",
      "Epoch [529/1000], Loss: 0.8699\n",
      "Epoch [530/1000], Loss: 0.8696\n",
      "Epoch [531/1000], Loss: 0.8694\n",
      "Epoch [532/1000], Loss: 0.8691\n",
      "Epoch [533/1000], Loss: 0.8689\n",
      "Epoch [534/1000], Loss: 0.8686\n",
      "Epoch [535/1000], Loss: 0.8684\n",
      "Epoch [536/1000], Loss: 0.8682\n",
      "Epoch [537/1000], Loss: 0.8680\n",
      "Epoch [538/1000], Loss: 0.8677\n",
      "Epoch [539/1000], Loss: 0.8670\n",
      "Epoch [540/1000], Loss: 0.8663\n",
      "Epoch [541/1000], Loss: 0.8655\n",
      "Epoch [542/1000], Loss: 0.8652\n",
      "Epoch [543/1000], Loss: 0.8654\n",
      "Epoch [544/1000], Loss: 0.8654\n",
      "Epoch [545/1000], Loss: 0.8658\n",
      "Epoch [546/1000], Loss: 0.8650\n",
      "Epoch [547/1000], Loss: 0.8640\n",
      "Epoch [548/1000], Loss: 0.8626\n",
      "Epoch [549/1000], Loss: 0.8615\n",
      "Epoch [550/1000], Loss: 0.8610\n",
      "Epoch [551/1000], Loss: 0.8609\n",
      "Epoch [552/1000], Loss: 0.8610\n",
      "Epoch [553/1000], Loss: 0.8607\n",
      "Epoch [554/1000], Loss: 0.8605\n",
      "Epoch [555/1000], Loss: 0.8599\n",
      "Epoch [556/1000], Loss: 0.8596\n",
      "Epoch [557/1000], Loss: 0.8592\n",
      "Epoch [558/1000], Loss: 0.8586\n",
      "Epoch [559/1000], Loss: 0.8578\n",
      "Epoch [560/1000], Loss: 0.8570\n",
      "Epoch [561/1000], Loss: 0.8564\n",
      "Epoch [562/1000], Loss: 0.8561\n",
      "Epoch [563/1000], Loss: 0.8560\n",
      "Epoch [564/1000], Loss: 0.8560\n",
      "Epoch [565/1000], Loss: 0.8558\n",
      "Epoch [566/1000], Loss: 0.8553\n",
      "Epoch [567/1000], Loss: 0.8545\n",
      "Epoch [568/1000], Loss: 0.8537\n",
      "Epoch [569/1000], Loss: 0.8531\n",
      "Epoch [570/1000], Loss: 0.8527\n",
      "Epoch [571/1000], Loss: 0.8526\n",
      "Epoch [572/1000], Loss: 0.8525\n",
      "Epoch [573/1000], Loss: 0.8527\n",
      "Epoch [574/1000], Loss: 0.8527\n",
      "Epoch [575/1000], Loss: 0.8530\n",
      "Epoch [576/1000], Loss: 0.8534\n",
      "Epoch [577/1000], Loss: 0.8533\n",
      "Epoch [578/1000], Loss: 0.8527\n",
      "Epoch [579/1000], Loss: 0.8509\n",
      "Epoch [580/1000], Loss: 0.8492\n",
      "Epoch [581/1000], Loss: 0.8482\n",
      "Epoch [582/1000], Loss: 0.8483\n",
      "Epoch [583/1000], Loss: 0.8491\n",
      "Epoch [584/1000], Loss: 0.8491\n",
      "Epoch [585/1000], Loss: 0.8484\n",
      "Epoch [586/1000], Loss: 0.8468\n",
      "Epoch [587/1000], Loss: 0.8455\n",
      "Epoch [588/1000], Loss: 0.8451\n",
      "Epoch [589/1000], Loss: 0.8453\n",
      "Epoch [590/1000], Loss: 0.8456\n",
      "Epoch [591/1000], Loss: 0.8451\n",
      "Epoch [592/1000], Loss: 0.8442\n",
      "Epoch [593/1000], Loss: 0.8432\n",
      "Epoch [594/1000], Loss: 0.8428\n",
      "Epoch [595/1000], Loss: 0.8428\n",
      "Epoch [596/1000], Loss: 0.8430\n",
      "Epoch [597/1000], Loss: 0.8434\n",
      "Epoch [598/1000], Loss: 0.8434\n",
      "Epoch [599/1000], Loss: 0.8439\n",
      "Epoch [600/1000], Loss: 0.8444\n",
      "Epoch [601/1000], Loss: 0.8443\n",
      "Epoch [602/1000], Loss: 0.8438\n",
      "Epoch [603/1000], Loss: 0.8411\n",
      "Epoch [604/1000], Loss: 0.8391\n",
      "Epoch [605/1000], Loss: 0.8386\n",
      "Epoch [606/1000], Loss: 0.8393\n",
      "Epoch [607/1000], Loss: 0.8401\n",
      "Epoch [608/1000], Loss: 0.8395\n",
      "Epoch [609/1000], Loss: 0.8382\n",
      "Epoch [610/1000], Loss: 0.8368\n",
      "Epoch [611/1000], Loss: 0.8362\n",
      "Epoch [612/1000], Loss: 0.8364\n",
      "Epoch [613/1000], Loss: 0.8363\n",
      "Epoch [614/1000], Loss: 0.8357\n",
      "Epoch [615/1000], Loss: 0.8348\n",
      "Epoch [616/1000], Loss: 0.8341\n",
      "Epoch [617/1000], Loss: 0.8338\n",
      "Epoch [618/1000], Loss: 0.8337\n",
      "Epoch [619/1000], Loss: 0.8332\n",
      "Epoch [620/1000], Loss: 0.8325\n",
      "Epoch [621/1000], Loss: 0.8319\n",
      "Epoch [622/1000], Loss: 0.8316\n",
      "Epoch [623/1000], Loss: 0.8315\n",
      "Epoch [624/1000], Loss: 0.8314\n",
      "Epoch [625/1000], Loss: 0.8309\n",
      "Epoch [626/1000], Loss: 0.8302\n",
      "Epoch [627/1000], Loss: 0.8295\n",
      "Epoch [628/1000], Loss: 0.8290\n",
      "Epoch [629/1000], Loss: 0.8287\n",
      "Epoch [630/1000], Loss: 0.8285\n",
      "Epoch [631/1000], Loss: 0.8282\n",
      "Epoch [632/1000], Loss: 0.8278\n",
      "Epoch [633/1000], Loss: 0.8273\n",
      "Epoch [634/1000], Loss: 0.8270\n",
      "Epoch [635/1000], Loss: 0.8269\n",
      "Epoch [636/1000], Loss: 0.8271\n",
      "Epoch [637/1000], Loss: 0.8275\n",
      "Epoch [638/1000], Loss: 0.8281\n",
      "Epoch [639/1000], Loss: 0.8283\n",
      "Epoch [640/1000], Loss: 0.8279\n",
      "Epoch [641/1000], Loss: 0.8267\n",
      "Epoch [642/1000], Loss: 0.8248\n",
      "Epoch [643/1000], Loss: 0.8234\n",
      "Epoch [644/1000], Loss: 0.8227\n",
      "Epoch [645/1000], Loss: 0.8228\n",
      "Epoch [646/1000], Loss: 0.8232\n",
      "Epoch [647/1000], Loss: 0.8235\n",
      "Epoch [648/1000], Loss: 0.8231\n",
      "Epoch [649/1000], Loss: 0.8223\n",
      "Epoch [650/1000], Loss: 0.8212\n",
      "Epoch [651/1000], Loss: 0.8202\n",
      "Epoch [652/1000], Loss: 0.8197\n",
      "Epoch [653/1000], Loss: 0.8196\n",
      "Epoch [654/1000], Loss: 0.8198\n",
      "Epoch [655/1000], Loss: 0.8199\n",
      "Epoch [656/1000], Loss: 0.8197\n",
      "Epoch [657/1000], Loss: 0.8196\n",
      "Epoch [658/1000], Loss: 0.8197\n",
      "Epoch [659/1000], Loss: 0.8200\n",
      "Epoch [660/1000], Loss: 0.8212\n",
      "Epoch [661/1000], Loss: 0.8213\n",
      "Epoch [662/1000], Loss: 0.8204\n",
      "Epoch [663/1000], Loss: 0.8178\n",
      "Epoch [664/1000], Loss: 0.8156\n",
      "Epoch [665/1000], Loss: 0.8148\n",
      "Epoch [666/1000], Loss: 0.8154\n",
      "Epoch [667/1000], Loss: 0.8160\n",
      "Epoch [668/1000], Loss: 0.8159\n",
      "Epoch [669/1000], Loss: 0.8149\n",
      "Epoch [670/1000], Loss: 0.8137\n",
      "Epoch [671/1000], Loss: 0.8132\n",
      "Epoch [672/1000], Loss: 0.8131\n",
      "Epoch [673/1000], Loss: 0.8128\n",
      "Epoch [674/1000], Loss: 0.8119\n",
      "Epoch [675/1000], Loss: 0.8109\n",
      "Epoch [676/1000], Loss: 0.8103\n",
      "Epoch [677/1000], Loss: 0.8102\n",
      "Epoch [678/1000], Loss: 0.8103\n",
      "Epoch [679/1000], Loss: 0.8103\n",
      "Epoch [680/1000], Loss: 0.8097\n",
      "Epoch [681/1000], Loss: 0.8089\n",
      "Epoch [682/1000], Loss: 0.8081\n",
      "Epoch [683/1000], Loss: 0.8077\n",
      "Epoch [684/1000], Loss: 0.8075\n",
      "Epoch [685/1000], Loss: 0.8074\n",
      "Epoch [686/1000], Loss: 0.8070\n",
      "Epoch [687/1000], Loss: 0.8065\n",
      "Epoch [688/1000], Loss: 0.8057\n",
      "Epoch [689/1000], Loss: 0.8051\n",
      "Epoch [690/1000], Loss: 0.8047\n",
      "Epoch [691/1000], Loss: 0.8044\n",
      "Epoch [692/1000], Loss: 0.8042\n",
      "Epoch [693/1000], Loss: 0.8040\n",
      "Epoch [694/1000], Loss: 0.8037\n",
      "Epoch [695/1000], Loss: 0.8033\n",
      "Epoch [696/1000], Loss: 0.8029\n",
      "Epoch [697/1000], Loss: 0.8027\n",
      "Epoch [698/1000], Loss: 0.8027\n",
      "Epoch [699/1000], Loss: 0.8031\n",
      "Epoch [700/1000], Loss: 0.8043\n",
      "Epoch [701/1000], Loss: 0.8053\n",
      "Epoch [702/1000], Loss: 0.8072\n",
      "Epoch [703/1000], Loss: 0.8057\n",
      "Epoch [704/1000], Loss: 0.8039\n",
      "Epoch [705/1000], Loss: 0.8003\n",
      "Epoch [706/1000], Loss: 0.7986\n",
      "Epoch [707/1000], Loss: 0.7990\n",
      "Epoch [708/1000], Loss: 0.8003\n",
      "Epoch [709/1000], Loss: 0.8014\n",
      "Epoch [710/1000], Loss: 0.8004\n",
      "Epoch [711/1000], Loss: 0.7988\n",
      "Epoch [712/1000], Loss: 0.7970\n",
      "Epoch [713/1000], Loss: 0.7963\n",
      "Epoch [714/1000], Loss: 0.7966\n",
      "Epoch [715/1000], Loss: 0.7971\n",
      "Epoch [716/1000], Loss: 0.7968\n",
      "Epoch [717/1000], Loss: 0.7957\n",
      "Epoch [718/1000], Loss: 0.7944\n",
      "Epoch [719/1000], Loss: 0.7937\n",
      "Epoch [720/1000], Loss: 0.7938\n",
      "Epoch [721/1000], Loss: 0.7940\n",
      "Epoch [722/1000], Loss: 0.7939\n",
      "Epoch [723/1000], Loss: 0.7932\n",
      "Epoch [724/1000], Loss: 0.7922\n",
      "Epoch [725/1000], Loss: 0.7915\n",
      "Epoch [726/1000], Loss: 0.7911\n",
      "Epoch [727/1000], Loss: 0.7909\n",
      "Epoch [728/1000], Loss: 0.7909\n",
      "Epoch [729/1000], Loss: 0.7906\n",
      "Epoch [730/1000], Loss: 0.7902\n",
      "Epoch [731/1000], Loss: 0.7895\n",
      "Epoch [732/1000], Loss: 0.7889\n",
      "Epoch [733/1000], Loss: 0.7884\n",
      "Epoch [734/1000], Loss: 0.7881\n",
      "Epoch [735/1000], Loss: 0.7879\n",
      "Epoch [736/1000], Loss: 0.7878\n",
      "Epoch [737/1000], Loss: 0.7877\n",
      "Epoch [738/1000], Loss: 0.7879\n",
      "Epoch [739/1000], Loss: 0.7883\n",
      "Epoch [740/1000], Loss: 0.7893\n",
      "Epoch [741/1000], Loss: 0.7905\n",
      "Epoch [742/1000], Loss: 0.7923\n",
      "Epoch [743/1000], Loss: 0.7924\n",
      "Epoch [744/1000], Loss: 0.7907\n",
      "Epoch [745/1000], Loss: 0.7872\n",
      "Epoch [746/1000], Loss: 0.7849\n",
      "Epoch [747/1000], Loss: 0.7849\n",
      "Epoch [748/1000], Loss: 0.7860\n",
      "Epoch [749/1000], Loss: 0.7867\n",
      "Epoch [750/1000], Loss: 0.7856\n",
      "Epoch [751/1000], Loss: 0.7836\n",
      "Epoch [752/1000], Loss: 0.7824\n",
      "Epoch [753/1000], Loss: 0.7824\n",
      "Epoch [754/1000], Loss: 0.7828\n",
      "Epoch [755/1000], Loss: 0.7824\n",
      "Epoch [756/1000], Loss: 0.7814\n",
      "Epoch [757/1000], Loss: 0.7804\n",
      "Epoch [758/1000], Loss: 0.7803\n",
      "Epoch [759/1000], Loss: 0.7806\n",
      "Epoch [760/1000], Loss: 0.7804\n",
      "Epoch [761/1000], Loss: 0.7798\n",
      "Epoch [762/1000], Loss: 0.7789\n",
      "Epoch [763/1000], Loss: 0.7787\n",
      "Epoch [764/1000], Loss: 0.7792\n",
      "Epoch [765/1000], Loss: 0.7798\n",
      "Epoch [766/1000], Loss: 0.7796\n",
      "Epoch [767/1000], Loss: 0.7788\n",
      "Epoch [768/1000], Loss: 0.7779\n",
      "Epoch [769/1000], Loss: 0.7774\n",
      "Epoch [770/1000], Loss: 0.7772\n",
      "Epoch [771/1000], Loss: 0.7768\n",
      "Epoch [772/1000], Loss: 0.7755\n",
      "Epoch [773/1000], Loss: 0.7743\n",
      "Epoch [774/1000], Loss: 0.7737\n",
      "Epoch [775/1000], Loss: 0.7737\n",
      "Epoch [776/1000], Loss: 0.7737\n",
      "Epoch [777/1000], Loss: 0.7735\n",
      "Epoch [778/1000], Loss: 0.7731\n",
      "Epoch [779/1000], Loss: 0.7728\n",
      "Epoch [780/1000], Loss: 0.7727\n",
      "Epoch [781/1000], Loss: 0.7727\n",
      "Epoch [782/1000], Loss: 0.7725\n",
      "Epoch [783/1000], Loss: 0.7719\n",
      "Epoch [784/1000], Loss: 0.7710\n",
      "Epoch [785/1000], Loss: 0.7703\n",
      "Epoch [786/1000], Loss: 0.7697\n",
      "Epoch [787/1000], Loss: 0.7693\n",
      "Epoch [788/1000], Loss: 0.7689\n",
      "Epoch [789/1000], Loss: 0.7685\n",
      "Epoch [790/1000], Loss: 0.7680\n",
      "Epoch [791/1000], Loss: 0.7676\n",
      "Epoch [792/1000], Loss: 0.7673\n",
      "Epoch [793/1000], Loss: 0.7671\n",
      "Epoch [794/1000], Loss: 0.7670\n",
      "Epoch [795/1000], Loss: 0.7670\n",
      "Epoch [796/1000], Loss: 0.7671\n",
      "Epoch [797/1000], Loss: 0.7673\n",
      "Epoch [798/1000], Loss: 0.7676\n",
      "Epoch [799/1000], Loss: 0.7679\n",
      "Epoch [800/1000], Loss: 0.7681\n",
      "Epoch [801/1000], Loss: 0.7675\n",
      "Epoch [802/1000], Loss: 0.7665\n",
      "Epoch [803/1000], Loss: 0.7649\n",
      "Epoch [804/1000], Loss: 0.7634\n",
      "Epoch [805/1000], Loss: 0.7625\n",
      "Epoch [806/1000], Loss: 0.7624\n",
      "Epoch [807/1000], Loss: 0.7627\n",
      "Epoch [808/1000], Loss: 0.7633\n",
      "Epoch [809/1000], Loss: 0.7640\n",
      "Epoch [810/1000], Loss: 0.7639\n",
      "Epoch [811/1000], Loss: 0.7643\n",
      "Epoch [812/1000], Loss: 0.7635\n",
      "Epoch [813/1000], Loss: 0.7642\n",
      "Epoch [814/1000], Loss: 0.7651\n",
      "Epoch [815/1000], Loss: 0.7662\n",
      "Epoch [816/1000], Loss: 0.7674\n",
      "Epoch [817/1000], Loss: 0.7639\n",
      "Epoch [818/1000], Loss: 0.7605\n",
      "Epoch [819/1000], Loss: 0.7581\n",
      "Epoch [820/1000], Loss: 0.7585\n",
      "Epoch [821/1000], Loss: 0.7605\n",
      "Epoch [822/1000], Loss: 0.7613\n",
      "Epoch [823/1000], Loss: 0.7607\n",
      "Epoch [824/1000], Loss: 0.7581\n",
      "Epoch [825/1000], Loss: 0.7563\n",
      "Epoch [826/1000], Loss: 0.7559\n",
      "Epoch [827/1000], Loss: 0.7564\n",
      "Epoch [828/1000], Loss: 0.7568\n",
      "Epoch [829/1000], Loss: 0.7560\n",
      "Epoch [830/1000], Loss: 0.7550\n",
      "Epoch [831/1000], Loss: 0.7542\n",
      "Epoch [832/1000], Loss: 0.7541\n",
      "Epoch [833/1000], Loss: 0.7541\n",
      "Epoch [834/1000], Loss: 0.7537\n",
      "Epoch [835/1000], Loss: 0.7530\n",
      "Epoch [836/1000], Loss: 0.7522\n",
      "Epoch [837/1000], Loss: 0.7518\n",
      "Epoch [838/1000], Loss: 0.7517\n",
      "Epoch [839/1000], Loss: 0.7517\n",
      "Epoch [840/1000], Loss: 0.7513\n",
      "Epoch [841/1000], Loss: 0.7507\n",
      "Epoch [842/1000], Loss: 0.7500\n",
      "Epoch [843/1000], Loss: 0.7495\n",
      "Epoch [844/1000], Loss: 0.7492\n",
      "Epoch [845/1000], Loss: 0.7492\n",
      "Epoch [846/1000], Loss: 0.7491\n",
      "Epoch [847/1000], Loss: 0.7489\n",
      "Epoch [848/1000], Loss: 0.7485\n",
      "Epoch [849/1000], Loss: 0.7482\n",
      "Epoch [850/1000], Loss: 0.7479\n",
      "Epoch [851/1000], Loss: 0.7479\n",
      "Epoch [852/1000], Loss: 0.7482\n",
      "Epoch [853/1000], Loss: 0.7485\n",
      "Epoch [854/1000], Loss: 0.7492\n",
      "Epoch [855/1000], Loss: 0.7491\n",
      "Epoch [856/1000], Loss: 0.7494\n",
      "Epoch [857/1000], Loss: 0.7482\n",
      "Epoch [858/1000], Loss: 0.7477\n",
      "Epoch [859/1000], Loss: 0.7464\n",
      "Epoch [860/1000], Loss: 0.7455\n",
      "Epoch [861/1000], Loss: 0.7448\n",
      "Epoch [862/1000], Loss: 0.7441\n",
      "Epoch [863/1000], Loss: 0.7437\n",
      "Epoch [864/1000], Loss: 0.7434\n",
      "Epoch [865/1000], Loss: 0.7434\n",
      "Epoch [866/1000], Loss: 0.7436\n",
      "Epoch [867/1000], Loss: 0.7437\n",
      "Epoch [868/1000], Loss: 0.7435\n",
      "Epoch [869/1000], Loss: 0.7430\n",
      "Epoch [870/1000], Loss: 0.7422\n",
      "Epoch [871/1000], Loss: 0.7414\n",
      "Epoch [872/1000], Loss: 0.7407\n",
      "Epoch [873/1000], Loss: 0.7404\n",
      "Epoch [874/1000], Loss: 0.7404\n",
      "Epoch [875/1000], Loss: 0.7405\n",
      "Epoch [876/1000], Loss: 0.7401\n",
      "Epoch [877/1000], Loss: 0.7397\n",
      "Epoch [878/1000], Loss: 0.7388\n",
      "Epoch [879/1000], Loss: 0.7379\n",
      "Epoch [880/1000], Loss: 0.7372\n",
      "Epoch [881/1000], Loss: 0.7369\n",
      "Epoch [882/1000], Loss: 0.7368\n",
      "Epoch [883/1000], Loss: 0.7370\n",
      "Epoch [884/1000], Loss: 0.7372\n",
      "Epoch [885/1000], Loss: 0.7372\n",
      "Epoch [886/1000], Loss: 0.7370\n",
      "Epoch [887/1000], Loss: 0.7366\n",
      "Epoch [888/1000], Loss: 0.7360\n",
      "Epoch [889/1000], Loss: 0.7355\n",
      "Epoch [890/1000], Loss: 0.7351\n",
      "Epoch [891/1000], Loss: 0.7349\n",
      "Epoch [892/1000], Loss: 0.7348\n",
      "Epoch [893/1000], Loss: 0.7347\n",
      "Epoch [894/1000], Loss: 0.7343\n",
      "Epoch [895/1000], Loss: 0.7337\n",
      "Epoch [896/1000], Loss: 0.7330\n",
      "Epoch [897/1000], Loss: 0.7323\n",
      "Epoch [898/1000], Loss: 0.7321\n",
      "Epoch [899/1000], Loss: 0.7320\n",
      "Epoch [900/1000], Loss: 0.7324\n",
      "Epoch [901/1000], Loss: 0.7327\n",
      "Epoch [902/1000], Loss: 0.7328\n",
      "Epoch [903/1000], Loss: 0.7324\n",
      "Epoch [904/1000], Loss: 0.7315\n",
      "Epoch [905/1000], Loss: 0.7302\n",
      "Epoch [906/1000], Loss: 0.7292\n",
      "Epoch [907/1000], Loss: 0.7285\n",
      "Epoch [908/1000], Loss: 0.7282\n",
      "Epoch [909/1000], Loss: 0.7282\n",
      "Epoch [910/1000], Loss: 0.7281\n",
      "Epoch [911/1000], Loss: 0.7278\n",
      "Epoch [912/1000], Loss: 0.7273\n",
      "Epoch [913/1000], Loss: 0.7267\n",
      "Epoch [914/1000], Loss: 0.7262\n",
      "Epoch [915/1000], Loss: 0.7260\n",
      "Epoch [916/1000], Loss: 0.7260\n",
      "Epoch [917/1000], Loss: 0.7264\n",
      "Epoch [918/1000], Loss: 0.7270\n",
      "Epoch [919/1000], Loss: 0.7278\n",
      "Epoch [920/1000], Loss: 0.7284\n",
      "Epoch [921/1000], Loss: 0.7291\n",
      "Epoch [922/1000], Loss: 0.7286\n",
      "Epoch [923/1000], Loss: 0.7290\n",
      "Epoch [924/1000], Loss: 0.7284\n",
      "Epoch [925/1000], Loss: 0.7281\n",
      "Epoch [926/1000], Loss: 0.7272\n",
      "Epoch [927/1000], Loss: 0.7258\n",
      "Epoch [928/1000], Loss: 0.7236\n",
      "Epoch [929/1000], Loss: 0.7227\n",
      "Epoch [930/1000], Loss: 0.7231\n",
      "Epoch [931/1000], Loss: 0.7237\n",
      "Epoch [932/1000], Loss: 0.7247\n",
      "Epoch [933/1000], Loss: 0.7236\n",
      "Epoch [934/1000], Loss: 0.7218\n",
      "Epoch [935/1000], Loss: 0.7199\n",
      "Epoch [936/1000], Loss: 0.7190\n",
      "Epoch [937/1000], Loss: 0.7194\n",
      "Epoch [938/1000], Loss: 0.7201\n",
      "Epoch [939/1000], Loss: 0.7204\n",
      "Epoch [940/1000], Loss: 0.7194\n",
      "Epoch [941/1000], Loss: 0.7182\n",
      "Epoch [942/1000], Loss: 0.7170\n",
      "Epoch [943/1000], Loss: 0.7166\n",
      "Epoch [944/1000], Loss: 0.7167\n",
      "Epoch [945/1000], Loss: 0.7168\n",
      "Epoch [946/1000], Loss: 0.7165\n",
      "Epoch [947/1000], Loss: 0.7159\n",
      "Epoch [948/1000], Loss: 0.7153\n",
      "Epoch [949/1000], Loss: 0.7149\n",
      "Epoch [950/1000], Loss: 0.7148\n",
      "Epoch [951/1000], Loss: 0.7149\n",
      "Epoch [952/1000], Loss: 0.7146\n",
      "Epoch [953/1000], Loss: 0.7143\n",
      "Epoch [954/1000], Loss: 0.7136\n",
      "Epoch [955/1000], Loss: 0.7129\n",
      "Epoch [956/1000], Loss: 0.7124\n",
      "Epoch [957/1000], Loss: 0.7121\n",
      "Epoch [958/1000], Loss: 0.7120\n",
      "Epoch [959/1000], Loss: 0.7118\n",
      "Epoch [960/1000], Loss: 0.7116\n",
      "Epoch [961/1000], Loss: 0.7114\n",
      "Epoch [962/1000], Loss: 0.7111\n",
      "Epoch [963/1000], Loss: 0.7110\n",
      "Epoch [964/1000], Loss: 0.7111\n",
      "Epoch [965/1000], Loss: 0.7115\n",
      "Epoch [966/1000], Loss: 0.7126\n",
      "Epoch [967/1000], Loss: 0.7133\n",
      "Epoch [968/1000], Loss: 0.7148\n",
      "Epoch [969/1000], Loss: 0.7137\n",
      "Epoch [970/1000], Loss: 0.7129\n",
      "Epoch [971/1000], Loss: 0.7101\n",
      "Epoch [972/1000], Loss: 0.7081\n",
      "Epoch [973/1000], Loss: 0.7072\n",
      "Epoch [974/1000], Loss: 0.7074\n",
      "Epoch [975/1000], Loss: 0.7083\n",
      "Epoch [976/1000], Loss: 0.7091\n",
      "Epoch [977/1000], Loss: 0.7098\n",
      "Epoch [978/1000], Loss: 0.7093\n",
      "Epoch [979/1000], Loss: 0.7085\n",
      "Epoch [980/1000], Loss: 0.7072\n",
      "Epoch [981/1000], Loss: 0.7063\n",
      "Epoch [982/1000], Loss: 0.7058\n",
      "Epoch [983/1000], Loss: 0.7061\n",
      "Epoch [984/1000], Loss: 0.7063\n",
      "Epoch [985/1000], Loss: 0.7064\n",
      "Epoch [986/1000], Loss: 0.7062\n",
      "Epoch [987/1000], Loss: 0.7056\n",
      "Epoch [988/1000], Loss: 0.7051\n",
      "Epoch [989/1000], Loss: 0.7046\n",
      "Epoch [990/1000], Loss: 0.7044\n",
      "Epoch [991/1000], Loss: 0.7036\n",
      "Epoch [992/1000], Loss: 0.7030\n",
      "Epoch [993/1000], Loss: 0.7022\n",
      "Epoch [994/1000], Loss: 0.7016\n",
      "Epoch [995/1000], Loss: 0.7013\n",
      "Epoch [996/1000], Loss: 0.7012\n",
      "Epoch [997/1000], Loss: 0.7008\n",
      "Epoch [998/1000], Loss: 0.7002\n",
      "Epoch [999/1000], Loss: 0.6996\n",
      "Epoch [1000/1000], Loss: 0.6989\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 1000\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: \n",
      " Accuracy: 0.76, Avg loss: 0.698524 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_predictions = []\n",
    "train_targets = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "size = len(train_dataloader.dataset)\n",
    "num_batches = len(train_dataloader)\n",
    "loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in train_dataloader:\n",
    "        pred = model(X)\n",
    "        loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        train_predictions.append(pred.argmax(1))\n",
    "        train_targets.append(y)\n",
    "        \n",
    "loss /= num_batches\n",
    "correct /= size\n",
    "print(f\"Training Error: \\n Accuracy: {(correct):>0.2f}, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 0.76, Avg loss: 0.698523 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "size = len(val_dataloader.dataset)\n",
    "num_batches = len(val_dataloader)\n",
    "loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in val_dataloader:\n",
    "        pred = model(X)\n",
    "        loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        val_predictions.append(pred.argmax(1))\n",
    "        val_targets.append(y)\n",
    "\n",
    "loss /= num_batches\n",
    "correct /= size\n",
    "print(f\"Validation Error: \\n Accuracy: {(correct):>0.2f}, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = train_predictions[0].tolist()\n",
    "train_targets = train_targets[0].tolist()\n",
    "\n",
    "train_predictions = [x for x in train_predictions]\n",
    "train_targets = [x for x in train_targets]\n",
    "\n",
    "val_predictions = val_predictions[0].tolist()\n",
    "val_targets = val_targets[0].tolist()\n",
    "\n",
    "val_predictions = [x for x in val_predictions]\n",
    "val_targets = [x for x in val_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, targets, num_classes):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for multiclass classification tasks: accuracy, precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): Predicted class labels (sequence_length).\n",
    "        targets (list): True class labels (sequence_length).\n",
    "        num_classes (int): Number of classes in the classification task.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics: accuracy, precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    # Ensure predictions and targets have the same length\n",
    "    assert len(predictions) == len(targets)\n",
    "    \n",
    "    # Convert lists to numpy arrays if they are not already\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # Initialize evaluation metrics arrays for each class\n",
    "    true_positives = np.zeros(num_classes)\n",
    "    true_negatives = np.zeros(num_classes)\n",
    "    false_positives = np.zeros(num_classes)\n",
    "    false_negatives = np.zeros(num_classes)\n",
    "    \n",
    "    # Compute true positives, false positives, false negatives, and class counts for each class\n",
    "    for i in range(num_classes):\n",
    "        true_positives[i] = np.sum((predictions == i) & (targets == i))\n",
    "        true_negatives[i] = np.sum((predictions != i) & (targets != i))\n",
    "        false_positives[i] = np.sum((predictions == i) & (targets != i))\n",
    "        false_negatives[i] = np.sum((predictions != i) & (targets == i))\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = np.mean(predictions == targets)\n",
    "    \n",
    "    # Compute precision, recall, and F1-score for each class\n",
    "    precisions = true_positives / (true_positives + false_positives)\n",
    "    recalls = true_positives / (true_positives + false_negatives)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls)\n",
    "    accuracies = (true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives)\n",
    "    \n",
    "    # Compute micro-averaged precision, recall, and F1-score\n",
    "    micro_precision = np.sum(true_positives) / np.sum(true_positives + false_positives)\n",
    "    micro_recall = np.sum(true_positives) / np.sum(true_positives + false_negatives)\n",
    "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n",
    "    \n",
    "    # Compute macro-averaged precision, recall, and F1-score\n",
    "    macro_precision = np.mean(precisions)\n",
    "    macro_recall = np.mean(recalls)\n",
    "    macro_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    # Return evaluation metrics as dictionary\n",
    "    evaluation_metrics = {\n",
    "        'extended_metrics':{\n",
    "            'true_positives': true_positives,\n",
    "            'true_negatives': true_negatives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives,\n",
    "            'precision': precisions,\n",
    "            'recall': recalls,\n",
    "            'f1_score': f1_scores,\n",
    "            'accuracy': accuracies\n",
    "        },\n",
    "        'aggregated_metrics':{\n",
    "            'accuracy': accuracy,\n",
    "            'micro_precision': micro_precision,\n",
    "            'micro_recall': micro_recall,\n",
    "            'micro_f1': micro_f1,\n",
    "            'macro_precision': macro_precision,\n",
    "            'macro_recall': macro_recall,\n",
    "            'macro_f1': macro_f1\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluation_results = evaluate(predictions=train_predictions, targets=train_targets, num_classes=5)\n",
    "val_evaluation_results = evaluate(predictions=val_predictions, targets=val_targets, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7560\n",
      "Micro_precision: 0.7560\n",
      "Micro_recall: 0.7560\n",
      "Micro_f1: 0.7560\n",
      "Macro_precision: 0.7554\n",
      "Macro_recall: 0.7558\n",
      "Macro_f1: 0.7550\n",
      "\n",
      "\n",
      "Accuracy: 0.7560\n",
      "Micro_precision: 0.7560\n",
      "Micro_recall: 0.7560\n",
      "Micro_f1: 0.7560\n",
      "Macro_precision: 0.7554\n",
      "Macro_recall: 0.7558\n",
      "Macro_f1: 0.7550\n"
     ]
    }
   ],
   "source": [
    "for metric, value in train_evaluation_results['aggregated_metrics'].items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for metric, value in val_evaluation_results['aggregated_metrics'].items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0\n",
      "True_positives: 7204.0000\n",
      "True_negatives: 32416.0000\n",
      "False_positives: 1284.0000\n",
      "False_negatives: 1202.0000\n",
      "Precision: 0.8487\n",
      "Recall: 0.8570\n",
      "F1_score: 0.8528\n",
      "Accuracy: 0.9410\n",
      "\n",
      "Class 1\n",
      "True_positives: 6340.0000\n",
      "True_negatives: 31029.0000\n",
      "False_positives: 2655.0000\n",
      "False_negatives: 2082.0000\n",
      "Precision: 0.7048\n",
      "Recall: 0.7528\n",
      "F1_score: 0.7280\n",
      "Accuracy: 0.8875\n",
      "\n",
      "Class 2\n",
      "True_positives: 5290.0000\n",
      "True_negatives: 31578.0000\n",
      "False_positives: 2169.0000\n",
      "False_negatives: 3069.0000\n",
      "Precision: 0.7092\n",
      "Recall: 0.6329\n",
      "F1_score: 0.6689\n",
      "Accuracy: 0.8756\n",
      "\n",
      "Class 3\n",
      "True_positives: 6289.0000\n",
      "True_negatives: 31558.0000\n",
      "False_positives: 2085.0000\n",
      "False_negatives: 2174.0000\n",
      "Precision: 0.7510\n",
      "Recall: 0.7431\n",
      "F1_score: 0.7470\n",
      "Accuracy: 0.8989\n",
      "\n",
      "Class 4\n",
      "True_positives: 6709.0000\n",
      "True_negatives: 31569.0000\n",
      "False_positives: 2081.0000\n",
      "False_negatives: 1747.0000\n",
      "Precision: 0.7633\n",
      "Recall: 0.7934\n",
      "F1_score: 0.7780\n",
      "Accuracy: 0.9091\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'\\nClass {i}')\n",
    "    for metric, value in train_evaluation_results['extended_metrics'].items():\n",
    "        print(f'{metric.capitalize()}: {value[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0\n",
      "True_positives: 7204.0000\n",
      "True_negatives: 32416.0000\n",
      "False_positives: 1284.0000\n",
      "False_negatives: 1202.0000\n",
      "Precision: 0.8487\n",
      "Recall: 0.8570\n",
      "F1_score: 0.8528\n",
      "Accuracy: 0.9410\n",
      "\n",
      "Class 1\n",
      "True_positives: 6340.0000\n",
      "True_negatives: 31029.0000\n",
      "False_positives: 2655.0000\n",
      "False_negatives: 2082.0000\n",
      "Precision: 0.7048\n",
      "Recall: 0.7528\n",
      "F1_score: 0.7280\n",
      "Accuracy: 0.8875\n",
      "\n",
      "Class 2\n",
      "True_positives: 5290.0000\n",
      "True_negatives: 31578.0000\n",
      "False_positives: 2169.0000\n",
      "False_negatives: 3069.0000\n",
      "Precision: 0.7092\n",
      "Recall: 0.6329\n",
      "F1_score: 0.6689\n",
      "Accuracy: 0.8756\n",
      "\n",
      "Class 3\n",
      "True_positives: 6289.0000\n",
      "True_negatives: 31558.0000\n",
      "False_positives: 2085.0000\n",
      "False_negatives: 2174.0000\n",
      "Precision: 0.7510\n",
      "Recall: 0.7431\n",
      "F1_score: 0.7470\n",
      "Accuracy: 0.8989\n",
      "\n",
      "Class 4\n",
      "True_positives: 6709.0000\n",
      "True_negatives: 31569.0000\n",
      "False_positives: 2081.0000\n",
      "False_negatives: 1747.0000\n",
      "Precision: 0.7633\n",
      "Recall: 0.7934\n",
      "F1_score: 0.7780\n",
      "Accuracy: 0.9091\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'\\nClass {i}')\n",
    "    for metric, value in val_evaluation_results['extended_metrics'].items():\n",
    "        print(f'{metric.capitalize()}: {value[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 43.54it/s]\n",
      "100%|██████████| 40/40 [00:00<00:00, 47.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mapply\n",
    "import torch\n",
    "# set mapply\n",
    "mapply.init(n_workers=-1, progressbar=True)\n",
    "\n",
    "import json\n",
    "# Load training set\n",
    "train_set = pd.read_csv('train_set.csv')\n",
    "\n",
    "train_set['tokens'] = train_set['tokens'].apply(json.loads)\n",
    "train_set['filtered_tokens'] = train_set['filtered_tokens'].apply(json.loads)\n",
    "train_set['filtered_unique_tokens'] = train_set['filtered_unique_tokens'].apply(json.loads)\n",
    "\n",
    "# Load validation set\n",
    "val_set = pd.read_csv('val_set.csv')\n",
    "\n",
    "val_set['tokens'] = val_set['tokens'].apply(json.loads)\n",
    "val_set['filtered_tokens'] = val_set['filtered_tokens'].apply(json.loads)\n",
    "val_set['filtered_unique_tokens'] = val_set['filtered_unique_tokens'].apply(json.loads)\n",
    "\n",
    "label = {\n",
    "    'rap': torch.tensor(0, dtype=torch.long),\n",
    "    'rb': torch.tensor(1, dtype=torch.long),\n",
    "    'pop': torch.tensor(2, dtype=torch.long),\n",
    "    'rock': torch.tensor(3, dtype=torch.long),\n",
    "    'country': torch.tensor(4, dtype=torch.long)\n",
    "}\n",
    "\n",
    "train_set['label'] = train_set['tag'].mapply(lambda x: label[x])\n",
    "val_set['label'] = val_set['tag'].mapply(lambda x: label[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch\n",
    "import torchtext.vocab as vocab\n",
    "\n",
    "# Load FastText embeddings\n",
    "fasttext_embeddings = vocab.FastText('simple')\n",
    "\n",
    "def vectorize_2(lyric_tokens):\n",
    "    lyric_tokens = lyric_tokens[0:5]\n",
    "    lyric_tokens = [token for token in lyric_tokens if token in fasttext_embeddings.stoi]\n",
    "    lyrics_vector = [fasttext_embeddings[token].tolist() for token in lyric_tokens]\n",
    "    return lyrics_vector\n",
    "\n",
    "# Compute FastText embeddings\n",
    "train_set['FastText_embedding_2'] = train_set['filtered_tokens'].apply(vectorize_2)\n",
    "val_set['FastText_embedding_2'] = val_set['filtered_tokens'].apply(vectorize_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [torch.tensor(seq) for seq in train_set['FastText_embedding_2'].to_list() if len(seq) != 0]\n",
    "labels = [label for seq, label in zip(train_set['FastText_embedding_2'].to_list(), train_set['label'].to_list()) if len(seq) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics_df):\n",
    "        \n",
    "        self.x = sequences\n",
    "        self.y = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Sort batch based on sequence length for pack_padded_sequence\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the length of the longest sequence in the batch\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    \n",
    "    # Compute sequence lengths for pack_padded_sequence\n",
    "    seq_lengths = torch.tensor([len(seq) for seq in sequences])\n",
    "    \n",
    "    return padded_sequences, labels, seq_lengths\n",
    "\n",
    "training_set = LyricsDataset(train_set)\n",
    "train_dataloader = DataLoader(training_set, batch_size=50000, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "validation_set = LyricsDataset(val_set)\n",
    "val_dataloader = DataLoader(validation_set, batch_size=50000, shuffle=True, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8843/1923054973.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.6130\n",
      "Epoch [2/100], Loss: 1.6085\n",
      "Epoch [3/100], Loss: 1.6047\n",
      "Epoch [4/100], Loss: 1.6014\n",
      "Epoch [5/100], Loss: 1.5985\n",
      "Epoch [6/100], Loss: 1.5958\n",
      "Epoch [7/100], Loss: 1.5932\n",
      "Epoch [8/100], Loss: 1.5905\n",
      "Epoch [9/100], Loss: 1.5877\n",
      "Epoch [10/100], Loss: 1.5847\n",
      "Epoch [11/100], Loss: 1.5816\n",
      "Epoch [12/100], Loss: 1.5782\n",
      "Epoch [13/100], Loss: 1.5746\n",
      "Epoch [14/100], Loss: 1.5708\n",
      "Epoch [15/100], Loss: 1.5668\n",
      "Epoch [16/100], Loss: 1.5626\n",
      "Epoch [17/100], Loss: 1.5583\n",
      "Epoch [18/100], Loss: 1.5538\n",
      "Epoch [19/100], Loss: 1.5492\n",
      "Epoch [20/100], Loss: 1.5445\n",
      "Epoch [21/100], Loss: 1.5397\n",
      "Epoch [22/100], Loss: 1.5349\n",
      "Epoch [23/100], Loss: 1.5302\n",
      "Epoch [24/100], Loss: 1.5257\n",
      "Epoch [25/100], Loss: 1.5214\n",
      "Epoch [26/100], Loss: 1.5174\n",
      "Epoch [27/100], Loss: 1.5137\n",
      "Epoch [28/100], Loss: 1.5103\n",
      "Epoch [29/100], Loss: 1.5071\n",
      "Epoch [30/100], Loss: 1.5039\n",
      "Epoch [31/100], Loss: 1.5007\n",
      "Epoch [32/100], Loss: 1.4974\n",
      "Epoch [33/100], Loss: 1.4942\n",
      "Epoch [34/100], Loss: 1.4910\n",
      "Epoch [35/100], Loss: 1.4880\n",
      "Epoch [36/100], Loss: 1.4850\n",
      "Epoch [37/100], Loss: 1.4821\n",
      "Epoch [38/100], Loss: 1.4794\n",
      "Epoch [39/100], Loss: 1.4768\n",
      "Epoch [40/100], Loss: 1.4743\n",
      "Epoch [41/100], Loss: 1.4718\n",
      "Epoch [42/100], Loss: 1.4695\n",
      "Epoch [43/100], Loss: 1.4674\n",
      "Epoch [44/100], Loss: 1.4653\n",
      "Epoch [45/100], Loss: 1.4634\n",
      "Epoch [46/100], Loss: 1.4615\n",
      "Epoch [47/100], Loss: 1.4596\n",
      "Epoch [48/100], Loss: 1.4578\n",
      "Epoch [49/100], Loss: 1.4559\n",
      "Epoch [50/100], Loss: 1.4541\n",
      "Epoch [51/100], Loss: 1.4523\n",
      "Epoch [52/100], Loss: 1.4506\n",
      "Epoch [53/100], Loss: 1.4489\n",
      "Epoch [54/100], Loss: 1.4472\n",
      "Epoch [55/100], Loss: 1.4456\n",
      "Epoch [56/100], Loss: 1.4441\n",
      "Epoch [57/100], Loss: 1.4426\n",
      "Epoch [58/100], Loss: 1.4411\n",
      "Epoch [59/100], Loss: 1.4396\n",
      "Epoch [60/100], Loss: 1.4382\n",
      "Epoch [61/100], Loss: 1.4368\n",
      "Epoch [62/100], Loss: 1.4354\n",
      "Epoch [63/100], Loss: 1.4339\n",
      "Epoch [64/100], Loss: 1.4325\n",
      "Epoch [65/100], Loss: 1.4311\n",
      "Epoch [66/100], Loss: 1.4298\n",
      "Epoch [67/100], Loss: 1.4284\n",
      "Epoch [68/100], Loss: 1.4271\n",
      "Epoch [69/100], Loss: 1.4258\n",
      "Epoch [70/100], Loss: 1.4245\n",
      "Epoch [71/100], Loss: 1.4232\n",
      "Epoch [72/100], Loss: 1.4219\n",
      "Epoch [73/100], Loss: 1.4206\n",
      "Epoch [74/100], Loss: 1.4193\n",
      "Epoch [75/100], Loss: 1.4180\n",
      "Epoch [76/100], Loss: 1.4168\n",
      "Epoch [77/100], Loss: 1.4155\n",
      "Epoch [78/100], Loss: 1.4143\n",
      "Epoch [79/100], Loss: 1.4131\n",
      "Epoch [80/100], Loss: 1.4118\n",
      "Epoch [81/100], Loss: 1.4105\n",
      "Epoch [82/100], Loss: 1.4093\n",
      "Epoch [83/100], Loss: 1.4081\n",
      "Epoch [84/100], Loss: 1.4068\n",
      "Epoch [85/100], Loss: 1.4055\n",
      "Epoch [86/100], Loss: 1.4043\n",
      "Epoch [87/100], Loss: 1.4030\n",
      "Epoch [88/100], Loss: 1.4017\n",
      "Epoch [89/100], Loss: 1.4004\n",
      "Epoch [90/100], Loss: 1.3991\n",
      "Epoch [91/100], Loss: 1.3978\n",
      "Epoch [92/100], Loss: 1.3966\n",
      "Epoch [93/100], Loss: 1.3954\n",
      "Epoch [94/100], Loss: 1.3945\n",
      "Epoch [95/100], Loss: 1.3933\n",
      "Epoch [96/100], Loss: 1.3914\n",
      "Epoch [97/100], Loss: 1.3904\n",
      "Epoch [98/100], Loss: 1.3893\n",
      "Epoch [99/100], Loss: 1.3876\n",
      "Epoch [100/100], Loss: 1.3866\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = int(hidden_size/2)\n",
    "        self.lstm = nn.LSTM(input_size, int(hidden_size/2), batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, seq_lengths):\n",
    "        packed_input = pack_padded_sequence(input_seq, seq_lengths, batch_first=True, enforce_sorted=True)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.fc(output[:, -1, :])  # Get the last output of each sequence\n",
    "        return output\n",
    "\n",
    "# Example parameters\n",
    "embedding_dim = 300  # Dimension of word embeddings\n",
    "hidden_dim = 128  # Hidden dimension of the LSTM\n",
    "output_dim = 5  # Number of output classes\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for X, y, seq_lengths in train_dataloader:\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X, seq_lengths)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8843/4234377109.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: \n",
      " Accuracy: 0.43, Avg loss: 1.385330 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_predictions = []\n",
    "train_targets = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "size = len(train_dataloader.dataset)\n",
    "num_batches = len(train_dataloader)\n",
    "loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y, seq_lengths in train_dataloader:\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        pred = model(X, seq_lengths)\n",
    "        loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        train_predictions.append(pred.argmax(1))\n",
    "        train_targets.append(y)\n",
    "        \n",
    "loss /= num_batches\n",
    "correct /= size\n",
    "print(f\"Training Error: \\n Accuracy: {(correct):>0.2f}, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8843/1500241406.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 0.43, Avg loss: 1.385330 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "size = len(val_dataloader.dataset)\n",
    "num_batches = len(val_dataloader)\n",
    "loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y, seq_lengths in val_dataloader:\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        pred = model(X, seq_lengths)\n",
    "        loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        val_predictions.append(pred.argmax(1))\n",
    "        val_targets.append(y)\n",
    "\n",
    "loss /= num_batches\n",
    "correct /= size\n",
    "print(f\"Validation Error: \\n Accuracy: {(correct):>0.2f}, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = train_predictions[0].tolist()\n",
    "train_targets = train_targets[0].tolist()\n",
    "\n",
    "train_predictions = [x for x in train_predictions]\n",
    "train_targets = [x for x in train_targets]\n",
    "\n",
    "val_predictions = val_predictions[0].tolist()\n",
    "val_targets = val_targets[0].tolist()\n",
    "\n",
    "val_predictions = [x for x in val_predictions]\n",
    "val_targets = [x for x in val_targets]\n",
    "\n",
    "train_evaluation_results = evaluate(predictions=train_predictions, targets=train_targets, num_classes=5)\n",
    "val_evaluation_results = evaluate(predictions=val_predictions, targets=val_targets, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4292\n",
      "Micro_precision: 0.4292\n",
      "Micro_recall: 0.4292\n",
      "Micro_f1: 0.4292\n",
      "Macro_precision: 0.4185\n",
      "Macro_recall: 0.4289\n",
      "Macro_f1: 0.4179\n",
      "\n",
      "\n",
      "Accuracy: 0.4292\n",
      "Micro_precision: 0.4292\n",
      "Micro_recall: 0.4292\n",
      "Micro_f1: 0.4292\n",
      "Macro_precision: 0.4185\n",
      "Macro_recall: 0.4289\n",
      "Macro_f1: 0.4179\n"
     ]
    }
   ],
   "source": [
    "for metric, value in train_evaluation_results['aggregated_metrics'].items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for metric, value in val_evaluation_results['aggregated_metrics'].items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0\n",
      "True_positives: 5451.0000\n",
      "True_negatives: 27992.0000\n",
      "False_positives: 5682.0000\n",
      "False_negatives: 2938.0000\n",
      "Precision: 0.4896\n",
      "Recall: 0.6498\n",
      "F1_score: 0.5584\n",
      "Accuracy: 0.7951\n",
      "\n",
      "Class 1\n",
      "True_positives: 2944.0000\n",
      "True_negatives: 29288.0000\n",
      "False_positives: 4365.0000\n",
      "False_negatives: 5466.0000\n",
      "Precision: 0.4028\n",
      "Recall: 0.3501\n",
      "F1_score: 0.3746\n",
      "Accuracy: 0.7663\n",
      "\n",
      "Class 2\n",
      "True_positives: 1867.0000\n",
      "True_negatives: 30237.0000\n",
      "False_positives: 3477.0000\n",
      "False_negatives: 6482.0000\n",
      "Precision: 0.3494\n",
      "Recall: 0.2236\n",
      "F1_score: 0.2727\n",
      "Accuracy: 0.7632\n",
      "\n",
      "Class 3\n",
      "True_positives: 3529.0000\n",
      "True_negatives: 28458.0000\n",
      "False_positives: 5144.0000\n",
      "False_negatives: 4932.0000\n",
      "Precision: 0.4069\n",
      "Recall: 0.4171\n",
      "F1_score: 0.4119\n",
      "Accuracy: 0.7605\n",
      "\n",
      "Class 4\n",
      "True_positives: 4261.0000\n",
      "True_negatives: 28266.0000\n",
      "False_positives: 5343.0000\n",
      "False_negatives: 4193.0000\n",
      "Precision: 0.4437\n",
      "Recall: 0.5040\n",
      "F1_score: 0.4719\n",
      "Accuracy: 0.7733\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'\\nClass {i}')\n",
    "    for metric, value in train_evaluation_results['extended_metrics'].items():\n",
    "        print(f'{metric.capitalize()}: {value[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0\n",
      "True_positives: 5451.0000\n",
      "True_negatives: 27992.0000\n",
      "False_positives: 5682.0000\n",
      "False_negatives: 2938.0000\n",
      "Precision: 0.4896\n",
      "Recall: 0.6498\n",
      "F1_score: 0.5584\n",
      "Accuracy: 0.7951\n",
      "\n",
      "Class 1\n",
      "True_positives: 2944.0000\n",
      "True_negatives: 29288.0000\n",
      "False_positives: 4365.0000\n",
      "False_negatives: 5466.0000\n",
      "Precision: 0.4028\n",
      "Recall: 0.3501\n",
      "F1_score: 0.3746\n",
      "Accuracy: 0.7663\n",
      "\n",
      "Class 2\n",
      "True_positives: 1867.0000\n",
      "True_negatives: 30237.0000\n",
      "False_positives: 3477.0000\n",
      "False_negatives: 6482.0000\n",
      "Precision: 0.3494\n",
      "Recall: 0.2236\n",
      "F1_score: 0.2727\n",
      "Accuracy: 0.7632\n",
      "\n",
      "Class 3\n",
      "True_positives: 3529.0000\n",
      "True_negatives: 28458.0000\n",
      "False_positives: 5144.0000\n",
      "False_negatives: 4932.0000\n",
      "Precision: 0.4069\n",
      "Recall: 0.4171\n",
      "F1_score: 0.4119\n",
      "Accuracy: 0.7605\n",
      "\n",
      "Class 4\n",
      "True_positives: 4261.0000\n",
      "True_negatives: 28266.0000\n",
      "False_positives: 5343.0000\n",
      "False_negatives: 4193.0000\n",
      "Precision: 0.4437\n",
      "Recall: 0.5040\n",
      "F1_score: 0.4719\n",
      "Accuracy: 0.7733\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'\\nClass {i}')\n",
    "    for metric, value in val_evaluation_results['extended_metrics'].items():\n",
    "        print(f'{metric.capitalize()}: {value[i]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8843/2227556864.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.6115\n",
      "Epoch [2/100], Loss: 1.6080\n",
      "Epoch [3/100], Loss: 1.6050\n",
      "Epoch [4/100], Loss: 1.6023\n",
      "Epoch [5/100], Loss: 1.5997\n",
      "Epoch [6/100], Loss: 1.5972\n",
      "Epoch [7/100], Loss: 1.5946\n",
      "Epoch [8/100], Loss: 1.5919\n",
      "Epoch [9/100], Loss: 1.5890\n",
      "Epoch [10/100], Loss: 1.5860\n",
      "Epoch [11/100], Loss: 1.5827\n",
      "Epoch [12/100], Loss: 1.5793\n",
      "Epoch [13/100], Loss: 1.5756\n",
      "Epoch [14/100], Loss: 1.5717\n",
      "Epoch [15/100], Loss: 1.5676\n",
      "Epoch [16/100], Loss: 1.5633\n",
      "Epoch [17/100], Loss: 1.5589\n",
      "Epoch [18/100], Loss: 1.5544\n",
      "Epoch [19/100], Loss: 1.5498\n",
      "Epoch [20/100], Loss: 1.5451\n",
      "Epoch [21/100], Loss: 1.5405\n",
      "Epoch [22/100], Loss: 1.5360\n",
      "Epoch [23/100], Loss: 1.5317\n",
      "Epoch [24/100], Loss: 1.5277\n",
      "Epoch [25/100], Loss: 1.5240\n",
      "Epoch [26/100], Loss: 1.5204\n",
      "Epoch [27/100], Loss: 1.5170\n",
      "Epoch [28/100], Loss: 1.5136\n",
      "Epoch [29/100], Loss: 1.5100\n",
      "Epoch [30/100], Loss: 1.5062\n",
      "Epoch [31/100], Loss: 1.5024\n",
      "Epoch [32/100], Loss: 1.4985\n",
      "Epoch [33/100], Loss: 1.4946\n",
      "Epoch [34/100], Loss: 1.4910\n",
      "Epoch [35/100], Loss: 1.4877\n",
      "Epoch [36/100], Loss: 1.4846\n",
      "Epoch [37/100], Loss: 1.4817\n",
      "Epoch [38/100], Loss: 1.4790\n",
      "Epoch [39/100], Loss: 1.4765\n",
      "Epoch [40/100], Loss: 1.4741\n",
      "Epoch [41/100], Loss: 1.4719\n",
      "Epoch [42/100], Loss: 1.4697\n",
      "Epoch [43/100], Loss: 1.4677\n",
      "Epoch [44/100], Loss: 1.4656\n",
      "Epoch [45/100], Loss: 1.4636\n",
      "Epoch [46/100], Loss: 1.4616\n",
      "Epoch [47/100], Loss: 1.4596\n",
      "Epoch [48/100], Loss: 1.4576\n",
      "Epoch [49/100], Loss: 1.4556\n",
      "Epoch [50/100], Loss: 1.4538\n",
      "Epoch [51/100], Loss: 1.4520\n",
      "Epoch [52/100], Loss: 1.4503\n",
      "Epoch [53/100], Loss: 1.4487\n",
      "Epoch [54/100], Loss: 1.4470\n",
      "Epoch [55/100], Loss: 1.4454\n",
      "Epoch [56/100], Loss: 1.4438\n",
      "Epoch [57/100], Loss: 1.4423\n",
      "Epoch [58/100], Loss: 1.4408\n",
      "Epoch [59/100], Loss: 1.4393\n",
      "Epoch [60/100], Loss: 1.4378\n",
      "Epoch [61/100], Loss: 1.4364\n",
      "Epoch [62/100], Loss: 1.4349\n",
      "Epoch [63/100], Loss: 1.4335\n",
      "Epoch [64/100], Loss: 1.4321\n",
      "Epoch [65/100], Loss: 1.4307\n",
      "Epoch [66/100], Loss: 1.4294\n",
      "Epoch [67/100], Loss: 1.4280\n",
      "Epoch [68/100], Loss: 1.4266\n",
      "Epoch [69/100], Loss: 1.4253\n",
      "Epoch [70/100], Loss: 1.4240\n",
      "Epoch [71/100], Loss: 1.4227\n",
      "Epoch [72/100], Loss: 1.4214\n",
      "Epoch [73/100], Loss: 1.4201\n",
      "Epoch [74/100], Loss: 1.4188\n",
      "Epoch [75/100], Loss: 1.4175\n",
      "Epoch [76/100], Loss: 1.4162\n",
      "Epoch [77/100], Loss: 1.4149\n",
      "Epoch [78/100], Loss: 1.4136\n",
      "Epoch [79/100], Loss: 1.4123\n",
      "Epoch [80/100], Loss: 1.4111\n",
      "Epoch [81/100], Loss: 1.4098\n",
      "Epoch [82/100], Loss: 1.4085\n",
      "Epoch [83/100], Loss: 1.4072\n",
      "Epoch [84/100], Loss: 1.4059\n",
      "Epoch [85/100], Loss: 1.4046\n",
      "Epoch [86/100], Loss: 1.4033\n",
      "Epoch [87/100], Loss: 1.4020\n",
      "Epoch [88/100], Loss: 1.4007\n",
      "Epoch [89/100], Loss: 1.3994\n",
      "Epoch [90/100], Loss: 1.3984\n",
      "Epoch [91/100], Loss: 1.3974\n",
      "Epoch [92/100], Loss: 1.3956\n",
      "Epoch [93/100], Loss: 1.3944\n",
      "Epoch [94/100], Loss: 1.3935\n",
      "Epoch [95/100], Loss: 1.3918\n",
      "Epoch [96/100], Loss: 1.3907\n",
      "Epoch [97/100], Loss: 1.3895\n",
      "Epoch [98/100], Loss: 1.3880\n",
      "Epoch [99/100], Loss: 1.3870\n",
      "Epoch [100/100], Loss: 1.3855\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# LSTM model\n",
    "class RNNodel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = int(hidden_size/2)\n",
    "        self.lstm = nn.RNN(input_size, int(hidden_size/2), batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq, seq_lengths):\n",
    "        packed_input = pack_padded_sequence(input_seq, seq_lengths, batch_first=True, enforce_sorted=True)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.fc(output[:, -1, :])  # Get the last output of each sequence\n",
    "        return output\n",
    "\n",
    "# Example parameters\n",
    "embedding_dim = 300  # Dimension of word embeddings\n",
    "hidden_dim = 128  # Hidden dimension of the LSTM\n",
    "output_dim = 5  # Number of output classes\n",
    "\n",
    "# Initialize the model\n",
    "model = LSTMModel(embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for X, y, seq_lengths in train_dataloader:\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X, seq_lengths)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8843/4234377109.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: \n",
      " Accuracy: 0.43, Avg loss: 1.384185 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_predictions = []\n",
    "train_targets = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "size = len(train_dataloader.dataset)\n",
    "num_batches = len(train_dataloader)\n",
    "loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y, seq_lengths in train_dataloader:\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        pred = model(X, seq_lengths)\n",
    "        loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        train_predictions.append(pred.argmax(1))\n",
    "        train_targets.append(y)\n",
    "        \n",
    "loss /= num_batches\n",
    "correct /= size\n",
    "print(f\"Training Error: \\n Accuracy: {(correct):>0.2f}, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8843/1500241406.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " Accuracy: 0.43, Avg loss: 1.384186 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "val_predictions = []\n",
    "val_targets = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "size = len(val_dataloader.dataset)\n",
    "num_batches = len(val_dataloader)\n",
    "loss, correct = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y, seq_lengths in val_dataloader:\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        pred = model(X, seq_lengths)\n",
    "        loss += loss_fn(pred, y).item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        val_predictions.append(pred.argmax(1))\n",
    "        val_targets.append(y)\n",
    "\n",
    "loss /= num_batches\n",
    "correct /= size\n",
    "print(f\"Validation Error: \\n Accuracy: {(correct):>0.2f}, Avg loss: {loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = train_predictions[0].tolist()\n",
    "train_targets = train_targets[0].tolist()\n",
    "\n",
    "train_predictions = [x for x in train_predictions]\n",
    "train_targets = [x for x in train_targets]\n",
    "\n",
    "val_predictions = val_predictions[0].tolist()\n",
    "val_targets = val_targets[0].tolist()\n",
    "\n",
    "val_predictions = [x for x in val_predictions]\n",
    "val_targets = [x for x in val_targets]\n",
    "\n",
    "train_evaluation_results = evaluate(predictions=train_predictions, targets=train_targets, num_classes=5)\n",
    "val_evaluation_results = evaluate(predictions=val_predictions, targets=val_targets, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4339\n",
      "Micro_precision: 0.4339\n",
      "Micro_recall: 0.4339\n",
      "Micro_f1: 0.4339\n",
      "Macro_precision: 0.4249\n",
      "Macro_recall: 0.4337\n",
      "Macro_f1: 0.4247\n",
      "\n",
      "\n",
      "Accuracy: 0.4339\n",
      "Micro_precision: 0.4339\n",
      "Micro_recall: 0.4339\n",
      "Micro_f1: 0.4339\n",
      "Macro_precision: 0.4249\n",
      "Macro_recall: 0.4337\n",
      "Macro_f1: 0.4247\n"
     ]
    }
   ],
   "source": [
    "for metric, value in train_evaluation_results['aggregated_metrics'].items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for metric, value in val_evaluation_results['aggregated_metrics'].items():\n",
    "    print(f\"{metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0\n",
      "True_positives: 5499.0000\n",
      "True_negatives: 27898.0000\n",
      "False_positives: 5776.0000\n",
      "False_negatives: 2890.0000\n",
      "Precision: 0.4877\n",
      "Recall: 0.6555\n",
      "F1_score: 0.5593\n",
      "Accuracy: 0.7940\n",
      "\n",
      "Class 1\n",
      "True_positives: 3037.0000\n",
      "True_negatives: 29181.0000\n",
      "False_positives: 4472.0000\n",
      "False_negatives: 5373.0000\n",
      "Precision: 0.4044\n",
      "Recall: 0.3611\n",
      "F1_score: 0.3816\n",
      "Accuracy: 0.7659\n",
      "\n",
      "Class 2\n",
      "True_positives: 2085.0000\n",
      "True_negatives: 29920.0000\n",
      "False_positives: 3794.0000\n",
      "False_negatives: 6264.0000\n",
      "Precision: 0.3547\n",
      "Recall: 0.2497\n",
      "F1_score: 0.2931\n",
      "Accuracy: 0.7609\n",
      "\n",
      "Class 3\n",
      "True_positives: 3601.0000\n",
      "True_negatives: 28415.0000\n",
      "False_positives: 5187.0000\n",
      "False_negatives: 4860.0000\n",
      "Precision: 0.4098\n",
      "Recall: 0.4256\n",
      "F1_score: 0.4175\n",
      "Accuracy: 0.7611\n",
      "\n",
      "Class 4\n",
      "True_positives: 4028.0000\n",
      "True_negatives: 29025.0000\n",
      "False_positives: 4584.0000\n",
      "False_negatives: 4426.0000\n",
      "Precision: 0.4677\n",
      "Recall: 0.4765\n",
      "F1_score: 0.4720\n",
      "Accuracy: 0.7858\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'\\nClass {i}')\n",
    "    for metric, value in train_evaluation_results['extended_metrics'].items():\n",
    "        print(f'{metric.capitalize()}: {value[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0\n",
      "True_positives: 5499.0000\n",
      "True_negatives: 27898.0000\n",
      "False_positives: 5776.0000\n",
      "False_negatives: 2890.0000\n",
      "Precision: 0.4877\n",
      "Recall: 0.6555\n",
      "F1_score: 0.5593\n",
      "Accuracy: 0.7940\n",
      "\n",
      "Class 1\n",
      "True_positives: 3037.0000\n",
      "True_negatives: 29181.0000\n",
      "False_positives: 4472.0000\n",
      "False_negatives: 5373.0000\n",
      "Precision: 0.4044\n",
      "Recall: 0.3611\n",
      "F1_score: 0.3816\n",
      "Accuracy: 0.7659\n",
      "\n",
      "Class 2\n",
      "True_positives: 2085.0000\n",
      "True_negatives: 29920.0000\n",
      "False_positives: 3794.0000\n",
      "False_negatives: 6264.0000\n",
      "Precision: 0.3547\n",
      "Recall: 0.2497\n",
      "F1_score: 0.2931\n",
      "Accuracy: 0.7609\n",
      "\n",
      "Class 3\n",
      "True_positives: 3601.0000\n",
      "True_negatives: 28415.0000\n",
      "False_positives: 5187.0000\n",
      "False_negatives: 4860.0000\n",
      "Precision: 0.4098\n",
      "Recall: 0.4256\n",
      "F1_score: 0.4175\n",
      "Accuracy: 0.7611\n",
      "\n",
      "Class 4\n",
      "True_positives: 4028.0000\n",
      "True_negatives: 29025.0000\n",
      "False_positives: 4584.0000\n",
      "False_negatives: 4426.0000\n",
      "Precision: 0.4677\n",
      "Recall: 0.4765\n",
      "F1_score: 0.4720\n",
      "Accuracy: 0.7858\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'\\nClass {i}')\n",
    "    for metric, value in val_evaluation_results['extended_metrics'].items():\n",
    "        print(f'{metric.capitalize()}: {value[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
